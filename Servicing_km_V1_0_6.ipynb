{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdopsmang/Colab-projects/blob/main/Servicing_km_V1_0_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did not work on RR path, rather added bus details upto DP, in consolidated report, also batch processing done in few cases"
      ],
      "metadata": {
        "id": "JVbZ291M-7mv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CMKeh0lHx8tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f969a4-4497-4a9f-dfb4-ce43d7e777da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.32.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gspread\n",
        "!pip install gspread google-auth-oauthlib google-auth-httplib2\n",
        "!pip install gspread google-auth oauth2client\n",
        "!pip install gspread oauth2client pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GIVCOLMf4oFc"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "from google.oauth2.service_account import Credentials\n",
        "gc = gspread.authorize(creds)\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from datetime import datetime\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dX9vEbLhRZAG"
      },
      "outputs": [],
      "source": [
        "# Details our Database spreadsheet which has details of all files, ID in Transport Department\n",
        "\n",
        "Database_File_spreadsheet_ID = gc.open_by_key('1nJKyvV1WQmZzvbjOP7Hsp1bQJmiNsYoJOH_jIL7H9PI') #ID of Database Spreadsheet\n",
        "MASsheetID = Database_File_spreadsheet_ID.worksheet('MAS')                                    #MAS Worksheet\n",
        "OdometersheetID = Database_File_spreadsheet_ID.worksheet('Odometer')                          #Odometer Worksheet\n",
        "\n",
        "#STS RR SVP\n",
        "RR_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C2').value                  # Get the spreadsheet ID from cell C2\n",
        "RR_Oil_Lub = gc.open_by_key(RR_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open RR_Oil_Lub spreadsheet\n",
        "\n",
        "RR_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B2').value                        # Get the worksheet name from cell B2\n",
        "engine_oil_CH_BSIV_CH_MAS_rr = RR_Oil_Lub.worksheet(RR_Engine_oil_CH_BSIV_worksheet_name)  # Open RR CH engineoil worksheet\n",
        "\n",
        "RR_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B3').value                        # Get the worksheet name from cell B3\n",
        "engine_oil_CK_BSVI_CK_MAS_rr = RR_Oil_Lub.worksheet(RR_Engine_oil_CK_BSVI_worksheet_name)  # Open RR CK engineoil worksheet\n",
        "\n",
        "RR_Report1_worksheet_name = MASsheetID.acell('B8').value                                # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "RR_Report1_Worksheet = RR_Oil_Lub.worksheet(RR_Report1_worksheet_name)                  # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "#STS ATR SVP\n",
        "ATR_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C52').value                  # Get the spreadsheet ID from cell C51 ATR\n",
        "ATR_Oil_Lub = gc.open_by_key(ATR_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open ATR_Oil_Lub spreadsheet\n",
        "\n",
        "ATR_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B52').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CH_BSIV_CH_MAS_ATR = ATR_Oil_Lub.worksheet(ATR_Engine_oil_CH_BSIV_worksheet_name)  # Open ATR CH engineoil worksheet\n",
        "\n",
        "ATR_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B53').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CK_BSVI_CK_MAS_ATR = ATR_Oil_Lub.worksheet(ATR_Engine_oil_CK_BSVI_worksheet_name)  # Open ATR CK engineoil worksheet\n",
        "\n",
        "#ATR_Report1_worksheet_name = MASsheetID.acell('B58').value                           #******This may not be required       # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "#ATR_Report1_Worksheet = ATR_Oil_Lub.worksheet(ATR_Report1_worksheet_name)              #This may not be required     # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "#STS FG\n",
        "FG_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C102').value                  # Get the spreadsheet ID from cell C51 ATR\n",
        "FG_Oil_Lub = gc.open_by_key(FG_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open ATR_Oil_Lub spreadsheet\n",
        "\n",
        "FG_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B102').value                         # Get the worksheet name from cell B2\n",
        "engine_oil_CH_BSIV_CH_MAS_FG = FG_Oil_Lub.worksheet(FG_Engine_oil_CH_BSIV_worksheet_name)  # Open RR engineoil worksheet\n",
        "\n",
        "FG_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B103').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CK_BSVI_CK_MAS_FG = FG_Oil_Lub.worksheet(FG_Engine_oil_CK_BSVI_worksheet_name)  # Open ATR CK engineoil worksheet\n",
        "\n",
        "#FG_Report1_worksheet_name = MASsheetID.acell('B108').value                    #*****This may not be required            # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "#FG_Report1_Worksheet = FG_Oil_Lub.worksheet(FG_Report1_worksheet_name)          #This may not be required         # Open RR Km between and after last servicing worksheet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear data in range A5:A200\n",
        "clear_range_all_OdometersheetID = 'A5:R200'\n",
        "clear_range_all_RR_Report1_Worksheet = ['B2:B200', 'AC2:AC200']\n",
        "\n",
        "OdometersheetID.batch_clear([clear_range_all_OdometersheetID])\n",
        "RR_Report1_Worksheet.batch_clear(clear_range_all_RR_Report1_Worksheet)\n",
        "\n",
        "# === PROCESS BUS DATA ===\n",
        "\n",
        "# Process SVP data\n",
        "SVP_Odometer_id = OdometersheetID.acell('A2').value\n",
        "Odometer_spreadsheet_SVP = gc.open_by_key(SVP_Odometer_id)\n",
        "bus_number_SVP = sorted([worksheet.title for worksheet in Odometer_spreadsheet_SVP])\n",
        "spreadsheet_name_SVP = Odometer_spreadsheet_SVP.title\n",
        "start_row_svp = 2\n",
        "\n",
        "# Process FG data\n",
        "FG_Odometer_id = OdometersheetID.acell('B2').value\n",
        "Odometer_spreadsheet_FG = gc.open_by_key(FG_Odometer_id)\n",
        "bus_number_FG = sorted([worksheet.title for worksheet in Odometer_spreadsheet_FG])\n",
        "spreadsheet_name_FG = Odometer_spreadsheet_FG.title\n",
        "start_row_fg = start_row_svp + len(bus_number_SVP)\n",
        "\n",
        "# Process Baratang data\n",
        "BT_Odometer_id = OdometersheetID.acell('C2').value\n",
        "Odometer_spreadsheet_BT = gc.open_by_key(BT_Odometer_id)\n",
        "bus_number_BT = sorted([worksheet.title for worksheet in Odometer_spreadsheet_BT])\n",
        "spreadsheet_name_BT = Odometer_spreadsheet_BT.title\n",
        "start_row_bt = start_row_fg + len(bus_number_FG)\n",
        "\n",
        "# Process Rangat data\n",
        "RT_Odometer_id = OdometersheetID.acell('D2').value\n",
        "Odometer_spreadsheet_RT = gc.open_by_key(RT_Odometer_id)\n",
        "bus_number_RT = sorted([worksheet.title for worksheet in Odometer_spreadsheet_RT])\n",
        "spreadsheet_name_RT = Odometer_spreadsheet_RT.title\n",
        "start_row_rt = start_row_bt + len(bus_number_BT)\n",
        "\n",
        "# Process Mayabunder data\n",
        "MB_Odometer_id = OdometersheetID.acell('E2').value\n",
        "Odometer_spreadsheet_MB = gc.open_by_key(MB_Odometer_id)\n",
        "bus_number_MB = sorted([worksheet.title for worksheet in Odometer_spreadsheet_MB])\n",
        "spreadsheet_name_MB = Odometer_spreadsheet_MB.title\n",
        "start_row_mb = start_row_rt + len(bus_number_RT)\n",
        "\n",
        "# Process Diglipur data\n",
        "DP_Odometer_id = OdometersheetID.acell('F2').value\n",
        "Odometer_spreadsheet_DP = gc.open_by_key(DP_Odometer_id)\n",
        "bus_number_DP = sorted([worksheet.title for worksheet in Odometer_spreadsheet_DP])\n",
        "spreadsheet_name_DP = Odometer_spreadsheet_DP.title\n",
        "start_row_dp = start_row_mb + len(bus_number_MB)\n",
        "\n",
        "# === UPDATE ODOMETERSHEERTID WITH BUS NUMBERS ===\n",
        "\n",
        "# Function to update OdometersheetID column with bus numbers - one by one to avoid quote issues\n",
        "def update_column_values(worksheet, column_letter, start_row, values_list):\n",
        "    for i, value in enumerate(values_list):\n",
        "        row = start_row + i\n",
        "        cell = f\"{column_letter}{row}\"\n",
        "        worksheet.update_acell(cell, value)\n",
        "\n",
        "    print(f\"Updated {len(values_list)} values in column {column_letter}\")\n",
        "\n",
        "# Function for batch update with string pre-processing to avoid quotes\n",
        "def batch_update_with_preprocessing(worksheet, column_letter, start_row, values_list):\n",
        "    # Create a list of dictionaries for batch update\n",
        "    requests = []\n",
        "    for i, value in enumerate(values_list):\n",
        "        row = start_row + i\n",
        "        # If value is numeric string, try to convert to number\n",
        "        try:\n",
        "            if value.isdigit():\n",
        "                value = int(value)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        requests.append({\n",
        "            'range': f'{column_letter}{row}',\n",
        "            'values': [[value]]\n",
        "        })\n",
        "\n",
        "    if requests:\n",
        "        worksheet.batch_update(requests)\n",
        "        print(f\"Batch updated {len(values_list)} values in column {column_letter}\")\n",
        "\n",
        "# Update spreadsheet names in row 4\n",
        "OdometersheetID.update_acell('A4', spreadsheet_name_SVP)\n",
        "OdometersheetID.update_acell('B4', spreadsheet_name_FG)\n",
        "OdometersheetID.update_acell('C4', spreadsheet_name_BT)\n",
        "OdometersheetID.update_acell('D4', spreadsheet_name_RT)\n",
        "OdometersheetID.update_acell('E4', spreadsheet_name_MB)\n",
        "OdometersheetID.update_acell('F4', spreadsheet_name_DP)\n",
        "print(\"Updated all spreadsheet names in row 4\")\n",
        "\n",
        "# Use either individual updates or batch updates with preprocessing\n",
        "USE_BATCH = True  # Set to False if you want individual cell updates\n",
        "\n",
        "if USE_BATCH:\n",
        "    # Update OdometersheetID columns in batches\n",
        "    batch_update_with_preprocessing(OdometersheetID, 'A', 5, bus_number_SVP)\n",
        "    batch_update_with_preprocessing(OdometersheetID, 'B', 5, bus_number_FG)\n",
        "    batch_update_with_preprocessing(OdometersheetID, 'C', 5, bus_number_BT)\n",
        "    batch_update_with_preprocessing(OdometersheetID, 'D', 5, bus_number_RT)\n",
        "    batch_update_with_preprocessing(OdometersheetID, 'E', 5, bus_number_MB)\n",
        "    batch_update_with_preprocessing(OdometersheetID, 'F', 5, bus_number_DP)\n",
        "else:\n",
        "    # Update OdometersheetID columns one by one\n",
        "    update_column_values(OdometersheetID, 'A', 5, bus_number_SVP)\n",
        "    update_column_values(OdometersheetID, 'B', 5, bus_number_FG)\n",
        "    update_column_values(OdometersheetID, 'C', 5, bus_number_BT)\n",
        "    update_column_values(OdometersheetID, 'D', 5, bus_number_RT)\n",
        "    update_column_values(OdometersheetID, 'E', 5, bus_number_MB)\n",
        "    update_column_values(OdometersheetID, 'F', 5, bus_number_DP)\n",
        "\n",
        "# === UPDATE REPORT1 WORKSHEET WITH ALL BUS NUMBERS ===\n",
        "\n",
        "# Combine all bus lists for Report1\n",
        "all_buses_with_sources = []\n",
        "for bus, source in zip(bus_number_SVP, [spreadsheet_name_SVP] * len(bus_number_SVP)):\n",
        "    all_buses_with_sources.append((bus, source))\n",
        "for bus, source in zip(bus_number_FG, [spreadsheet_name_FG] * len(bus_number_FG)):\n",
        "    all_buses_with_sources.append((bus, source))\n",
        "for bus, source in zip(bus_number_BT, [spreadsheet_name_BT] * len(bus_number_BT)):\n",
        "    all_buses_with_sources.append((bus, source))\n",
        "for bus, source in zip(bus_number_RT, [spreadsheet_name_RT] * len(bus_number_RT)):\n",
        "    all_buses_with_sources.append((bus, source))\n",
        "for bus, source in zip(bus_number_MB, [spreadsheet_name_MB] * len(bus_number_MB)):\n",
        "    all_buses_with_sources.append((bus, source))\n",
        "for bus, source in zip(bus_number_DP, [spreadsheet_name_DP] * len(bus_number_DP)):\n",
        "    all_buses_with_sources.append((bus, source))\n",
        "\n",
        "# Update Report1 with all bus data\n",
        "if USE_BATCH:\n",
        "    # Create batch update requests\n",
        "    requests = []\n",
        "    for i, (bus, source) in enumerate(all_buses_with_sources):\n",
        "        row = i + 2  # Start from row 2\n",
        "        # Try to convert numeric strings to integers\n",
        "        try:\n",
        "            if isinstance(bus, str) and bus.isdigit():\n",
        "                bus = int(bus)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        requests.append({\n",
        "            'range': f'B{row}',\n",
        "            'values': [[bus]]\n",
        "        })\n",
        "        requests.append({\n",
        "            'range': f'AC{row}',\n",
        "            'values': [[source]]\n",
        "        })\n",
        "\n",
        "    if requests:\n",
        "        RR_Report1_Worksheet.batch_update(requests)\n",
        "        print(f\"Batch updated {len(all_buses_with_sources)} bus entries in Report1 worksheet\")\n",
        "else:\n",
        "    # Update one by one\n",
        "    for i, (bus, source) in enumerate(all_buses_with_sources):\n",
        "        row = i + 2  # Start from row 2\n",
        "        RR_Report1_Worksheet.update_cell(row, 2, bus)  # Column B\n",
        "        RR_Report1_Worksheet.update_cell(row, 29, source)  # Column AC\n",
        "\n",
        "    print(f\"Updated {len(all_buses_with_sources)} bus entries in Report1 worksheet\")\n",
        "\n",
        "print(\"All operations completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPYzQZdOPQOE",
        "outputId": "1afa67f1-d5da-465c-f1b1-7f1bb34b56d8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated all spreadsheet names in row 4\n",
            "Batch updated 55 values in column A\n",
            "Batch updated 20 values in column B\n",
            "Batch updated 1 values in column C\n",
            "Batch updated 1 values in column D\n",
            "Batch updated 1 values in column E\n",
            "Batch updated 1 values in column F\n",
            "Batch updated 79 bus entries in Report1 worksheet\n",
            "All operations completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deepseek Revision ongoing in CODE #112 KM in Column D in Report1 Sheet, here it is searching all Odometer, i.e. from Odometer_spreadsheet_SVP or Odometer_spreadsheet_FG or Odometer_spreadsheet_BT or Odometer_spreadsheet_RT or Odometer_spreadsheet_MB or Odometer_spreadsheet_DP\n",
        "# Working fine\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# CONFIGURATION\n",
        "odometer_spreadsheets = [Odometer_spreadsheet_SVP, Odometer_spreadsheet_FG, Odometer_spreadsheet_BT,\n",
        "                         Odometer_spreadsheet_RT, Odometer_spreadsheet_MB, Odometer_spreadsheet_DP]\n",
        "spreadsheet_names = [\"SVP\", \"FG\", \"BT\", \"RT\", \"MB\", \"DP\"]\n",
        "\n",
        "# Clear columns C and D\n",
        "RR_Report1_Worksheet.batch_clear([\"C2:D\"])\n",
        "\n",
        "# Get current date in expected format (e.g., \"05,May25\")\n",
        "current_date_str = datetime.now().strftime('%d,%b%y')\n",
        "\n",
        "# Get main data from Report1\n",
        "main_data = RR_Report1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus + date (using today's date)\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        bus = re.sub(r'[^a-zA-Z0-9]', '', row[1].strip()).upper()  # Clean bus name\n",
        "        if bus:\n",
        "            row_map[i] = (bus, current_date_str)\n",
        "            bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize lookup maps\n",
        "bus_date_value_map = {}\n",
        "match_type_map = {}\n",
        "fallback_date_map = {}\n",
        "source_spreadsheet_map = {}\n",
        "\n",
        "# Function to find closest earlier date value\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    try:\n",
        "        target_date = datetime.strptime(target_date_str, '%d,%b%y')\n",
        "        valid_rows = []\n",
        "        for row_idx, row in enumerate(sheet_data[1:], start=2):\n",
        "            if len(row) <= value_column_index:\n",
        "                continue\n",
        "            date_cell = row[1].strip()\n",
        "            if not date_cell:\n",
        "                continue\n",
        "            # Try multiple date formats\n",
        "            try:\n",
        "                row_date = datetime.strptime(date_cell, '%d,%b%y')\n",
        "            except:\n",
        "                try:\n",
        "                    row_date = datetime.strptime(date_cell, '%d-%b-%y')\n",
        "                except:\n",
        "                    continue\n",
        "            if row_date <= target_date and row[value_column_index].strip():\n",
        "                valid_rows.append((row_date, date_cell, row[value_column_index].strip()))\n",
        "        if valid_rows:\n",
        "            valid_rows.sort(reverse=True)  # Newest date first\n",
        "            best = valid_rows[0]\n",
        "            return best[2], best[1]  # value, fallback_date\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error in fallback for {target_date_str}: {e}\")\n",
        "    return None, \"\"\n",
        "\n",
        "# Search logic with enhanced logging\n",
        "for bus in bus_set:\n",
        "    found = False\n",
        "    for idx, spreadsheet in enumerate(odometer_spreadsheets):\n",
        "        try:\n",
        "            time.sleep(1)  # Avoid API throttling\n",
        "            sheet = spreadsheet.worksheet(bus)\n",
        "            data = sheet.get_all_values()\n",
        "            print(f\"\\n✅ Found bus {bus} in {spreadsheet_names[idx]}\")\n",
        "            found = True\n",
        "            # Check for exact date match\n",
        "            exact_match = False\n",
        "            for row in data[1:]:\n",
        "                if len(row) > 12 and row[1].strip() == current_date_str and row[12].strip():\n",
        "                    odometer_val = row[12].strip()\n",
        "                    if odometer_val.isdigit():  # Validate numeric\n",
        "                        bus_date_value_map[(bus, current_date_str)] = odometer_val\n",
        "                        match_type_map[(bus, current_date_str)] = \"exact\"\n",
        "                        fallback_date_map[(bus, current_date_str)] = \"\"\n",
        "                        source_spreadsheet_map[(bus, current_date_str)] = spreadsheet_names[idx]\n",
        "                        exact_match = True\n",
        "                        print(f\"📌 Exact match: {odometer_val} km on {current_date_str}\")\n",
        "                        break\n",
        "            if not exact_match:\n",
        "                # Fallback to closest date\n",
        "                val, fallback_date = find_closest_date_value(data, current_date_str, 12)\n",
        "                if val:\n",
        "                    bus_date_value_map[(bus, current_date_str)] = val\n",
        "                    match_type_map[(bus, current_date_str)] = \"fallback\"\n",
        "                    fallback_date_map[(bus, current_date_str)] = fallback_date\n",
        "                    source_spreadsheet_map[(bus, current_date_str)] = spreadsheet_names[idx]\n",
        "                    print(f\"🔄 Fallback match: {val} km on {fallback_date}\")\n",
        "            break  # Stop searching if found\n",
        "        except Exception as e:\n",
        "            if \"not found\" in str(e):\n",
        "                print(f\"❌ Bus {bus} not in {spreadsheet_names[idx]}\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"⚠️ Error accessing {bus} in {spreadsheet_names[idx]}: {e}\")\n",
        "    if not found:\n",
        "        print(f\"🚨 Bus {bus} not found in ANY spreadsheet!\")\n",
        "\n",
        "# Update Report1 with logging\n",
        "print(\"\\n📝 Writing data to Report1...\")\n",
        "updated_rows = []\n",
        "for row_idx, (bus, date_str) in row_map.items():\n",
        "    key = (bus, date_str)\n",
        "    val = bus_date_value_map.get(key, \"\")\n",
        "    fallback = fallback_date_map.get(key, \"\")\n",
        "    match_type = match_type_map.get(key, \"\")\n",
        "    source = source_spreadsheet_map.get(key, \"\")\n",
        "\n",
        "    # Build updated values for Column C and D\n",
        "    col_c = fallback if match_type == \"fallback\" else (source if match_type == \"exact\" else \"\")\n",
        "    col_d = val if val else \"\"\n",
        "\n",
        "    # Log updates for verification\n",
        "    if not val:\n",
        "        print(f\"⚠️ No data for Bus {bus} | Date {date_str}\")\n",
        "\n",
        "    updated_rows.append([col_c, col_d])\n",
        "\n",
        "# Batch update\n",
        "if updated_rows:\n",
        "    range_to_update = f\"C2:D{len(updated_rows) + 1}\"\n",
        "    RR_Report1_Worksheet.update(range_to_update, updated_rows)\n",
        "    print(\"✅ Report1 updated successfully!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report1.\")"
      ],
      "metadata": {
        "id": "HVEl5unftQaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4046ed-5b8c-4a7e-d913-b34483cbe193"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Total buses to process: 79\n",
            "Sample buses: ['514', '604', '588', '593', '469']\n",
            "\n",
            "✅ Found bus 514 in SVP\n",
            "🔄 Fallback match: 181869 km on 06,May25\n",
            "\n",
            "✅ Found bus 604 in SVP\n",
            "🔄 Fallback match: 114004 km on 04,May25\n",
            "\n",
            "✅ Found bus 588 in SVP\n",
            "🔄 Fallback match: 285148 km on 05,May25\n",
            "⚠️ Error accessing 593 in SVP: 593\n",
            "\n",
            "✅ Found bus 593 in FG\n",
            "🔄 Fallback match: 393139 km on 07,May25\n",
            "\n",
            "✅ Found bus 469 in SVP\n",
            "🔄 Fallback match: 236807 km on 29,Apr25\n",
            "\n",
            "✅ Found bus 495 in SVP\n",
            "🔄 Fallback match: 583802 km on 05,May25\n",
            "\n",
            "✅ Found bus 476 in SVP\n",
            "🔄 Fallback match: 387688 km on 06,May25\n",
            "\n",
            "✅ Found bus 521 in SVP\n",
            "🔄 Fallback match: 307305 km on 06,May25\n",
            "⚠️ Error accessing 529 in SVP: 529\n",
            "\n",
            "✅ Found bus 529 in FG\n",
            "🔄 Fallback match: 360784 km on 18,Feb25\n",
            "⚠️ Error accessing 592 in SVP: 592\n",
            "\n",
            "✅ Found bus 592 in FG\n",
            "🔄 Fallback match: 368608 km on 07,May25\n",
            "\n",
            "✅ Found bus 563 in SVP\n",
            "🔄 Fallback match: 731725 km on 30,Apr25\n",
            "\n",
            "✅ Found bus 511 in SVP\n",
            "🔄 Fallback match: 249770 km on 21,Apr25\n",
            "\n",
            "✅ Found bus 442 in SVP\n",
            "🔄 Fallback match: 276687 km on 02,May25\n",
            "\n",
            "✅ Found bus 538 in SVP\n",
            "🔄 Fallback match: 340969 km on 30,Apr25\n",
            "\n",
            "✅ Found bus 590 in SVP\n",
            "🔄 Fallback match: 434243 km on 01,May25\n",
            "\n",
            "✅ Found bus 595 in SVP\n",
            "🔄 Fallback match: 278700 km on 03,May25\n",
            "⚠️ Error accessing 560 in SVP: 560\n",
            "\n",
            "✅ Found bus 560 in FG\n",
            "🔄 Fallback match: 579745 km on 07,May25\n",
            "\n",
            "✅ Found bus 575 in SVP\n",
            "🔄 Fallback match: 391243 km on 13,Nov24\n",
            "\n",
            "✅ Found bus 599 in SVP\n",
            "🔄 Fallback match: 99371 km on 04,May25\n",
            "\n",
            "✅ Found bus 441 in SVP\n",
            "🔄 Fallback match: 278164 km on 12,Apr25\n",
            "\n",
            "✅ Found bus 444 in SVP\n",
            "🔄 Fallback match: 302591 km on 05,May25\n",
            "\n",
            "✅ Found bus 585 in SVP\n",
            "🔄 Fallback match: 358826 km on 04,May25\n",
            "\n",
            "✅ Found bus 534 in SVP\n",
            "🔄 Fallback match: 308573 km on 05,May25\n",
            "\n",
            "✅ Found bus 475 in SVP\n",
            "🔄 Fallback match: 327218 km on 12,Apr25\n",
            "\n",
            "✅ Found bus 459 in SVP\n",
            "🔄 Fallback match: 158773 km on 30,Apr25\n",
            "\n",
            "✅ Found bus 577 in SVP\n",
            "🔄 Fallback match: 613183 km on 03,May25\n",
            "\n",
            "✅ Found bus 486 in SVP\n",
            "🔄 Fallback match: 379760 km on 28,Mar25\n",
            "\n",
            "✅ Found bus 603 in SVP\n",
            "🔄 Fallback match: 97763 km on 05,May25\n",
            "\n",
            "✅ Found bus 612 in SVP\n",
            "🔄 Fallback match: 60829 km on 06,May25\n",
            "\n",
            "✅ Found bus 528 in SVP\n",
            "🔄 Fallback match: 302453 km on 25,Apr25\n",
            "\n",
            "✅ Found bus 530 in SVP\n",
            "🔄 Fallback match: 398042 km on 06,May25\n",
            "⚠️ Error accessing 516 in SVP: 516\n",
            "\n",
            "✅ Found bus 516 in FG\n",
            "🔄 Fallback match: 288876 km on 07,May25\n",
            "\n",
            "✅ Found bus 547 in SVP\n",
            "🔄 Fallback match: 702008 km on 04,May25\n",
            "\n",
            "✅ Found bus 457 in SVP\n",
            "🔄 Fallback match: 410880 km on 03,May25\n",
            "\n",
            "✅ Found bus 548 in SVP\n",
            "🔄 Fallback match: 586199 km on 05,May25\n",
            "⚠️ Error accessing 616 in SVP: 616\n",
            "\n",
            "✅ Found bus 616 in FG\n",
            "🔄 Fallback match: 70959 km on 06,May25\n",
            "\n",
            "✅ Found bus 440 in SVP\n",
            "🔄 Fallback match: 259274 km on 23,Apr25\n",
            "\n",
            "✅ Found bus 596 in SVP\n",
            "🔄 Fallback match: 95560 km on 04,May25\n",
            "⚠️ Error accessing 479 in SVP: 479\n",
            "\n",
            "✅ Found bus 479 in FG\n",
            "🔄 Fallback match: 424549 km on 07,May25\n",
            "⚠️ Error accessing 493 in SVP: 493\n",
            "\n",
            "✅ Found bus 493 in FG\n",
            "🔄 Fallback match: 284199 km on 01,May25\n",
            "⚠️ Error accessing 437 in SVP: 437\n",
            "\n",
            "✅ Found bus 437 in FG\n",
            "🔄 Fallback match: 250488 km on 30,Apr25\n",
            "⚠️ Error accessing 478 in SVP: 478\n",
            "\n",
            "✅ Found bus 478 in FG\n",
            "🔄 Fallback match: 289372 km on 07,May25\n",
            "\n",
            "✅ Found bus 473 in SVP\n",
            "🔄 Fallback match: 335615 km on 31,Jan25\n",
            "\n",
            "✅ Found bus 515 in SVP\n",
            "🔄 Fallback match: 474742 km on 14,Feb25\n",
            "\n",
            "✅ Found bus 574 in SVP\n",
            "🔄 Fallback match: 548919 km on 02,Dec24\n",
            "⚠️ Error accessing 615 in SVP: 615\n",
            "\n",
            "✅ Found bus 615 in FG\n",
            "🔄 Fallback match: 58303 km on 30,Apr25\n",
            "⚠️ Error accessing 980 in SVP: 980\n",
            "⚠️ Error accessing 980 in FG: 980\n",
            "⚠️ Error accessing 980 in BT: 980\n",
            "⚠️ Error accessing 980 in RT: 980\n",
            "\n",
            "✅ Found bus 980 in MB\n",
            "🔄 Fallback match: 361040 km on 05,Apr25\n",
            "\n",
            "✅ Found bus 578 in SVP\n",
            "🔄 Fallback match: 471325 km on 02,May25\n",
            "\n",
            "✅ Found bus 614 in SVP\n",
            "🔄 Fallback match: 59720 km on 05,May25\n",
            "\n",
            "✅ Found bus 474 in SVP\n",
            "🔄 Fallback match: 373312 km on 28,Apr25\n",
            "⚠️ Error accessing 572 in SVP: 572\n",
            "\n",
            "✅ Found bus 572 in FG\n",
            "🔄 Fallback match: 463993 km on 07,May25\n",
            "\n",
            "✅ Found bus 587 in SVP\n",
            "🔄 Fallback match: 295258 km on 05,May25\n",
            "⚠️ Error accessing 561 in SVP: 561\n",
            "\n",
            "✅ Found bus 561 in FG\n",
            "🔄 Fallback match: 569960 km on 07,May25\n",
            "\n",
            "✅ Found bus 472 in SVP\n",
            "🔄 Fallback match: 395371 km on 06,May25\n",
            "\n",
            "✅ Found bus 471 in SVP\n",
            "🔄 Fallback match: 401839 km on 06,May25\n",
            "\n",
            "✅ Found bus 546 in SVP\n",
            "🔄 Fallback match: 532369 km on 05,May25\n",
            "\n",
            "✅ Found bus 550 in SVP\n",
            "🔄 Fallback match: 680912 km on 14,Oct24\n",
            "⚠️ Error accessing 999 in SVP: 999\n",
            "⚠️ Error accessing 999 in FG: 999\n",
            "\n",
            "✅ Found bus 999 in BT\n",
            "🔄 Fallback match: 345158 km on 29,Mar25\n",
            "\n",
            "✅ Found bus 598 in SVP\n",
            "🔄 Fallback match: 113045 km on 03,May25\n",
            "⚠️ Error accessing 573 in SVP: 573\n",
            "\n",
            "✅ Found bus 573 in FG\n",
            "🔄 Fallback match: 538881 km on 07,May25\n",
            "⚠️ Error accessing 461 in SVP: 461\n",
            "\n",
            "✅ Found bus 461 in FG\n",
            "🔄 Fallback match: 331240 km on 07,May25\n",
            "\n",
            "✅ Found bus 597 in SVP\n",
            "🔄 Fallback match: 121187 km on 02,May25\n",
            "\n",
            "✅ Found bus 536 in SVP\n",
            "🔄 Fallback match: 327453 km on 30,Apr25\n",
            "\n",
            "✅ Found bus 589 in SVP\n",
            "🔄 Fallback match: 197762 km on 04,Mar25\n",
            "⚠️ Error accessing 990 in SVP: 990\n",
            "⚠️ Error accessing 990 in FG: 990\n",
            "⚠️ Error accessing 990 in BT: 990\n",
            "\n",
            "✅ Found bus 990 in RT\n",
            "🔄 Fallback match: 278164 km on 03,Mar25\n",
            "⚠️ Error accessing 463 in SVP: 463\n",
            "\n",
            "✅ Found bus 463 in FG\n",
            "🔄 Fallback match: 254649 km on 08,May25\n",
            "⚠️ Error accessing 970 in SVP: 970\n",
            "⚠️ Error accessing 970 in FG: 970\n",
            "⚠️ Error accessing 970 in BT: 970\n",
            "⚠️ Error accessing 970 in RT: 970\n",
            "⚠️ Error accessing 970 in MB: 970\n",
            "\n",
            "✅ Found bus 970 in DP\n",
            "🔄 Fallback match: 383845 km on 29,Mar25\n",
            "\n",
            "✅ Found bus 504 in SVP\n",
            "🔄 Fallback match: 378130 km on 30,Apr25\n",
            "⚠️ Error accessing 562 in SVP: 562\n",
            "\n",
            "✅ Found bus 562 in FG\n",
            "🔄 Fallback match: 590195 km on 07,May25\n",
            "\n",
            "✅ Found bus 508 in SVP\n",
            "🔄 Fallback match: 379571 km on 30,Apr25\n",
            "⚠️ Error accessing 617 in SVP: 617\n",
            "\n",
            "✅ Found bus 617 in FG\n",
            "🔄 Fallback match: 66479 km on 07,May25\n",
            "\n",
            "✅ Found bus 535 in SVP\n",
            "🔄 Fallback match: 347321 km on 30,Apr25\n",
            "\n",
            "✅ Found bus 523 in SVP\n",
            "🔄 Fallback match: 281016 km on 23,Apr25\n",
            "⚠️ Error accessing 507 in SVP: 507\n",
            "\n",
            "✅ Found bus 507 in FG\n",
            "🔄 Fallback match: 399437 km on 07,May25\n",
            "\n",
            "✅ Found bus 613 in SVP\n",
            "🔄 Fallback match: 53915 km on 17,Mar25\n",
            "\n",
            "✅ Found bus 462 in SVP\n",
            "🔄 Fallback match: 394278 km on 11,Apr25\n",
            "⚠️ Error accessing 438 in SVP: 438\n",
            "\n",
            "✅ Found bus 438 in FG\n",
            "🔄 Fallback match: 318057 km on 07,May25\n",
            "\n",
            "✅ Found bus 460 in SVP\n",
            "🔄 Fallback match: 367402 km on 06,Feb25\n",
            "\n",
            "✅ Found bus 605 in SVP\n",
            "🔄 Fallback match: 97548 km on 30,Apr25\n",
            "\n",
            "📝 Writing data to Report1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-b1d2be683ee7>:140: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  RR_Report1_Worksheet.update(range_to_update, updated_rows)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Report1 updated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Modifications Needed\n",
        "Multiple Target Worksheets:\n",
        "\n",
        "engine_oil_CH_BSIV_CH_MAS_rr (existing)\n",
        "engine_oil_CK_BSVI_CK_MAS_rr (new)\n",
        "engine_oil_CK_BSVI_CK_MAS_ATR (new)\n",
        "engine_oil_CH_BSIV_CH_MAS_ATR (new)\n",
        "\n",
        "Multiple Source Spreadsheets:\n",
        "\n",
        "Odometer_spreadsheet_SVP (existing)\n",
        "Odometer_spreadsheet_FG (new)\n",
        "Odometer_spreadsheet_BT (new)\n",
        "Odometer_spreadsheet_RT (new)\n",
        "Odometer_spreadsheet_MB (new)\n",
        "Odometer_spreadsheet_DP (new)"
      ],
      "metadata": {
        "id": "wvMmprUNKW7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple engine oil worksheets SVP CH CK, ATR CH CK, Added batch processing done\n",
        "\n",
        "# Modified to process multiple engine oil worksheets with batch processing\n",
        "# With fallback date display in M Column, For ATR_MAS_Engine Oil\n",
        "\n",
        "# Make sure to include these imports at the top of your code\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_ATR',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_ATR',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_rr',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_rr'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Function to process each worksheet\n",
        "def process_worksheet(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "    clear_range = 'L2:M'\n",
        "    print(f\"🧹 Clearing range {clear_range} in {worksheet_name}...\")\n",
        "    worksheet.batch_clear([clear_range])\n",
        "\n",
        "    print(\"🔍 Starting data processing with date fallback search...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping\n",
        "    print(\"🔢 Mapping bus numbers and dates...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            date_str = row[1].strip()\n",
        "            bus_num = row[4].strip()\n",
        "            if date_str and bus_num:\n",
        "                bus_date_map[bus_num].add(date_str)\n",
        "                row_map[i] = (bus_num, date_str)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers\")\n",
        "    print(f\"✅ Found {len(row_map)} rows to process\")\n",
        "\n",
        "    # Create a cache for worksheet data to avoid redundant fetches\n",
        "    bus_worksheet_cache = {}\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Function to process a single bus\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "\n",
        "        try:\n",
        "            # Check if we already have this bus's data in cache\n",
        "            if bus in bus_worksheet_cache:\n",
        "                sheet_data = bus_worksheet_cache[bus]\n",
        "            else:\n",
        "                # Fetch the bus worksheet data and store in cache\n",
        "                sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "                sheet_data = sheet.get_all_values()\n",
        "                bus_worksheet_cache[bus] = sheet_data\n",
        "\n",
        "            # Create a dictionary for faster exact date lookups\n",
        "            date_value_lookup = {}\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12:\n",
        "                        date_str_key = row[1].strip()\n",
        "                        date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for date_str in dates:\n",
        "                # Check for exact match using the lookup dictionary\n",
        "                exact_value = date_value_lookup.get(date_str)\n",
        "\n",
        "                if exact_value:\n",
        "                    results.append((bus, date_str, exact_value, \"exact\", \"\"))\n",
        "                    sheet_success += 1\n",
        "                else:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date))\n",
        "                        sheet_success += 1\n",
        "\n",
        "            print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Process buses in parallel using ThreadPoolExecutor\n",
        "    print(f\"🚀 Processing {len(bus_date_map)} buses in parallel...\")\n",
        "\n",
        "    # Convert to list for ThreadPoolExecutor\n",
        "    bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    # Adjust max_workers based on your system's capabilities\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "    # Process all results\n",
        "    for bus_results in all_results:\n",
        "        for bus, date_str, value, match_type, fallback_date in bus_results:\n",
        "            bus_date_value_map[(bus, date_str)] = value\n",
        "            match_type_map[(bus, date_str)] = match_type\n",
        "            fallback_date_map[(bus, date_str)] = fallback_date\n",
        "\n",
        "            if match_type == \"exact\":\n",
        "                success_count += 1\n",
        "            else:\n",
        "                fallback_count += 1\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 50  # Adjust based on API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update\n",
        "                worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 2  # Divide by 2 because we have 2 cells per row\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//2} rows updated\")\n",
        "\n",
        "                # Add a small delay between chunks to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in column L and column M\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Main execution\n",
        "print(\"🚀 Starting to process all engine oil worksheets...\")\n",
        "\n",
        "# Create a global cache for bus worksheets to reuse across all worksheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Process each worksheet in sequence\n",
        "for worksheet_name in engine_oil_worksheets:\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name}\\n{'='*80}\")\n",
        "    process_worksheet(worksheet_name)\n",
        "    # Add a delay between worksheets to avoid rate limiting\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with batch processing\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")\n"
      ],
      "metadata": {
        "id": "7POcdIrOaQjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lean version - Only processes rows with empty L column\n",
        "# With fallback date display in M Column, For SVP and RR and ATR, CH and CK MAS\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_ATR',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_ATR',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_rr',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_rr'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Global cache for bus worksheets to reuse across all worksheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Function to process each worksheet - LEAN VERSION (only empty L cells)\n",
        "def process_worksheet_lean(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # NO CLEARING OF RANGES - we're only updating empty cells\n",
        "    print(\"🔍 Starting data processing for EMPTY L cells only...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping ONLY FOR EMPTY L CELLS\n",
        "    print(\"🔢 Mapping bus numbers and dates for empty L cells...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "    empty_cell_count = 0\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            # Check if L column (index 11) is empty\n",
        "            l_value = row[11].strip() if len(row) > 11 else \"\"\n",
        "\n",
        "            if not l_value:  # Only process if L column is empty\n",
        "                date_str = row[1].strip()\n",
        "                bus_num = row[4].strip()\n",
        "                if date_str and bus_num:\n",
        "                    bus_date_map[bus_num].add(date_str)\n",
        "                    row_map[i] = (bus_num, date_str)\n",
        "                    empty_cell_count += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers with empty L cells\")\n",
        "    print(f\"✅ Found {empty_cell_count} empty L cells to process\")\n",
        "\n",
        "    # Exit early if no empty cells to process\n",
        "    if empty_cell_count == 0:\n",
        "        print(f\"✓ No empty L cells to process in {worksheet_name}. Moving to next worksheet.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Function to process a single bus\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "\n",
        "        try:\n",
        "            # Check if we already have this bus's data in cache\n",
        "            if bus in global_bus_worksheet_cache:\n",
        "                sheet_data = global_bus_worksheet_cache[bus]\n",
        "            else:\n",
        "                # Fetch the bus worksheet data and store in cache\n",
        "                sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "                sheet_data = sheet.get_all_values()\n",
        "                global_bus_worksheet_cache[bus] = sheet_data\n",
        "\n",
        "            # Create a dictionary for faster exact date lookups\n",
        "            date_value_lookup = {}\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12:\n",
        "                        date_str_key = row[1].strip()\n",
        "                        date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for date_str in dates:\n",
        "                # Check for exact match using the lookup dictionary\n",
        "                exact_value = date_value_lookup.get(date_str)\n",
        "\n",
        "                if exact_value:\n",
        "                    results.append((bus, date_str, exact_value, \"exact\", \"\"))\n",
        "                    sheet_success += 1\n",
        "                else:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date))\n",
        "                        sheet_success += 1\n",
        "\n",
        "            print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Only process if we have buses to check\n",
        "    if len(bus_date_map) > 0:\n",
        "        # Convert to list for ThreadPoolExecutor\n",
        "        bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        max_workers = min(5, len(bus_dates_list))  # Adjust based on data size\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "        # Process all results\n",
        "        for bus_results in all_results:\n",
        "            for bus, date_str, value, match_type, fallback_date in bus_results:\n",
        "                bus_date_value_map[(bus, date_str)] = value\n",
        "                match_type_map[(bus, date_str)] = match_type\n",
        "                fallback_date_map[(bus, date_str)] = fallback_date\n",
        "\n",
        "                if match_type == \"exact\":\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    fallback_count += 1\n",
        "    else:\n",
        "        print(\"⚠️ No buses to process - all L cells have values already\")\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Skip if no updates needed\n",
        "    if len(batch_updates) == 0:\n",
        "        print(\"✓ No updates needed for this worksheet\")\n",
        "        return\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 50  # Adjust based on API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update\n",
        "                worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 2  # Divide by 2 because we have 2 cells per row\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//2} rows updated\")\n",
        "\n",
        "                # Add a small delay between chunks to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in column L and column M\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count*2} rows could not be updated\")\n",
        "\n",
        "# Main execution - LEAN VERSION\n",
        "print(\"🚀 Starting LEAN processing - only empty L cells...\")\n",
        "\n",
        "# Process each worksheet in sequence\n",
        "for worksheet_name in engine_oil_worksheets:\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name}\\n{'='*80}\")\n",
        "    process_worksheet_lean(worksheet_name)\n",
        "    # Add a delay between worksheets to avoid rate limiting\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with LEAN mode (empty L cells only)\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")"
      ],
      "metadata": {
        "id": "DgZINOj9Gzh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below codes are redundent, No need to run, Use this If required in future"
      ],
      "metadata": {
        "id": "Uq1Go9jLsRov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "donb8KLRgWKg"
      },
      "outputs": [],
      "source": [
        "# RR_MAS_Engine OIl_ Best Code\n",
        "\n",
        "# Make sure to include these imports at the top of your code\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "\n",
        "\n",
        "# Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "clear_range_all_engine_oil_CH_BSIV_CH_MAS_rr = 'L2:M'\n",
        "engine_oil_CH_BSIV_CH_MAS_rr.batch_clear([clear_range_all_engine_oil_CH_BSIV_CH_MAS_rr])\n",
        "\n",
        "\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "print(\"🔍 Starting data processing with date fallback search...\")\n",
        "\n",
        "# 📥 Load main sheet\n",
        "print(\"📥 Loading main sheet data...\")\n",
        "main_data = engine_oil_CH_BSIV_CH_MAS_rr.get_all_values()\n",
        "print(f\"✅ Loaded {len(main_data)-1} rows from main sheet\")\n",
        "\n",
        "# Step 1: Collect unique (bus, date) and row mapping\n",
        "print(\"🔢 Mapping bus numbers and dates...\")\n",
        "bus_date_map = defaultdict(set)\n",
        "row_map = {}  # maps row number → (bus, date)\n",
        "\n",
        "for i, row in enumerate(main_data[1:], start=2):\n",
        "    try:\n",
        "        date_str = row[1].strip()\n",
        "        bus_num = row[4].strip()\n",
        "        if date_str and bus_num:\n",
        "            bus_date_map[bus_num].add(date_str)\n",
        "            row_map[i] = (bus_num, date_str)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"✅ Found {len(bus_date_map)} unique bus numbers\")\n",
        "print(f\"✅ Found {len(row_map)} rows to process\")\n",
        "\n",
        "# Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "bus_date_value_map = {}\n",
        "match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "fallback_date_map = {}  # Store the actual fallback date used\n",
        "success_count = 0\n",
        "fallback_count = 0\n",
        "\n",
        "for bus, dates in bus_date_map.items():\n",
        "    print(f\"📊 Processing bus {bus} ({len(dates)} dates)...\")\n",
        "    try:\n",
        "        sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "        sheet_data = sheet.get_all_values()\n",
        "        sheet_success = 0\n",
        "\n",
        "        for date_str in dates:\n",
        "            # Find exact match first\n",
        "            exact_value = None\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12 and row[1].strip() == date_str:\n",
        "                        exact_value = row[12]  # Column M\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if exact_value:\n",
        "                bus_date_value_map[(bus, date_str)] = exact_value\n",
        "                match_type_map[(bus, date_str)] = \"exact\"\n",
        "                fallback_date_map[(bus, date_str)] = \"\"  # No fallback date for exact matches\n",
        "                sheet_success += 1\n",
        "                success_count += 1\n",
        "            else:\n",
        "                # Try to find closest earlier date\n",
        "                value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                if value:\n",
        "                    bus_date_value_map[(bus, date_str)] = value\n",
        "                    match_type_map[(bus, date_str)] = \"fallback\"\n",
        "                    fallback_date_map[(bus, date_str)] = fallback_date  # Store the fallback date used\n",
        "                    sheet_success += 1\n",
        "                    fallback_count += 1\n",
        "\n",
        "        print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "# Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "print(\"\\n📝 Updating values and formatting in main sheet...\")\n",
        "updated_count = 0\n",
        "\n",
        "# Get the worksheet object for batch updates\n",
        "worksheet = engine_oil_CH_BSIV_CH_MAS_rr\n",
        "\n",
        "# Prepare batch formatting requests for different match types\n",
        "exact_match_cells = []\n",
        "fallback_match_cells = []\n",
        "\n",
        "for row_num, (bus, date_str) in row_map.items():\n",
        "    value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "    fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "    if value:\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Update cell value in column L (value)\n",
        "                worksheet.update_cell(row_num, 12, value)  # Column L = col 12\n",
        "\n",
        "                # Update column M with fallback date if applicable\n",
        "                worksheet.update_cell(row_num, 13, fallback_date)  # Column M = col 13\n",
        "\n",
        "                # Track which cells need which formatting\n",
        "                match_type = match_type_map.get((bus, date_str))\n",
        "                if match_type == \"exact\":\n",
        "                    # For exact matches - normal formatting\n",
        "                    exact_match_cells.append(f\"L{row_num}\")\n",
        "                elif match_type == \"fallback\":\n",
        "                    # For fallback matches - italic and right-aligned\n",
        "                    fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += 1\n",
        "                if updated_count % 10 == 0:  # Progress update every 10 rows\n",
        "                    print(f\"📊 Progress: Updated {updated_count}/{len(bus_date_value_map)} values\")\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    # Calculate wait time with exponential backoff\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update row {row_num}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update row {row_num} after {max_retries} retries\")\n",
        "\n",
        "# Apply formatting in batches to avoid rate limiting\n",
        "print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "\n",
        "print(f\"\\n✅ COMPLETE: Updated {updated_count} rows in column L and column M\")\n",
        "print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ATR_MAS_Engine OIl_ Best Code\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "\n",
        "\n",
        "# Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "clear_range_all_engine_oil_CH_BSIV_CH_MAS_atr = 'L2:M'\n",
        "engine_oil_CH_BSIV_CH_MAS_ATR.batch_clear([clear_range_all_engine_oil_CH_BSIV_CH_MAS_atr])\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "print(\"🔍 Starting data processing with date fallback search...\")\n",
        "\n",
        "# 📥 Load main sheet\n",
        "print(\"📥 Loading main sheet data...\")\n",
        "main_data = engine_oil_CH_BSIV_CH_MAS_ATR.get_all_values()\n",
        "print(f\"✅ Loaded {len(main_data)-1} rows from main sheet\")\n",
        "\n",
        "# Step 1: Collect unique (bus, date) and row mapping\n",
        "print(\"🔢 Mapping bus numbers and dates...\")\n",
        "bus_date_map = defaultdict(set)\n",
        "row_map = {}  # maps row number → (bus, date)\n",
        "\n",
        "for i, row in enumerate(main_data[1:], start=2):\n",
        "    try:\n",
        "        date_str = row[1].strip()\n",
        "        bus_num = row[4].strip()\n",
        "        if date_str and bus_num:\n",
        "            bus_date_map[bus_num].add(date_str)\n",
        "            row_map[i] = (bus_num, date_str)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"✅ Found {len(bus_date_map)} unique bus numbers\")\n",
        "print(f\"✅ Found {len(row_map)} rows to process\")\n",
        "\n",
        "# Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "bus_date_value_map = {}\n",
        "match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "fallback_date_map = {}  # Store the actual fallback date used\n",
        "success_count = 0\n",
        "fallback_count = 0\n",
        "\n",
        "for bus, dates in bus_date_map.items():\n",
        "    print(f\"📊 Processing bus {bus} ({len(dates)} dates)...\")\n",
        "    try:\n",
        "        sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "        sheet_data = sheet.get_all_values()\n",
        "        sheet_success = 0\n",
        "\n",
        "        for date_str in dates:\n",
        "            # Find exact match first\n",
        "            exact_value = None\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12 and row[1].strip() == date_str:\n",
        "                        exact_value = row[12]  # Column M\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if exact_value:\n",
        "                bus_date_value_map[(bus, date_str)] = exact_value\n",
        "                match_type_map[(bus, date_str)] = \"exact\"\n",
        "                fallback_date_map[(bus, date_str)] = \"\"  # No fallback date for exact matches\n",
        "                sheet_success += 1\n",
        "                success_count += 1\n",
        "            else:\n",
        "                # Try to find closest earlier date\n",
        "                value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                if value:\n",
        "                    bus_date_value_map[(bus, date_str)] = value\n",
        "                    match_type_map[(bus, date_str)] = \"fallback\"\n",
        "                    fallback_date_map[(bus, date_str)] = fallback_date  # Store the fallback date used\n",
        "                    sheet_success += 1\n",
        "                    fallback_count += 1\n",
        "\n",
        "        print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "# Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "print(\"\\n📝 Updating values and formatting in main sheet...\")\n",
        "updated_count = 0\n",
        "\n",
        "# Get the worksheet object for batch updates\n",
        "worksheet = engine_oil_CH_BSIV_CH_MAS_ATR\n",
        "\n",
        "# Prepare batch formatting requests for different match types\n",
        "exact_match_cells = []\n",
        "fallback_match_cells = []\n",
        "\n",
        "for row_num, (bus, date_str) in row_map.items():\n",
        "    value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "    fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "    if value:\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Update cell value in column L (value)\n",
        "                worksheet.update_cell(row_num, 12, value)  # Column L = col 12\n",
        "\n",
        "                # Update column M with fallback date if applicable\n",
        "                worksheet.update_cell(row_num, 13, fallback_date)  # Column M = col 13\n",
        "\n",
        "                # Track which cells need which formatting\n",
        "                match_type = match_type_map.get((bus, date_str))\n",
        "                if match_type == \"exact\":\n",
        "                    exact_match_cells.append(f\"L{row_num}\")\n",
        "                elif match_type == \"fallback\":\n",
        "                    fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += 1\n",
        "                if updated_count % 10 == 0:\n",
        "                    print(f\"📊 Progress: Updated {updated_count}/{len(bus_date_value_map)} values\")\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update row {row_num}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update row {row_num} after {max_retries} retries\")\n",
        "\n",
        "# Apply formatting in batches (currently commented out)\n",
        "print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "# You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "print(f\"\\n✅ COMPLETE: Updated {updated_count} rows in column L and column M\")\n",
        "print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n"
      ],
      "metadata": {
        "id": "WgcwRz-J2agi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FG_Working, Best Code\n",
        "\n",
        "# Modified to process multiple engine oil worksheets, Added batch processing done\n",
        "\n",
        "# Modified to process multiple engine oil worksheets with batch processing\n",
        "# With fallback date display in M Column, For ATR_MAS_Engine Oil\n",
        "\n",
        "# Make sure to include these imports at the top of your code\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_FG',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_FG'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Function to process each worksheet\n",
        "def process_worksheet(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "    clear_range = 'L2:M'\n",
        "    print(f\"🧹 Clearing range {clear_range} in {worksheet_name}...\")\n",
        "    worksheet.batch_clear([clear_range])\n",
        "\n",
        "    print(\"🔍 Starting data processing with date fallback search...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping\n",
        "    print(\"🔢 Mapping bus numbers and dates...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            date_str = row[1].strip()\n",
        "            bus_num = row[4].strip()\n",
        "            if date_str and bus_num:\n",
        "                bus_date_map[bus_num].add(date_str)\n",
        "                row_map[i] = (bus_num, date_str)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers\")\n",
        "    print(f\"✅ Found {len(row_map)} rows to process\")\n",
        "\n",
        "    # Create a cache for worksheet data to avoid redundant fetches\n",
        "    bus_worksheet_cache = {}\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Function to process a single bus\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "\n",
        "        try:\n",
        "            # Check if we already have this bus's data in cache\n",
        "            if bus in bus_worksheet_cache:\n",
        "                sheet_data = bus_worksheet_cache[bus]\n",
        "            else:\n",
        "                # Fetch the bus worksheet data and store in cache\n",
        "                sheet = Odometer_spreadsheet_FG.worksheet(bus)\n",
        "                sheet_data = sheet.get_all_values()\n",
        "                bus_worksheet_cache[bus] = sheet_data\n",
        "\n",
        "            # Create a dictionary for faster exact date lookups\n",
        "            date_value_lookup = {}\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12:\n",
        "                        date_str_key = row[1].strip()\n",
        "                        date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for date_str in dates:\n",
        "                # Check for exact match using the lookup dictionary\n",
        "                exact_value = date_value_lookup.get(date_str)\n",
        "\n",
        "                if exact_value:\n",
        "                    results.append((bus, date_str, exact_value, \"exact\", \"\"))\n",
        "                    sheet_success += 1\n",
        "                else:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date))\n",
        "                        sheet_success += 1\n",
        "\n",
        "            print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Process buses in parallel using ThreadPoolExecutor\n",
        "    print(f\"🚀 Processing {len(bus_date_map)} buses in parallel...\")\n",
        "\n",
        "    # Convert to list for ThreadPoolExecutor\n",
        "    bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    # Adjust max_workers based on your system's capabilities\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "    # Process all results\n",
        "    for bus_results in all_results:\n",
        "        for bus, date_str, value, match_type, fallback_date in bus_results:\n",
        "            bus_date_value_map[(bus, date_str)] = value\n",
        "            match_type_map[(bus, date_str)] = match_type\n",
        "            fallback_date_map[(bus, date_str)] = fallback_date\n",
        "\n",
        "            if match_type == \"exact\":\n",
        "                success_count += 1\n",
        "            else:\n",
        "                fallback_count += 1\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 50  # Adjust based on API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update\n",
        "                worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 2  # Divide by 2 because we have 2 cells per row\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//2} rows updated\")\n",
        "\n",
        "                # Add a small delay between chunks to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in column L and column M\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Main execution\n",
        "print(\"🚀 Starting to process all engine oil worksheets...\")\n",
        "\n",
        "# Create a global cache for bus worksheets to reuse across all worksheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Process each worksheet in sequence\n",
        "for worksheet_name in engine_oil_worksheets:\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name}\\n{'='*80}\")\n",
        "    process_worksheet(worksheet_name)\n",
        "    # Add a delay between worksheets to avoid rate limiting\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with batch processing\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")\n"
      ],
      "metadata": {
        "id": "ng3__2UKVfZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lean for all units, improved API limiter, #Lean combined all Units Engine Oil and Search all Odometer, but it search only missing values, if it works, then in future just add engine oil sheet and odometer sheet\n",
        "\n",
        "\n",
        "# Lean version - Only processes rows with empty L column\n",
        "# With fallback date display in M Column, For SVP, FG, BT, RT, MB, and DP\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_FG',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_FG',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_ATR',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_ATR',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_rr',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_rr'\n",
        "]\n",
        "\n",
        "# List of all odometer spreadsheets to search\n",
        "odometer_spreadsheets = [\n",
        "    'Odometer_spreadsheet_SVP',\n",
        "    'Odometer_spreadsheet_FG',\n",
        "    'Odometer_spreadsheet_BT',\n",
        "    'Odometer_spreadsheet_RT',\n",
        "    'Odometer_spreadsheet_MB',\n",
        "    'Odometer_spreadsheet_DP'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Global cache for bus worksheets to reuse across all worksheets and spreadsheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Function to process each worksheet - LEAN VERSION (only empty L cells)\n",
        "def process_worksheet_lean(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # NO CLEARING OF RANGES - we're only updating empty cells\n",
        "    print(\"🔍 Starting data processing for EMPTY L cells only...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping ONLY FOR EMPTY L CELLS\n",
        "    print(\"🔢 Mapping bus numbers and dates for empty L cells...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "    empty_cell_count = 0\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            # Check if L column (index 11) is empty\n",
        "            l_value = row[11].strip() if len(row) > 11 else \"\"\n",
        "\n",
        "            if not l_value:  # Only process if L column is empty\n",
        "                date_str = row[1].strip()\n",
        "                bus_num = row[4].strip()\n",
        "                if date_str and bus_num:\n",
        "                    bus_date_map[bus_num].add(date_str)\n",
        "                    row_map[i] = (bus_num, date_str)\n",
        "                    empty_cell_count += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers with empty L cells\")\n",
        "    print(f\"✅ Found {empty_cell_count} empty L cells to process\")\n",
        "\n",
        "    # Exit early if no empty cells to process\n",
        "    if empty_cell_count == 0:\n",
        "        print(f\"✓ No empty L cells to process in {worksheet_name}. Moving to next worksheet.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Track which spreadsheet contained the match for metadata\n",
        "    spreadsheet_source_map = {}  # Maps (bus, date) to spreadsheet name\n",
        "\n",
        "    # Function to process a single bus across all spreadsheets\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "        dates_to_process = set(dates)  # Create a copy to safely modify\n",
        "\n",
        "        # Try each spreadsheet in sequence\n",
        "        for spreadsheet_name in odometer_spreadsheets:\n",
        "            if not dates_to_process:  # If all dates have been found, skip further processing\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Generate cache key that includes the spreadsheet name\n",
        "                cache_key = f\"{spreadsheet_name}:{bus}\"\n",
        "\n",
        "                # Check if we already have this bus's data in cache\n",
        "                if cache_key in global_bus_worksheet_cache:\n",
        "                    sheet_data = global_bus_worksheet_cache[cache_key]\n",
        "                else:\n",
        "                    # Get the spreadsheet object dynamically\n",
        "                    try:\n",
        "                        spreadsheet = eval(spreadsheet_name)\n",
        "                        # Try to get the worksheet for this bus with retry mechanism\n",
        "                        max_retries = 5\n",
        "                        retry_count = 0\n",
        "                        while retry_count < max_retries:\n",
        "                            try:\n",
        "                                sheet = spreadsheet.worksheet(bus)\n",
        "                                sheet_data = sheet.get_all_values()\n",
        "                                global_bus_worksheet_cache[cache_key] = sheet_data\n",
        "                                break  # Success, exit retry loop\n",
        "                            except gspread.exceptions.APIError as api_error:\n",
        "                                if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                                    retry_count += 1\n",
        "                                    wait_time = (2 ** retry_count) + (random.random() * 2)\n",
        "                                    print(f\"⏳ Rate limit hit for bus {bus}, waiting {wait_time:.2f}s (retry {retry_count}/{max_retries})\")\n",
        "                                    time.sleep(wait_time)\n",
        "                                    if retry_count == max_retries:\n",
        "                                        print(f\"⚠️ Max retries reached for bus {bus} in {spreadsheet_name}\")\n",
        "                                        raise\n",
        "                                else:\n",
        "                                    # Not a rate limit issue, the worksheet likely doesn't exist\n",
        "                                    raise\n",
        "                            except Exception as e:\n",
        "                                # The worksheet doesn't exist in this spreadsheet\n",
        "                                if \"Worksheet not found\" in str(e) or \"404\" in str(e):\n",
        "                                    print(f\"  ℹ️ Bus {bus} not found in {spreadsheet_name}\")\n",
        "                                else:\n",
        "                                    print(f\"  ⚠️ Error accessing {bus} in {spreadsheet_name}: {str(e)}\")\n",
        "                                raise\n",
        "                    except Exception:\n",
        "                        # This bus doesn't exist in this spreadsheet or we couldn't access it\n",
        "                        continue\n",
        "\n",
        "                # Process each date for this bus in this spreadsheet\n",
        "                # Create a dictionary for faster exact date lookups\n",
        "                date_value_lookup = {}\n",
        "                for row in sheet_data[1:]:\n",
        "                    try:\n",
        "                        if len(row) > 12:\n",
        "                            date_str_key = row[1].strip()\n",
        "                            date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                dates_found_in_this_sheet = set()\n",
        "\n",
        "                # First try exact matches which are faster to check\n",
        "                for date_str in dates_to_process:\n",
        "                    # Check for exact match using the lookup dictionary\n",
        "                    exact_value = date_value_lookup.get(date_str)\n",
        "                    if exact_value:\n",
        "                        results.append((bus, date_str, exact_value, \"exact\", \"\", spreadsheet_name))\n",
        "                        sheet_success += 1\n",
        "                        dates_found_in_this_sheet.add(date_str)\n",
        "\n",
        "                # Then try fallback matches for remaining dates\n",
        "                remaining_dates = dates_to_process - dates_found_in_this_sheet\n",
        "                for date_str in remaining_dates:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date, spreadsheet_name))\n",
        "                        sheet_success += 1\n",
        "                        dates_found_in_this_sheet.add(date_str)\n",
        "\n",
        "                # Remove all processed dates\n",
        "                dates_to_process -= dates_found_in_this_sheet\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"Worksheet not found\" not in str(e) and \"404\" not in str(e):\n",
        "                    print(f\"❌ Could not process sheet '{bus}' in spreadsheet '{spreadsheet_name}': {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if sheet_success > 0:\n",
        "            print(f\"  ✅ Found {sheet_success} values for bus {bus}\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ No values found for bus {bus} in any spreadsheet\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    # Only process if we have buses to check\n",
        "    if len(bus_date_map) > 0:\n",
        "        # Convert to list for ThreadPoolExecutor\n",
        "        bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        max_workers = min(5, len(bus_dates_list))  # Adjust based on data size\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "        # Process all results\n",
        "        for bus_results in all_results:\n",
        "            for bus, date_str, value, match_type, fallback_date, source_spreadsheet in bus_results:\n",
        "                bus_date_value_map[(bus, date_str)] = value\n",
        "                match_type_map[(bus, date_str)] = match_type\n",
        "                fallback_date_map[(bus, date_str)] = fallback_date\n",
        "                spreadsheet_source_map[(bus, date_str)] = source_spreadsheet\n",
        "\n",
        "                if match_type == \"exact\":\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    fallback_count += 1\n",
        "    else:\n",
        "        print(\"⚠️ No buses to process - all L cells have values already\")\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "        spreadsheet_source = spreadsheet_source_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date if fallback_date else \"\"\n",
        "            })\n",
        "\n",
        "            # Add source spreadsheet info in Column N for tracking\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 14,  # Column N\n",
        "                'value': spreadsheet_source\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Skip if no updates needed\n",
        "    if len(batch_updates) == 0:\n",
        "        print(\"✓ No updates needed for this worksheet\")\n",
        "        return\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 25  # Reduced chunk size to avoid API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 8  # Increased retries\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update using our rate-limited function\n",
        "                def do_update():\n",
        "                    return worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                rate_limited_request(do_update)\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 3  # Divide by 3 because we have 3 cells per row (L, M, N)\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//3} rows updated\")\n",
        "\n",
        "                # Add a variable delay between chunks to avoid rate limits\n",
        "                # More aggressive backoff with increasing chunk index\n",
        "                wait_time = 2 + (chunk_index % 3) + (random.random() * 3)\n",
        "                print(f\"⏳ Waiting {wait_time:.2f}s before next chunk...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            except gspread.exceptions.APIError as api_error:\n",
        "                retry_count += 1\n",
        "                if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                    # Exponential backoff with jitter for rate limits\n",
        "                    wait_time = (2 ** retry_count) + (random.random() * 5)\n",
        "                    # Cap maximum wait time at 2 minutes\n",
        "                    wait_time = min(wait_time, 120)\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ API Error on chunk {chunk_index+1}: {api_error}\")\n",
        "                    if retry_count < 3:  # Only retry a few times for non-rate-limit errors\n",
        "                        time.sleep(5)\n",
        "                    else:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                if retry_count < 3:  # Only retry a few times for general errors\n",
        "                    time.sleep(5)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "            # Add extra delay when a chunk completely fails before moving to next chunk\n",
        "            wait_time = 15 + (random.random() * 10)\n",
        "            print(f\"⏳ Taking a longer break: {wait_time:.2f}s before continuing...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in columns L, M, and N\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Global rate limiting function\n",
        "def rate_limited_request(func, *args, **kwargs):\n",
        "    \"\"\"Execute a function with rate limiting and retries for API errors\"\"\"\n",
        "    max_retries = 8  # More retries for critical operations\n",
        "    retry_count = 0\n",
        "    base_wait_time = 2  # seconds\n",
        "\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except gspread.exceptions.APIError as api_error:\n",
        "            if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                retry_count += 1\n",
        "                # Exponential backoff with jitter\n",
        "                wait_time = (base_wait_time ** retry_count) + (random.random() * 5)\n",
        "                # Cap the wait time at 3 minutes\n",
        "                wait_time = min(wait_time, 180)\n",
        "                print(f\"⏳ Rate limit hit! Waiting {wait_time:.2f}s (retry {retry_count}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                # Re-raise non-rate-limit errors\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            # For other exceptions, retry a few times but with less patience\n",
        "            if retry_count < 3:\n",
        "                retry_count += 1\n",
        "                wait_time = base_wait_time + (random.random() * 2)\n",
        "                print(f\"⚠️ Error occurred: {str(e)}\")\n",
        "                print(f\"Retrying in {wait_time:.2f}s (retry {retry_count}/3)\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    # If we've exhausted all retries\n",
        "    raise Exception(f\"Failed after {max_retries} retries due to persistent rate limiting\")\n",
        "\n",
        "# Main execution - LEAN VERSION with improved rate limiting\n",
        "print(\"🚀 Starting LEAN processing - only empty L cells...\")\n",
        "print(f\"🔍 Will search across {len(odometer_spreadsheets)} odometer spreadsheets: {', '.join(odometer_spreadsheets)}\")\n",
        "\n",
        "# Process each worksheet with rate limiting and proper delays\n",
        "for i, worksheet_name in enumerate(engine_oil_worksheets):\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name} ({i+1}/{len(engine_oil_worksheets)})\\n{'='*80}\")\n",
        "\n",
        "    # Add a longer delay between worksheets to avoid rate limiting\n",
        "    if i > 0:\n",
        "        wait_time = 5 + (random.random() * 5)  # 5-10 second delay between worksheets\n",
        "        print(f\"⏳ Waiting {wait_time:.2f}s before processing next worksheet...\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "    # Process the worksheet with proper rate limiting\n",
        "    try:\n",
        "        process_worksheet_lean(worksheet_name)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing worksheet {worksheet_name}: {str(e)}\")\n",
        "        print(\"Continuing with next worksheet after a delay...\")\n",
        "        time.sleep(15)  # Longer delay after an error\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with LEAN mode (empty L cells only)\")\n",
        "print(f\"✓ Searched across {len(odometer_spreadsheets)} odometer spreadsheets\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azCz95R7KQK9",
        "outputId": "1bcca8f4-4b9d-4dbe-8480-7ebd8208ff64"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting LEAN processing - only empty L cells...\n",
            "🔍 Will search across 6 odometer spreadsheets: Odometer_spreadsheet_SVP, Odometer_spreadsheet_FG, Odometer_spreadsheet_BT, Odometer_spreadsheet_RT, Odometer_spreadsheet_MB, Odometer_spreadsheet_DP\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_FG (1/6)\n",
            "================================================================================\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_FG\n",
            "🔍 Starting data processing for EMPTY L cells only...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 985 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates for empty L cells...\n",
            "✅ Found 21 unique bus numbers with empty L cells\n",
            "✅ Found 486 empty L cells to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "  ⚠️ Error accessing 389 in Odometer_spreadsheet_SVP: 389\n",
            "  ⚠️ Error accessing 415 in Odometer_spreadsheet_SVP: 415\n",
            "  ⚠️ Error accessing 388 in Odometer_spreadsheet_SVP: 388\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_SVP: 419\n",
            "  ⚠️ Error accessing tyreplant in Odometer_spreadsheet_SVP: tyreplant\n",
            "  ⚠️ Error accessing 415 in Odometer_spreadsheet_FG: 415\n",
            "  ⚠️ Error accessing 388 in Odometer_spreadsheet_FG: 388\n",
            "  ⚠️ Error accessing 389 in Odometer_spreadsheet_FG: 389\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_FG: 419\n",
            "  ⚠️ Error accessing tyreplant in Odometer_spreadsheet_FG: tyreplant\n",
            "  ⚠️ Error accessing 389 in Odometer_spreadsheet_BT: 389  ⚠️ Error accessing tyreplant in Odometer_spreadsheet_BT: tyreplant\n",
            "  ⚠️ Error accessing 388 in Odometer_spreadsheet_BT: 388\n",
            "\n",
            "  ⚠️ Error accessing 415 in Odometer_spreadsheet_BT: 415\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_BT: 419\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_RT: 419  ⚠️ Error accessing 389 in Odometer_spreadsheet_RT: 389\n",
            "  ⚠️ Error accessing 415 in Odometer_spreadsheet_RT: 415\n",
            "\n",
            "  ⚠️ Error accessing tyreplant in Odometer_spreadsheet_RT: tyreplant\n",
            "  ⚠️ Error accessing 388 in Odometer_spreadsheet_RT: 388\n",
            "  ⚠️ Error accessing 388 in Odometer_spreadsheet_MB: 388\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_MB: 419\n",
            "  ⚠️ Error accessing 389 in Odometer_spreadsheet_MB: 389\n",
            "  ⚠️ Error accessing tyreplant in Odometer_spreadsheet_MB: tyreplant\n",
            "  ⚠️ Error accessing 415 in Odometer_spreadsheet_MB: 415\n",
            "  ⚠️ Error accessing 388 in Odometer_spreadsheet_DP: 388\n",
            "  ⚠️ No values found for bus 388 in any spreadsheet\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_DP: 419\n",
            "  ⚠️ No values found for bus 419 in any spreadsheet\n",
            "  ⚠️ Error accessing 415 in Odometer_spreadsheet_DP: 415\n",
            "  ⚠️ No values found for bus 415 in any spreadsheet\n",
            "  ⚠️ Error accessing tyreplant in Odometer_spreadsheet_DP: tyreplant\n",
            "  ⚠️ No values found for bus tyreplant in any spreadsheet\n",
            "  ⚠️ Error accessing 389 in Odometer_spreadsheet_DP: 389\n",
            "  ⚠️ No values found for bus 389 in any spreadsheet\n",
            "  ⚠️ Error accessing workshop in Odometer_spreadsheet_SVP: workshop\n",
            "  ⚠️ Error accessing 386 in Odometer_spreadsheet_SVP: 386\n",
            "  ⚠️ Error accessing workshop in Odometer_spreadsheet_FG: workshop\n",
            "  ⚠️ Error accessing 370 in Odometer_spreadsheet_SVP: 370\n",
            "  ⚠️ Error accessing 386 in Odometer_spreadsheet_FG: 386\n",
            "  ⚠️ Error accessing workshop in Odometer_spreadsheet_BT: workshop\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_SVP: 435\n",
            "  ⚠️ Error accessing 370 in Odometer_spreadsheet_FG: 370\n",
            "  ⚠️ Error accessing 386 in Odometer_spreadsheet_BT: 386\n",
            "  ⚠️ Error accessing workshop in Odometer_spreadsheet_RT: workshop\n",
            "  ⚠️ Error accessing 328 in Odometer_spreadsheet_SVP: 328\n",
            "  ⚠️ Error accessing 386 in Odometer_spreadsheet_RT: 386\n",
            "  ⚠️ Error accessing workshop in Odometer_spreadsheet_MB: workshop\n",
            "  ⚠️ Error accessing 370 in Odometer_spreadsheet_BT: 370\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_FG: 435\n",
            "  ⚠️ Error accessing 386 in Odometer_spreadsheet_MB: 386\n",
            "  ⚠️ Error accessing workshop in Odometer_spreadsheet_DP: workshop\n",
            "  ⚠️ No values found for bus workshop in any spreadsheet\n",
            "  ⚠️ Error accessing 328 in Odometer_spreadsheet_FG: 328\n",
            "  ⚠️ Error accessing 370 in Odometer_spreadsheet_RT: 370\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_BT: 435\n",
            "  ⚠️ Error accessing 386 in Odometer_spreadsheet_DP: 386\n",
            "  ⚠️ No values found for bus 386 in any spreadsheet\n",
            "  ⚠️ Error accessing 370 in Odometer_spreadsheet_MB: 370\n",
            "  ⚠️ Error accessing 328 in Odometer_spreadsheet_BT: 328\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_RT: 435\n",
            "  ⚠️ Error accessing 370 in Odometer_spreadsheet_DP: 370\n",
            "  ⚠️ No values found for bus 370 in any spreadsheet\n",
            "  ⚠️ Error accessing 328 in Odometer_spreadsheet_RT: 328\n",
            "  ⚠️ Error accessing 202 in Odometer_spreadsheet_SVP: 202\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_MB: 435\n",
            "  ⚠️ Error accessing 328 in Odometer_spreadsheet_MB: 328\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_DP: 435\n",
            "  ⚠️ No values found for bus 435 in any spreadsheet\n",
            "  ⚠️ Error accessing 436 in Odometer_spreadsheet_SVP: 436\n",
            "  ⚠️ Error accessing 202 in Odometer_spreadsheet_FG: 202\n",
            "  ⚠️ Error accessing 328 in Odometer_spreadsheet_DP: 328\n",
            "  ⚠️ No values found for bus 328 in any spreadsheet\n",
            "  ⚠️ Error accessing 344 in Odometer_spreadsheet_SVP: 344\n",
            "  ⚠️ Error accessing 436 in Odometer_spreadsheet_FG: 436\n",
            "  ⚠️ Error accessing 202 in Odometer_spreadsheet_BT: 202\n",
            "  ⚠️ Error accessing 350 in Odometer_spreadsheet_SVP: 350\n",
            "  ⚠️ Error accessing 436 in Odometer_spreadsheet_BT: 436\n",
            "  ⚠️ Error accessing 344 in Odometer_spreadsheet_FG: 344\n",
            "  ⚠️ Error accessing 202 in Odometer_spreadsheet_RT: 202\n",
            "  ⚠️ Error accessing tyre plant in Odometer_spreadsheet_SVP: tyre plant\n",
            "  ⚠️ Error accessing 202 in Odometer_spreadsheet_MB: 202\n",
            "  ⚠️ Error accessing 344 in Odometer_spreadsheet_BT: 344\n",
            "  ⚠️ Error accessing 436 in Odometer_spreadsheet_RT: 436\n",
            "  ⚠️ Error accessing 350 in Odometer_spreadsheet_FG: 350\n",
            "  ⚠️ Error accessing 202 in Odometer_spreadsheet_DP: 202\n",
            "  ⚠️ No values found for bus 202 in any spreadsheet\n",
            "  ⚠️ Error accessing 344 in Odometer_spreadsheet_RT: 344\n",
            "  ⚠️ Error accessing 436 in Odometer_spreadsheet_MB: 436\n",
            "  ⚠️ Error accessing tyre plant in Odometer_spreadsheet_FG: tyre plant\n",
            "  ⚠️ Error accessing 344 in Odometer_spreadsheet_MB: 344\n",
            "  ⚠️ Error accessing 436 in Odometer_spreadsheet_DP: 436\n",
            "  ⚠️ No values found for bus 436 in any spreadsheet\n",
            "  ⚠️ Error accessing 350 in Odometer_spreadsheet_BT: 350\n",
            "  ⚠️ Error accessing tyre plant in Odometer_spreadsheet_BT: tyre plant\n",
            "  ⚠️ Error accessing 344 in Odometer_spreadsheet_DP: 344\n",
            "  ⚠️ No values found for bus 344 in any spreadsheet\n",
            "  ⚠️ Error accessing tyre plant in Odometer_spreadsheet_RT: tyre plant\n",
            "  ⚠️ Error accessing 350 in Odometer_spreadsheet_RT: 350\n",
            "  ⚠️ Error accessing 378 in Odometer_spreadsheet_SVP: 378\n",
            "  ⚠️ Error accessing tyre plant in Odometer_spreadsheet_MB: tyre plant\n",
            "  ⚠️ Error accessing 350 in Odometer_spreadsheet_MB: 350\n",
            "  ⚠️ Error accessing Tyre Plant in Odometer_spreadsheet_SVP: Tyre Plant\n",
            "  ⚠️ Error accessing 378 in Odometer_spreadsheet_FG: 378\n",
            "  ⚠️ Error accessing tyre plant in Odometer_spreadsheet_DP: tyre plant\n",
            "  ⚠️ No values found for bus tyre plant in any spreadsheet\n",
            "  ⚠️ Error accessing 350 in Odometer_spreadsheet_DP: 350\n",
            "  ⚠️ No values found for bus 350 in any spreadsheet\n",
            "  ⚠️ Error accessing 572 in Odometer_spreadsheet_SVP: 572\n",
            "  ⚠️ Error accessing 378 in Odometer_spreadsheet_BT: 378\n",
            "  ⚠️ Error accessing Tyre Plant in Odometer_spreadsheet_FG: Tyre Plant\n",
            "  ⚠️ Error accessing 378 in Odometer_spreadsheet_RT: 378\n",
            "  ⚠️ Error accessing Tyre Plant in Odometer_spreadsheet_BT: Tyre Plant\n",
            "  ⚠️ Error accessing OTHER in Odometer_spreadsheet_SVP: OTHER\n",
            "  ⚠️ Error accessing 378 in Odometer_spreadsheet_MB: 378\n",
            "  ⚠️ Error accessing Tyre Plant in Odometer_spreadsheet_RT: Tyre Plant\n",
            "  ⚠️ Error accessing others in Odometer_spreadsheet_SVP: others\n",
            "  ⚠️ Error accessing Tyre Plant in Odometer_spreadsheet_MB: Tyre Plant\n",
            "  ⚠️ Error accessing 378 in Odometer_spreadsheet_DP: 378\n",
            "  ⚠️ No values found for bus 378 in any spreadsheet\n",
            "  ⚠️ Error accessing OTHER in Odometer_spreadsheet_FG: OTHER\n",
            "  ⚠️ Error accessing others in Odometer_spreadsheet_FG: others\n",
            "  ⚠️ Error accessing OTHER in Odometer_spreadsheet_BT: OTHER\n",
            "  ⚠️ Error accessing Tyre Plant in Odometer_spreadsheet_DP: Tyre Plant\n",
            "  ⚠️ No values found for bus Tyre Plant in any spreadsheet\n",
            "  ⚠️ Error accessing 572 in Odometer_spreadsheet_BT: 572\n",
            "  ⚠️ Error accessing TYRE PLANT in Odometer_spreadsheet_SVP: TYRE PLANT\n",
            "  ⚠️ Error accessing others in Odometer_spreadsheet_BT: others\n",
            "  ⚠️ Error accessing OTHER in Odometer_spreadsheet_RT: OTHER\n",
            "  ⚠️ Error accessing 572 in Odometer_spreadsheet_RT: 572\n",
            "  ⚠️ Error accessing OTHER in Odometer_spreadsheet_MB: OTHER\n",
            "  ⚠️ Error accessing others in Odometer_spreadsheet_RT: others\n",
            "  ⚠️ Error accessing 572 in Odometer_spreadsheet_MB: 572\n",
            "  ⚠️ Error accessing TYRE PLANT in Odometer_spreadsheet_FG: TYRE PLANT\n",
            "  ⚠️ Error accessing others in Odometer_spreadsheet_MB: others\n",
            "  ⚠️ Error accessing OTHER in Odometer_spreadsheet_DP: OTHER\n",
            "  ⚠️ No values found for bus OTHER in any spreadsheet\n",
            "  ⚠️ Error accessing TYRE PLANT in Odometer_spreadsheet_BT: TYRE PLANT\n",
            "  ⚠️ Error accessing 572 in Odometer_spreadsheet_DP: 572\n",
            "  ⚠️ No values found for bus 572 in any spreadsheet\n",
            "  ⚠️ Error accessing others in Odometer_spreadsheet_DP: others\n",
            "  ⚠️ No values found for bus others in any spreadsheet\n",
            "  ⚠️ Error accessing TYRE PLANT in Odometer_spreadsheet_RT: TYRE PLANT\n",
            "  ⚠️ Error accessing TYRE PLANT in Odometer_spreadsheet_MB: TYRE PLANT\n",
            "  ⚠️ Error accessing TYRE PLANT in Odometer_spreadsheet_DP: TYRE PLANT\n",
            "  ⚠️ No values found for bus TYRE PLANT in any spreadsheet\n",
            "\n",
            "✅ Found values for 0 exact date matches\n",
            "✅ Found values for 0 fallback date matches\n",
            "❌ Could not find values for 486 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CH_BSIV_CH_MAS_FG...\n",
            "✅ Prepared 0 cell updates\n",
            "✓ No updates needed for this worksheet\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_FG (2/6)\n",
            "================================================================================\n",
            "⏳ Waiting 9.47s before processing next worksheet...\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_FG\n",
            "🔍 Starting data processing for EMPTY L cells only...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 7 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates for empty L cells...\n",
            "✅ Found 3 unique bus numbers with empty L cells\n",
            "✅ Found 3 empty L cells to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "  ⚠️ Error accessing 615 in Odometer_spreadsheet_SVP: 615\n",
            "  ⚠️ Error accessing 617 in Odometer_spreadsheet_SVP: 617\n",
            "  ⚠️ Error accessing 616 in Odometer_spreadsheet_SVP: 616\n",
            "  ✅ Found 1 values for bus 615\n",
            "  ✅ Found 1 values for bus 617\n",
            "  ✅ Found 1 values for bus 616\n",
            "\n",
            "✅ Found values for 3 exact date matches\n",
            "✅ Found values for 0 fallback date matches\n",
            "❌ Could not find values for 0 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CK_BSVI_CK_MAS_FG...\n",
            "✅ Prepared 9 cell updates\n",
            "🔄 Executing batch updates in 1 chunks...\n",
            "📊 Progress: Chunk 1/1 complete - 3/3 rows updated\n",
            "⏳ Waiting 3.99s before next chunk...\n",
            "\n",
            "🎨 Applying formatting to distinguish match types...\n",
            "\n",
            "✅ COMPLETE for engine_oil_CK_BSVI_CK_MAS_FG: Updated 3 rows in columns L, M, and N\n",
            "📊 Summary: 3 exact matches (normal format), 0 fallback matches (italic and right-aligned)\n",
            "❌ 0 rows could not be updated\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_ATR (3/6)\n",
            "================================================================================\n",
            "⏳ Waiting 5.76s before processing next worksheet...\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_ATR\n",
            "🔍 Starting data processing for EMPTY L cells only...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 900 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates for empty L cells...\n",
            "✅ Found 20 unique bus numbers with empty L cells\n",
            "✅ Found 47 empty L cells to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "  ⚠️ Error accessing 565 in Odometer_spreadsheet_SVP: 565\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_SVP: 381\n",
            "  ⚠️ Error accessing 565 in Odometer_spreadsheet_FG: 565\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_SVP: 368\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_FG: 381\n",
            "  ⚠️ Error accessing 565 in Odometer_spreadsheet_BT: 565\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_SVP: 576\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_FG: 368\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_BT: 381\n",
            "  ⚠️ Error accessing 579 in Odometer_spreadsheet_SVP: 579\n",
            "  ⚠️ Error accessing 565 in Odometer_spreadsheet_RT: 565\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_FG: 576\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_BT: 368\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_RT: 381\n",
            "  ⚠️ Error accessing 565 in Odometer_spreadsheet_MB: 565\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_RT: 368\n",
            "  ⚠️ Error accessing 579 in Odometer_spreadsheet_FG: 579\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_BT: 576\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_MB: 381\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_MB: 368\n",
            "  ⚠️ Error accessing 565 in Odometer_spreadsheet_DP: 565\n",
            "  ⚠️ No values found for bus 565 in any spreadsheet\n",
            "  ⚠️ Error accessing 579 in Odometer_spreadsheet_BT: 579\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_DP: 381\n",
            "  ⚠️ No values found for bus 381 in any spreadsheet\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_RT: 576\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_DP: 368\n",
            "  ⚠️ No values found for bus 368 in any spreadsheet\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_MB: 576\n",
            "  ⚠️ Error accessing 579 in Odometer_spreadsheet_RT: 579\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_SVP: 319\n",
            "  ⚠️ Error accessing 579 in Odometer_spreadsheet_MB: 579\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_DP: 576\n",
            "  ⚠️ No values found for bus 576 in any spreadsheet\n",
            "  ⚠️ Error accessing 564 in Odometer_spreadsheet_SVP: 564\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_FG: 319\n",
            "  ⚠️ Error accessing 579 in Odometer_spreadsheet_DP: 579\n",
            "  ⚠️ No values found for bus 579 in any spreadsheet\n",
            "  ⚠️ Error accessing 584 in Odometer_spreadsheet_SVP: 584\n",
            "  ⚠️ Error accessing 564 in Odometer_spreadsheet_FG: 564\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_BT: 319\n",
            "  ⚠️ Error accessing 566 in Odometer_spreadsheet_SVP: 566\n",
            "  ⚠️ Error accessing 564 in Odometer_spreadsheet_BT: 564\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_RT: 319\n",
            "  ⚠️ Error accessing 584 in Odometer_spreadsheet_FG: 584\n",
            "  ⚠️ Error accessing 591 in Odometer_spreadsheet_SVP: 591\n",
            "  ⚠️ Error accessing 566 in Odometer_spreadsheet_FG: 566\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_MB: 319\n",
            "  ⚠️ Error accessing 564 in Odometer_spreadsheet_RT: 564\n",
            "  ⚠️ Error accessing 584 in Odometer_spreadsheet_BT: 584\n",
            "  ⚠️ Error accessing 566 in Odometer_spreadsheet_BT: 566\n",
            "  ⚠️ Error accessing 591 in Odometer_spreadsheet_FG: 591\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_DP: 319\n",
            "  ⚠️ No values found for bus 319 in any spreadsheet\n",
            "  ⚠️ Error accessing 564 in Odometer_spreadsheet_MB: 564\n",
            "  ⚠️ Error accessing 584 in Odometer_spreadsheet_RT: 584\n",
            "  ⚠️ Error accessing 591 in Odometer_spreadsheet_BT: 591\n",
            "  ⚠️ Error accessing 564 in Odometer_spreadsheet_DP: 564\n",
            "  ⚠️ No values found for bus 564 in any spreadsheet\n",
            "  ⚠️ Error accessing 584 in Odometer_spreadsheet_MB: 584\n",
            "  ⚠️ Error accessing 584 in Odometer_spreadsheet_DP: 584\n",
            "  ⚠️ No values found for bus 584 in any spreadsheet\n",
            "  ⚠️ Error accessing 566 in Odometer_spreadsheet_RT: 566\n",
            "  ⚠️ Error accessing 545 in Odometer_spreadsheet_SVP: 545\n",
            "  ⚠️ Error accessing 591 in Odometer_spreadsheet_RT: 591\n",
            "  ⚠️ Error accessing 566 in Odometer_spreadsheet_MB: 566\n",
            "  ⚠️ Error accessing 591 in Odometer_spreadsheet_MB: 591\n",
            "  ⚠️ Error accessing 549 in Odometer_spreadsheet_SVP: 549\n",
            "  ⚠️ Error accessing 545 in Odometer_spreadsheet_FG: 545\n",
            "  ⚠️ Error accessing 566 in Odometer_spreadsheet_DP: 566\n",
            "  ⚠️ No values found for bus 566 in any spreadsheet\n",
            "  ⚠️ Error accessing 591 in Odometer_spreadsheet_DP: 591\n",
            "  ⚠️ No values found for bus 591 in any spreadsheet\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_SVP: 553\n",
            "  ⚠️ Error accessing 549 in Odometer_spreadsheet_FG: 549\n",
            "  ⚠️ Error accessing 545 in Odometer_spreadsheet_BT: 545\n",
            "  ⚠️ Error accessing 549 in Odometer_spreadsheet_BT: 549\n",
            "  ⚠️ Error accessing 545 in Odometer_spreadsheet_RT: 545\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_FG: 553\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_SVP: 544\n",
            "  ⚠️ Error accessing 545 in Odometer_spreadsheet_MB: 545\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_BT: 553\n",
            "  ⚠️ Error accessing 430 in Odometer_spreadsheet_SVP: 430\n",
            "  ⚠️ Error accessing 549 in Odometer_spreadsheet_RT: 549\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_FG: 544\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_RT: 553\n",
            "  ⚠️ Error accessing 549 in Odometer_spreadsheet_MB: 549\n",
            "  ⚠️ Error accessing 430 in Odometer_spreadsheet_FG: 430\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_BT: 544\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_MB: 553\n",
            "  ⚠️ Error accessing 545 in Odometer_spreadsheet_DP: 545\n",
            "  ⚠️ No values found for bus 545 in any spreadsheet\n",
            "  ⚠️ Error accessing 549 in Odometer_spreadsheet_DP: 549\n",
            "  ⚠️ No values found for bus 549 in any spreadsheet\n",
            "  ⚠️ Error accessing 430 in Odometer_spreadsheet_BT: 430\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_DP: 553\n",
            "  ⚠️ No values found for bus 553 in any spreadsheet\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_RT: 544\n",
            "  ⚠️ Error accessing 430 in Odometer_spreadsheet_RT: 430\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_MB: 544\n",
            "  ⚠️ Error accessing 430 in Odometer_spreadsheet_MB: 430\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_SVP: 317\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_DP: 544\n",
            "  ⚠️ No values found for bus 544 in any spreadsheet\n",
            "  ⚠️ Error accessing 430 in Odometer_spreadsheet_DP: 430\n",
            "  ⚠️ No values found for bus 430 in any spreadsheet\n",
            "  ⚠️ Error accessing 433 in Odometer_spreadsheet_SVP: 433\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_FG: 317\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_SVP: 409\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_BT: 317\n",
            "  ⚠️ Error accessing 433 in Odometer_spreadsheet_FG: 433\n",
            "  ⚠️ Error accessing Grease Gun in Odometer_spreadsheet_SVP: Grease Gun\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_RT: 317\n",
            "  ⚠️ Error accessing 433 in Odometer_spreadsheet_BT: 433\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_FG: 409\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_MB: 317\n",
            "  ⚠️ Error accessing 557 in Odometer_spreadsheet_SVP: 557\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_BT: 409\n",
            "  ⚠️ Error accessing Grease Gun in Odometer_spreadsheet_FG: Grease Gun\n",
            "  ⚠️ Error accessing 433 in Odometer_spreadsheet_RT: 433\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_DP: 317\n",
            "  ⚠️ No values found for bus 317 in any spreadsheet\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_RT: 409\n",
            "  ⚠️ Error accessing 433 in Odometer_spreadsheet_MB: 433\n",
            "  ⚠️ Error accessing Grease Gun in Odometer_spreadsheet_BT: Grease Gun\n",
            "  ⚠️ Error accessing 557 in Odometer_spreadsheet_FG: 557\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_MB: 409\n",
            "  ⚠️ Error accessing 433 in Odometer_spreadsheet_DP: 433\n",
            "  ⚠️ No values found for bus 433 in any spreadsheet\n",
            "  ⚠️ Error accessing Grease Gun in Odometer_spreadsheet_RT: Grease Gun\n",
            "  ⚠️ Error accessing 557 in Odometer_spreadsheet_BT: 557\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_DP: 409\n",
            "  ⚠️ No values found for bus 409 in any spreadsheet\n",
            "  ⚠️ Error accessing Grease Gun in Odometer_spreadsheet_MB: Grease Gun\n",
            "  ⚠️ Error accessing 557 in Odometer_spreadsheet_RT: 557\n",
            "  ⚠️ Error accessing Grease Gun in Odometer_spreadsheet_DP: Grease Gun\n",
            "  ⚠️ No values found for bus Grease Gun in any spreadsheet\n",
            "  ⚠️ Error accessing 557 in Odometer_spreadsheet_MB: 557\n",
            "  ⚠️ Error accessing 557 in Odometer_spreadsheet_DP: 557\n",
            "  ⚠️ No values found for bus 557 in any spreadsheet\n",
            "\n",
            "✅ Found values for 0 exact date matches\n",
            "✅ Found values for 0 fallback date matches\n",
            "❌ Could not find values for 47 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CH_BSIV_CH_MAS_ATR...\n",
            "✅ Prepared 0 cell updates\n",
            "✓ No updates needed for this worksheet\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_ATR (4/6)\n",
            "================================================================================\n",
            "⏳ Waiting 7.10s before processing next worksheet...\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_ATR\n",
            "🔍 Starting data processing for EMPTY L cells only...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 35 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates for empty L cells...\n",
            "✅ Found 1 unique bus numbers with empty L cells\n",
            "✅ Found 1 empty L cells to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "  ⚠️ Error accessing 600 in Odometer_spreadsheet_SVP: 600\n",
            "  ⚠️ Error accessing 600 in Odometer_spreadsheet_FG: 600\n",
            "  ⚠️ Error accessing 600 in Odometer_spreadsheet_BT: 600\n",
            "  ⚠️ Error accessing 600 in Odometer_spreadsheet_RT: 600\n",
            "  ⚠️ Error accessing 600 in Odometer_spreadsheet_MB: 600\n",
            "  ⚠️ Error accessing 600 in Odometer_spreadsheet_DP: 600\n",
            "  ⚠️ No values found for bus 600 in any spreadsheet\n",
            "\n",
            "✅ Found values for 0 exact date matches\n",
            "✅ Found values for 0 fallback date matches\n",
            "❌ Could not find values for 1 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CK_BSVI_CK_MAS_ATR...\n",
            "✅ Prepared 0 cell updates\n",
            "✓ No updates needed for this worksheet\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_rr (5/6)\n",
            "================================================================================\n",
            "⏳ Waiting 9.63s before processing next worksheet...\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_rr\n",
            "🔍 Starting data processing for EMPTY L cells only...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 932 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates for empty L cells...\n",
            "✅ Found 28 unique bus numbers with empty L cells\n",
            "✅ Found 200 empty L cells to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "  ⚠️ Error accessing 326 in Odometer_spreadsheet_SVP: 326\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_SVP: 317\n",
            "  ⚠️ Error accessing 326 in Odometer_spreadsheet_FG: 326\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_FG: 317\n",
            "  ⚠️ Error accessing 326 in Odometer_spreadsheet_BT: 326\n",
            "  ⚠️ Error accessing 477 in Odometer_spreadsheet_SVP: 477\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_BT: 317\n",
            "  ⚠️ Error accessing 322 in Odometer_spreadsheet_SVP: 322\n",
            "  ⚠️ Error accessing 326 in Odometer_spreadsheet_RT: 326\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_RT: 317\n",
            "  ⚠️ Error accessing 477 in Odometer_spreadsheet_FG: 477\n",
            "  ⚠️ Error accessing 342 in Odometer_spreadsheet_SVP: 342\n",
            "  ⚠️ Error accessing 326 in Odometer_spreadsheet_MB: 326\n",
            "  ⚠️ Error accessing 477 in Odometer_spreadsheet_BT: 477\n",
            "  ⚠️ Error accessing 322 in Odometer_spreadsheet_FG: 322\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_MB: 317\n",
            "  ⚠️ Error accessing 326 in Odometer_spreadsheet_DP: 326\n",
            "  ⚠️ No values found for bus 326 in any spreadsheet\n",
            "  ⚠️ Error accessing 342 in Odometer_spreadsheet_FG: 342\n",
            "  ⚠️ Error accessing 322 in Odometer_spreadsheet_BT: 322\n",
            "  ⚠️ Error accessing 317 in Odometer_spreadsheet_DP: 317\n",
            "  ⚠️ No values found for bus 317 in any spreadsheet\n",
            "  ⚠️ Error accessing 477 in Odometer_spreadsheet_RT: 477\n",
            "  ⚠️ Error accessing 342 in Odometer_spreadsheet_BT: 342\n",
            "  ⚠️ Error accessing 477 in Odometer_spreadsheet_MB: 477\n",
            "  ⚠️ Error accessing 322 in Odometer_spreadsheet_RT: 322\n",
            "  ⚠️ Error accessing 342 in Odometer_spreadsheet_RT: 342\n",
            "  ⚠️ Error accessing 477 in Odometer_spreadsheet_DP: 477\n",
            "  ⚠️ No values found for bus 477 in any spreadsheet\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_SVP: 381\n",
            "  ⚠️ Error accessing 322 in Odometer_spreadsheet_MB: 322\n",
            "  ⚠️ Error accessing 342 in Odometer_spreadsheet_MB: 342\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_SVP: 319\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_FG: 381\n",
            "  ⚠️ Error accessing 322 in Odometer_spreadsheet_DP: 322\n",
            "  ⚠️ No values found for bus 322 in any spreadsheet\n",
            "  ⚠️ Error accessing 342 in Odometer_spreadsheet_DP: 342\n",
            "  ⚠️ No values found for bus 342 in any spreadsheet\n",
            "  ⚠️ Error accessing 443 in Odometer_spreadsheet_SVP: 443\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_FG: 319\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_BT: 381\n",
            "  ⚠️ Error accessing 443 in Odometer_spreadsheet_FG: 443\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_SVP: 576\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_BT: 319\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_RT: 381\n",
            "  ⚠️ Error accessing 443 in Odometer_spreadsheet_BT: 443\n",
            "  ⚠️ Error accessing RCS in Odometer_spreadsheet_SVP: RCS\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_MB: 381\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_RT: 319\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_FG: 576\n",
            "  ⚠️ Error accessing 381 in Odometer_spreadsheet_DP: 381\n",
            "  ⚠️ No values found for bus 381 in any spreadsheet\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_MB: 319\n",
            "  ⚠️ Error accessing 443 in Odometer_spreadsheet_RT: 443\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_BT: 576\n",
            "  ⚠️ Error accessing RCS in Odometer_spreadsheet_FG: RCS\n",
            "  ⚠️ Error accessing 319 in Odometer_spreadsheet_DP: 319\n",
            "  ⚠️ No values found for bus 319 in any spreadsheet\n",
            "  ⚠️ Error accessing 443 in Odometer_spreadsheet_MB: 443\n",
            "  ⚠️ Error accessing RCS in Odometer_spreadsheet_BT: RCS\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_RT: 576\n",
            "  ⚠️ Error accessing 443 in Odometer_spreadsheet_DP: 443\n",
            "  ⚠️ No values found for bus 443 in any spreadsheet\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_MB: 576\n",
            "  ⚠️ Error accessing RCS in Odometer_spreadsheet_RT: RCS\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_SVP: 419\n",
            "  ⚠️ Error accessing RCS in Odometer_spreadsheet_MB: RCS\n",
            "  ⚠️ Error accessing 576 in Odometer_spreadsheet_DP: 576\n",
            "  ⚠️ No values found for bus 576 in any spreadsheet\n",
            "  ⚠️ Error accessing 341 in Odometer_spreadsheet_SVP: 341\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_FG: 419\n",
            "  ⚠️ Error accessing RCS in Odometer_spreadsheet_DP: RCS\n",
            "  ⚠️ No values found for bus RCS in any spreadsheet\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_SVP: 553\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_BT: 419\n",
            "  ⚠️ Error accessing 341 in Odometer_spreadsheet_FG: 341\n",
            "  ⚠️ Error accessing 314 in Odometer_spreadsheet_SVP: 314\n",
            "  ⚠️ Error accessing 341 in Odometer_spreadsheet_BT: 341\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_FG: 553\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_RT: 419\n",
            "  ⚠️ Error accessing 513 in Odometer_spreadsheet_SVP: 513\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_MB: 419\n",
            "  ⚠️ Error accessing 314 in Odometer_spreadsheet_FG: 314\n",
            "  ⚠️ Error accessing 341 in Odometer_spreadsheet_RT: 341\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_BT: 553\n",
            "  ⚠️ Error accessing 513 in Odometer_spreadsheet_FG: 513\n",
            "  ⚠️ Error accessing 419 in Odometer_spreadsheet_DP: 419\n",
            "  ⚠️ No values found for bus 419 in any spreadsheet\n",
            "  ⚠️ Error accessing 314 in Odometer_spreadsheet_BT: 314\n",
            "  ⚠️ Error accessing 341 in Odometer_spreadsheet_MB: 341\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_RT: 553\n",
            "  ⚠️ Error accessing 314 in Odometer_spreadsheet_RT: 314\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_MB: 553\n",
            "  ⚠️ Error accessing 513 in Odometer_spreadsheet_BT: 513\n",
            "  ⚠️ Error accessing 341 in Odometer_spreadsheet_DP: 341\n",
            "  ⚠️ No values found for bus 341 in any spreadsheet\n",
            "  ⚠️ Error accessing 314 in Odometer_spreadsheet_MB: 314\n",
            "  ⚠️ Error accessing 553 in Odometer_spreadsheet_DP: 553\n",
            "  ⚠️ No values found for bus 553 in any spreadsheet\n",
            "  ⚠️ Error accessing 513 in Odometer_spreadsheet_RT: 513\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_SVP: 544\n",
            "  ⚠️ Error accessing 314 in Odometer_spreadsheet_DP: 314\n",
            "  ⚠️ No values found for bus 314 in any spreadsheet\n",
            "  ⚠️ Error accessing 513 in Odometer_spreadsheet_MB: 513\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_FG: 544\n",
            "  ⚠️ Error accessing 352 in Odometer_spreadsheet_SVP: 352\n",
            "  ⚠️ Error accessing 513 in Odometer_spreadsheet_DP: 513\n",
            "  ⚠️ No values found for bus 513 in any spreadsheet\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_BT: 544\n",
            "  ⚠️ Error accessing 487 in Odometer_spreadsheet_SVP: 487\n",
            "  ⚠️ Error accessing 352 in Odometer_spreadsheet_FG: 352\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_RT: 544\n",
            "  ⚠️ Error accessing ES in Odometer_spreadsheet_SVP: ES\n",
            "  ⚠️ Error accessing 352 in Odometer_spreadsheet_BT: 352\n",
            "  ⚠️ Error accessing 487 in Odometer_spreadsheet_FG: 487\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_MB: 544\n",
            "  ⚠️ Error accessing ES in Odometer_spreadsheet_FG: ES\n",
            "  ⚠️ Error accessing 487 in Odometer_spreadsheet_BT: 487\n",
            "  ⚠️ Error accessing 544 in Odometer_spreadsheet_DP: 544\n",
            "  ⚠️ No values found for bus 544 in any spreadsheet\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_SVP: 368\n",
            "  ⚠️ Error accessing 352 in Odometer_spreadsheet_RT: 352\n",
            "  ⚠️ Error accessing ES in Odometer_spreadsheet_BT: ES\n",
            "  ⚠️ Error accessing 487 in Odometer_spreadsheet_RT: 487\n",
            "  ⚠️ Error accessing 352 in Odometer_spreadsheet_MB: 352\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_FG: 368\n",
            "  ⚠️ Error accessing 487 in Odometer_spreadsheet_MB: 487\n",
            "  ⚠️ Error accessing ES in Odometer_spreadsheet_RT: ES\n",
            "  ⚠️ Error accessing 352 in Odometer_spreadsheet_DP: 352\n",
            "  ⚠️ No values found for bus 352 in any spreadsheet\n",
            "  ⚠️ Error accessing D.G.SET in Odometer_spreadsheet_SVP: D.G.SET\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_BT: 368\n",
            "  ⚠️ Error accessing ES in Odometer_spreadsheet_MB: ES\n",
            "  ⚠️ Error accessing 487 in Odometer_spreadsheet_DP: 487\n",
            "  ⚠️ No values found for bus 487 in any spreadsheet\n",
            "  ⚠️ Error accessing D.G.SET in Odometer_spreadsheet_FG: D.G.SET\n",
            "  ⚠️ Error accessing ES in Odometer_spreadsheet_DP: ES\n",
            "  ⚠️ No values found for bus ES in any spreadsheet\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_SVP: 435\n",
            "  ⚠️ Error accessing D.G.SET in Odometer_spreadsheet_BT: D.G.SET\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_RT: 368\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_SVP: 409\n",
            "  ⚠️ Error accessing D.G.SET in Odometer_spreadsheet_RT: D.G.SET\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_MB: 368\n",
            "  ⚠️ Error accessing 368 in Odometer_spreadsheet_DP: 368\n",
            "  ⚠️ No values found for bus 368 in any spreadsheet\n",
            "  ⚠️ Error accessing AirComp. in Odometer_spreadsheet_SVP: AirComp.\n",
            "  ⚠️ Error accessing D.G.SET in Odometer_spreadsheet_MB: D.G.SET\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_FG: 435\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_FG: 409\n",
            "⏳ Rate limit hit for bus AirComp., waiting 3.46s (retry 1/5)\n",
            "  ⚠️ Error accessing D.G.SET in Odometer_spreadsheet_DP: D.G.SET\n",
            "  ⚠️ No values found for bus D.G.SET in any spreadsheet\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_BT: 435\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_BT: 409\n",
            "⏳ Rate limit hit for bus 409, waiting 2.60s (retry 1/5)\n",
            "  ⚠️ Error accessing 482 in Odometer_spreadsheet_SVP: 482\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_RT: 435\n",
            "⏳ Rate limit hit for bus 482, waiting 3.19s (retry 1/5)\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_MB: 435\n",
            "  ⚠️ Error accessing 552 in Odometer_spreadsheet_SVP: 552\n",
            "⏳ Rate limit hit for bus 552, waiting 3.11s (retry 1/5)\n",
            "  ⚠️ Error accessing 435 in Odometer_spreadsheet_DP: 435\n",
            "  ⚠️ No values found for bus 435 in any spreadsheet\n",
            "  ⚠️ Error accessing 518 in Odometer_spreadsheet_SVP: 518\n",
            "⏳ Rate limit hit for bus 518, waiting 2.60s (retry 1/5)\n",
            "⏳ Rate limit hit for bus 409, waiting 4.41s (retry 2/5)\n",
            "⏳ Rate limit hit for bus 518, waiting 5.11s (retry 2/5)\n",
            "⏳ Rate limit hit for bus 552, waiting 5.86s (retry 2/5)\n",
            "⏳ Rate limit hit for bus 482, waiting 5.01s (retry 2/5)\n",
            "⏳ Rate limit hit for bus AirComp., waiting 5.93s (retry 2/5)\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_RT: 409\n",
            "⏳ Rate limit hit for bus 409, waiting 3.96s (retry 1/5)\n",
            "⏳ Rate limit hit for bus 518, waiting 9.91s (retry 3/5)\n",
            "  ⚠️ Error accessing 482 in Odometer_spreadsheet_FG: 482\n",
            "⏳ Rate limit hit for bus 482, waiting 2.23s (retry 1/5)\n",
            "  ⚠️ Error accessing 552 in Odometer_spreadsheet_FG: 552\n",
            "  ⚠️ Error accessing 552 in Odometer_spreadsheet_BT: 552\n",
            "  ⚠️ Error accessing 552 in Odometer_spreadsheet_RT: 552\n",
            "  ⚠️ Error accessing 552 in Odometer_spreadsheet_MB: 552\n",
            "⏳ Rate limit hit for bus 552, waiting 2.60s (retry 1/5)\n",
            "  ⚠️ Error accessing AirComp. in Odometer_spreadsheet_FG: AirComp.\n",
            "  ⚠️ Error accessing AirComp. in Odometer_spreadsheet_BT: AirComp.\n",
            "  ⚠️ Error accessing AirComp. in Odometer_spreadsheet_RT: AirComp.\n",
            "  ⚠️ Error accessing AirComp. in Odometer_spreadsheet_MB: AirComp.\n",
            "  ⚠️ Error accessing AirComp. in Odometer_spreadsheet_DP: AirComp.\n",
            "  ⚠️ No values found for bus AirComp. in any spreadsheet\n",
            "⏳ Rate limit hit for bus B/T, waiting 2.68s (retry 1/5)\n",
            "  ⚠️ Error accessing 482 in Odometer_spreadsheet_BT: 482\n",
            "  ⚠️ Error accessing 482 in Odometer_spreadsheet_RT: 482\n",
            "  ⚠️ Error accessing 482 in Odometer_spreadsheet_MB: 482\n",
            "  ⚠️ Error accessing 482 in Odometer_spreadsheet_DP: 482\n",
            "  ⚠️ No values found for bus 482 in any spreadsheet\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_MB: 409\n",
            "  ⚠️ Error accessing 409 in Odometer_spreadsheet_DP: 409\n",
            "  ⚠️ No values found for bus 409 in any spreadsheet\n",
            "  ⚠️ Error accessing 552 in Odometer_spreadsheet_DP: 552\n",
            "  ⚠️ No values found for bus 552 in any spreadsheet\n",
            "  ⚠️ Error accessing B/T in Odometer_spreadsheet_SVP: B/T\n",
            "  ⚠️ Error accessing B/T in Odometer_spreadsheet_FG: B/T\n",
            "  ⚠️ Error accessing B/T in Odometer_spreadsheet_BT: B/T\n",
            "  ⚠️ Error accessing B/T in Odometer_spreadsheet_RT: B/T\n",
            "  ⚠️ Error accessing B/T in Odometer_spreadsheet_MB: B/T\n",
            "  ⚠️ Error accessing B/T in Odometer_spreadsheet_DP: B/T\n",
            "  ⚠️ No values found for bus B/T in any spreadsheet\n",
            "  ⚠️ Error accessing 518 in Odometer_spreadsheet_FG: 518\n",
            "  ⚠️ Error accessing 518 in Odometer_spreadsheet_BT: 518\n",
            "  ⚠️ Error accessing 518 in Odometer_spreadsheet_RT: 518\n",
            "  ⚠️ Error accessing 518 in Odometer_spreadsheet_MB: 518\n",
            "  ⚠️ Error accessing 518 in Odometer_spreadsheet_DP: 518\n",
            "  ⚠️ No values found for bus 518 in any spreadsheet\n",
            "\n",
            "✅ Found values for 0 exact date matches\n",
            "✅ Found values for 0 fallback date matches\n",
            "❌ Could not find values for 200 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CH_BSIV_CH_MAS_rr...\n",
            "✅ Prepared 0 cell updates\n",
            "✓ No updates needed for this worksheet\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_rr (6/6)\n",
            "================================================================================\n",
            "⏳ Waiting 9.04s before processing next worksheet...\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_rr\n",
            "🔍 Starting data processing for EMPTY L cells only...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 12 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates for empty L cells...\n",
            "✅ Found 1 unique bus numbers with empty L cells\n",
            "✅ Found 1 empty L cells to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "  ⚠️ Error accessing 610 in Odometer_spreadsheet_SVP: 610\n",
            "  ⚠️ Error accessing 610 in Odometer_spreadsheet_FG: 610\n",
            "  ⚠️ Error accessing 610 in Odometer_spreadsheet_BT: 610\n",
            "  ⚠️ Error accessing 610 in Odometer_spreadsheet_RT: 610\n",
            "  ⚠️ Error accessing 610 in Odometer_spreadsheet_MB: 610\n",
            "  ⚠️ Error accessing 610 in Odometer_spreadsheet_DP: 610\n",
            "  ⚠️ No values found for bus 610 in any spreadsheet\n",
            "\n",
            "✅ Found values for 0 exact date matches\n",
            "✅ Found values for 0 fallback date matches\n",
            "❌ Could not find values for 1 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CK_BSVI_CK_MAS_rr...\n",
            "✅ Prepared 0 cell updates\n",
            "✓ No updates needed for this worksheet\n",
            "\n",
            "✅ All worksheets have been processed successfully!\n",
            "💡 Performance Summary:\n",
            "✓ Processed 6 worksheets with LEAN mode (empty L cells only)\n",
            "✓ Searched across 6 odometer spreadsheets\n",
            "✓ Used parallel processing for bus data with ThreadPoolExecutor\n",
            "✓ Implemented caching to reduce API calls\n",
            "✓ Used batch updates to minimize API requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # High Speed, Lean for all units, improved API limiter, #Lean combined all Units Engine Oil and Search all Odometer, but it search only missing values, if it works, then in future just add engine oil sheet and odometer sheet\n",
        "\n",
        "\n",
        "\n",
        "# The above code is returning value in Column N, i dont need it, also incorporate batch processing to increase speed"
      ],
      "metadata": {
        "id": "J09_k_HVL0ea"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGEUEmt0Qi6A0jZ+WoOd6D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}