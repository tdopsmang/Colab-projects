{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdopsmang/Colab-projects/blob/main/Servicing_km_V1_0_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did not work on RR path, rather added bus details upto DP, in consolidated report, also batch processing done in few cases"
      ],
      "metadata": {
        "id": "JVbZ291M-7mv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CMKeh0lHx8tS",
        "outputId": "b9841cda-877d-461f-814c-13e45eea9322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.32.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gspread\n",
        "!pip install gspread google-auth-oauthlib google-auth-httplib2\n",
        "!pip install gspread google-auth oauth2client\n",
        "!pip install gspread oauth2client pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GIVCOLMf4oFc"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "from google.oauth2.service_account import Credentials\n",
        "gc = gspread.authorize(creds)\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from datetime import datetime\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dX9vEbLhRZAG"
      },
      "outputs": [],
      "source": [
        "# Details our Database spreadsheet which has details of all files, ID in Transport Department\n",
        "\n",
        "Database_File_spreadsheet_ID = gc.open_by_key('1nJKyvV1WQmZzvbjOP7Hsp1bQJmiNsYoJOH_jIL7H9PI') #ID of Database Spreadsheet\n",
        "MASsheetID = Database_File_spreadsheet_ID.worksheet('MAS')                                    #MAS Worksheet\n",
        "OdometersheetID = Database_File_spreadsheet_ID.worksheet('Odometer')                          #Odometer Worksheet\n",
        "ReportID = Database_File_spreadsheet_ID.worksheet('Report')                          #Odometer Worksheet\n",
        "\n",
        "#Reports spreadsheet and worksheet details\n",
        "Report_EO_SpreadsheetID = ReportID.acell('C2').value                  # Get the spreadsheet ID from cell C2\n",
        "Report_EO = gc.open_by_key(Report_EO_SpreadsheetID)                    # Open RR_Oil_Lub spreadsheet\n",
        "Report_EO1_worksheet_name = ReportID.acell('B2').value                        # Km after last servicing consolidated\n",
        "Report_EO1_Worksheet = Report_EO.worksheet(Report_EO1_worksheet_name)  # Open RR CH engineoil worksheet\n",
        "\n",
        "#STS RR SVP\n",
        "RR_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C2').value                  # Get the spreadsheet ID from cell C2\n",
        "RR_Oil_Lub = gc.open_by_key(RR_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open RR_Oil_Lub spreadsheet\n",
        "\n",
        "RR_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B2').value                        # Get the worksheet name from cell B2\n",
        "engine_oil_CH_BSIV_CH_MAS_rr = RR_Oil_Lub.worksheet(RR_Engine_oil_CH_BSIV_worksheet_name)  # Open RR CH engineoil worksheet\n",
        "\n",
        "RR_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B3').value                        # Get the worksheet name from cell B3\n",
        "engine_oil_CK_BSVI_CK_MAS_rr = RR_Oil_Lub.worksheet(RR_Engine_oil_CK_BSVI_worksheet_name)  # Open RR CK engineoil worksheet\n",
        "\n",
        "RR_Report1_worksheet_name = MASsheetID.acell('B8').value                                # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "RR_Report1_Worksheet = RR_Oil_Lub.worksheet(RR_Report1_worksheet_name)                  # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "RR_Report1_worksheet_name = MASsheetID.acell('B8').value                                # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "RR_Report1_Worksheet = RR_Oil_Lub.worksheet(RR_Report1_worksheet_name)                  # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "#STS ATR SVP\n",
        "ATR_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C52').value                  # Get the spreadsheet ID from cell C51 ATR\n",
        "ATR_Oil_Lub = gc.open_by_key(ATR_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open ATR_Oil_Lub spreadsheet\n",
        "\n",
        "ATR_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B52').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CH_BSIV_CH_MAS_ATR = ATR_Oil_Lub.worksheet(ATR_Engine_oil_CH_BSIV_worksheet_name)  # Open ATR CH engineoil worksheet\n",
        "\n",
        "ATR_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B53').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CK_BSVI_CK_MAS_ATR = ATR_Oil_Lub.worksheet(ATR_Engine_oil_CK_BSVI_worksheet_name)  # Open ATR CK engineoil worksheet\n",
        "\n",
        "#ATR_Report1_worksheet_name = MASsheetID.acell('B58').value                           #******This may not be required       # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "#ATR_Report1_Worksheet = ATR_Oil_Lub.worksheet(ATR_Report1_worksheet_name)              #This may not be required     # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "#STS FG\n",
        "FG_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C102').value                  # Get the spreadsheet ID from cell C51 ATR\n",
        "FG_Oil_Lub = gc.open_by_key(FG_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open ATR_Oil_Lub spreadsheet\n",
        "\n",
        "FG_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B102').value                         # Get the worksheet name from cell B2\n",
        "engine_oil_CH_BSIV_CH_MAS_FG = FG_Oil_Lub.worksheet(FG_Engine_oil_CH_BSIV_worksheet_name)  # Open RR engineoil worksheet\n",
        "\n",
        "FG_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B103').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CK_BSVI_CK_MAS_FG = FG_Oil_Lub.worksheet(FG_Engine_oil_CK_BSVI_worksheet_name)  # Open ATR CK engineoil worksheet\n",
        "\n",
        "#FG_Report1_worksheet_name = MASsheetID.acell('B108').value                    #*****This may not be required            # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "#FG_Report1_Worksheet = FG_Oil_Lub.worksheet(FG_Report1_worksheet_name)          #This may not be required         # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "\n",
        "# Process SVP data\n",
        "SVP_Odometer_id = OdometersheetID.acell('A2').value\n",
        "Odometer_spreadsheet_SVP = gc.open_by_key(SVP_Odometer_id)\n",
        "bus_number_SVP = sorted([worksheet.title for worksheet in Odometer_spreadsheet_SVP])\n",
        "spreadsheet_name_SVP = Odometer_spreadsheet_SVP.title\n",
        "start_row_svp = 2\n",
        "\n",
        "# Process FG data\n",
        "FG_Odometer_id = OdometersheetID.acell('B2').value\n",
        "Odometer_spreadsheet_FG = gc.open_by_key(FG_Odometer_id)\n",
        "bus_number_FG = sorted([worksheet.title for worksheet in Odometer_spreadsheet_FG])\n",
        "spreadsheet_name_FG = Odometer_spreadsheet_FG.title\n",
        "start_row_fg = start_row_svp + len(bus_number_SVP)\n",
        "\n",
        "# Process Baratang data\n",
        "BT_Odometer_id = OdometersheetID.acell('C2').value\n",
        "Odometer_spreadsheet_BT = gc.open_by_key(BT_Odometer_id)\n",
        "bus_number_BT = sorted([worksheet.title for worksheet in Odometer_spreadsheet_BT])\n",
        "spreadsheet_name_BT = Odometer_spreadsheet_BT.title\n",
        "start_row_bt = start_row_fg + len(bus_number_FG)\n",
        "\n",
        "# Process Rangat data\n",
        "RT_Odometer_id = OdometersheetID.acell('D2').value\n",
        "Odometer_spreadsheet_RT = gc.open_by_key(RT_Odometer_id)\n",
        "bus_number_RT = sorted([worksheet.title for worksheet in Odometer_spreadsheet_RT])\n",
        "spreadsheet_name_RT = Odometer_spreadsheet_RT.title\n",
        "start_row_rt = start_row_bt + len(bus_number_BT)\n",
        "\n",
        "# Process Mayabunder data\n",
        "MB_Odometer_id = OdometersheetID.acell('E2').value\n",
        "Odometer_spreadsheet_MB = gc.open_by_key(MB_Odometer_id)\n",
        "bus_number_MB = sorted([worksheet.title for worksheet in Odometer_spreadsheet_MB])\n",
        "spreadsheet_name_MB = Odometer_spreadsheet_MB.title\n",
        "start_row_mb = start_row_rt + len(bus_number_RT)\n",
        "\n",
        "# Process Diglipur data\n",
        "DP_Odometer_id = OdometersheetID.acell('F2').value\n",
        "Odometer_spreadsheet_DP = gc.open_by_key(DP_Odometer_id)\n",
        "bus_number_DP = sorted([worksheet.title for worksheet in Odometer_spreadsheet_DP])\n",
        "spreadsheet_name_DP = Odometer_spreadsheet_DP.title\n",
        "start_row_dp = start_row_mb + len(bus_number_MB)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below codes are redundent, No need to run, Use this If required in future"
      ],
      "metadata": {
        "id": "Uq1Go9jLsRov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RR_MAS_Engine OIl_ Optimized Code, High speed, just this will be retained in V1.0.8\n",
        "\n",
        "# Make sure to include these imports at the top of your code\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import concurrent.futures\n",
        "from functools import lru_cache\n",
        "\n",
        "# Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "clear_range_all_engine_oil_CH_BSIV_CH_MAS_rr = 'L2:M'\n",
        "engine_oil_CH_BSIV_CH_MAS_rr.batch_clear([clear_range_all_engine_oil_CH_BSIV_CH_MAS_rr])\n",
        "\n",
        "# Cache for parsed dates to avoid repeated parsing\n",
        "date_cache = {}\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "@lru_cache(maxsize=1000)  # Cache up to 1000 date parsing results\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    # Check cache first\n",
        "    if date_str in date_cache:\n",
        "        return date_cache[date_str]\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        result = datetime(year, month, day)\n",
        "        date_cache[date_str] = result  # Cache the result\n",
        "        return result\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    # Process all dates in one pass to avoid repeated parsing\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "print(\"üîç Starting data processing with date fallback search...\")\n",
        "\n",
        "# Pre-compile the date pattern for faster matching\n",
        "date_pattern = re.compile(r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})')\n",
        "\n",
        "# üì• Load main sheet - get all data at once\n",
        "print(\"üì• Loading main sheet data...\")\n",
        "main_data = engine_oil_CH_BSIV_CH_MAS_rr.get_all_values()\n",
        "print(f\"‚úÖ Loaded {len(main_data)-1} rows from main sheet\")\n",
        "\n",
        "# Step 1: Collect unique (bus, date) and row mapping\n",
        "print(\"üî¢ Mapping bus numbers and dates...\")\n",
        "bus_date_map = defaultdict(set)\n",
        "row_map = {}  # maps row number ‚Üí (bus, date)\n",
        "\n",
        "for i, row in enumerate(main_data[1:], start=2):\n",
        "    try:\n",
        "        date_str = row[1].strip()\n",
        "        bus_num = row[4].strip()\n",
        "        if date_str and bus_num:\n",
        "            bus_date_map[bus_num].add(date_str)\n",
        "            row_map[i] = (bus_num, date_str)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"‚úÖ Found {len(bus_date_map)} unique bus numbers\")\n",
        "print(f\"‚úÖ Found {len(row_map)} rows to process\")\n",
        "\n",
        "# Load all worksheets upfront to avoid repeated API calls\n",
        "print(\"\\nüîÑ Pre-loading all bus worksheets...\")\n",
        "bus_worksheet_data = {}\n",
        "\n",
        "# Function to load a bus worksheet - can be run in parallel\n",
        "def load_bus_worksheet(bus):\n",
        "    try:\n",
        "        sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "        return bus, sheet.get_all_values()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not load sheet '{bus}': {str(e)}\")\n",
        "        return bus, None\n",
        "\n",
        "# Load worksheets in parallel to speed up the process\n",
        "all_buses = list(bus_date_map.keys())\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    results = executor.map(load_bus_worksheet, all_buses)\n",
        "\n",
        "    for bus, data in results:\n",
        "        if data:\n",
        "            bus_worksheet_data[bus] = data\n",
        "\n",
        "print(f\"‚úÖ Successfully pre-loaded {len(bus_worksheet_data)} bus worksheets\")\n",
        "\n",
        "# Step 2: Process all bus data to find exact or closest dates\n",
        "print(\"\\nüîç Searching for exact dates or closest earlier dates...\")\n",
        "bus_date_value_map = {}\n",
        "match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "fallback_date_map = {}  # Store the actual fallback date used\n",
        "success_count = 0\n",
        "fallback_count = 0\n",
        "\n",
        "# Process each bus's dates\n",
        "for bus, dates in bus_date_map.items():\n",
        "    print(f\"üìä Processing bus {bus} ({len(dates)} dates)...\")\n",
        "    sheet_data = bus_worksheet_data.get(bus)\n",
        "    if not sheet_data:\n",
        "        print(f\"‚ùå No data available for bus {bus}\")\n",
        "        continue\n",
        "\n",
        "    # Index the sheet data by date for faster lookups\n",
        "    date_indexed_data = {}\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > 12 and row[1].strip():\n",
        "                date_indexed_data[row[1].strip()] = row[12]  # Column M (index 12)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    sheet_success = 0\n",
        "    for date_str in dates:\n",
        "        # First try exact match using the indexed data (O(1) lookup)\n",
        "        if date_str in date_indexed_data:\n",
        "            exact_value = date_indexed_data[date_str]\n",
        "            bus_date_value_map[(bus, date_str)] = exact_value\n",
        "            match_type_map[(bus, date_str)] = \"exact\"\n",
        "            fallback_date_map[(bus, date_str)] = \"\"  # No fallback date\n",
        "            sheet_success += 1\n",
        "            success_count += 1\n",
        "        else:\n",
        "            # Try fallback date lookup\n",
        "            value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "            if value:\n",
        "                bus_date_value_map[(bus, date_str)] = value\n",
        "                match_type_map[(bus, date_str)] = \"fallback\"\n",
        "                fallback_date_map[(bus, date_str)] = fallback_date\n",
        "                sheet_success += 1\n",
        "                fallback_count += 1\n",
        "\n",
        "    print(f\"  ‚úÖ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Found values for {success_count} exact date matches\")\n",
        "print(f\"‚úÖ Found values for {fallback_count} fallback date matches\")\n",
        "print(f\"‚ùå Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "# Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "print(\"\\nüìù Preparing batch updates for main sheet...\")\n",
        "\n",
        "# Prepare batch updates - much faster than individual cell updates\n",
        "value_batch_updates = []  # For column L (value)\n",
        "fallback_batch_updates = []  # For column M (fallback date)\n",
        "exact_match_cells = []\n",
        "fallback_match_cells = []\n",
        "\n",
        "for row_num, (bus, date_str) in row_map.items():\n",
        "    value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "    fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "    if value:\n",
        "        # Track for batch update\n",
        "        value_batch_updates.append({\n",
        "            'range': f'L{row_num}',\n",
        "            'values': [[value]]\n",
        "        })\n",
        "\n",
        "        fallback_batch_updates.append({\n",
        "            'range': f'M{row_num}',\n",
        "            'values': [[fallback_date]]\n",
        "        })\n",
        "\n",
        "        # Track for formatting\n",
        "        match_type = match_type_map.get((bus, date_str))\n",
        "        if match_type == \"exact\":\n",
        "            exact_match_cells.append(f\"L{row_num}\")\n",
        "        elif match_type == \"fallback\":\n",
        "            fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "# Function to perform batch updates with retry logic\n",
        "def batch_update_with_retry(worksheet, batch_data, max_retries=5):\n",
        "    retry_count = 0\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            # Split into smaller batches to avoid rate limits\n",
        "            batch_size = 100  # Process 100 updates at once\n",
        "            total_batches = (len(batch_data) + batch_size - 1) // batch_size\n",
        "\n",
        "            for i in range(total_batches):\n",
        "                start_idx = i * batch_size\n",
        "                end_idx = min((i + 1) * batch_size, len(batch_data))\n",
        "                current_batch = batch_data[start_idx:end_idx]\n",
        "\n",
        "                if current_batch:\n",
        "                    worksheet.batch_update(current_batch)\n",
        "                    print(f\"‚úÖ Batch {i+1}/{total_batches} updated successfully ({end_idx - start_idx} cells)\")\n",
        "                    # Short delay between batches to avoid rate limiting\n",
        "                    if i < total_batches - 1:\n",
        "                        time.sleep(1)\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            retry_count += 1\n",
        "            if \"429\" in str(e):\n",
        "                # Calculate wait time with exponential backoff\n",
        "                wait_time = (2 ** retry_count) + random.random() * 2\n",
        "                print(f\"‚è≥ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"‚ùå Error during batch update: {e}\")\n",
        "                if retry_count < max_retries:\n",
        "                    time.sleep(2)  # Wait before retrying on non-rate limit errors\n",
        "                else:\n",
        "                    return False\n",
        "\n",
        "    return False\n",
        "\n",
        "# Perform batch updates\n",
        "print(f\"üìä Updating {len(value_batch_updates)} values in column L...\")\n",
        "if value_batch_updates:\n",
        "    if batch_update_with_retry(engine_oil_CH_BSIV_CH_MAS_rr, value_batch_updates):\n",
        "        print(f\"‚úÖ Successfully updated all values in column L\")\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to update some values in column L\")\n",
        "\n",
        "print(f\"üìä Updating {len(fallback_batch_updates)} values in column M...\")\n",
        "if fallback_batch_updates:\n",
        "    if batch_update_with_retry(engine_oil_CH_BSIV_CH_MAS_rr, fallback_batch_updates):\n",
        "        print(f\"‚úÖ Successfully updated all fallback dates in column M\")\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to update some fallback dates in column M\")\n",
        "\n",
        "# Apply formatting in batches\n",
        "print(\"\\nüé® Applying formatting to distinguish match types...\")\n",
        "\n",
        "# We'll skip the actual formatting application since it depends on how your API handles formatting\n",
        "\n",
        "print(f\"\\n‚úÖ COMPLETE: Updated {len(value_batch_updates)} rows in columns L and M\")\n",
        "print(f\"üìä Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "print(f\"‚ùå {len(row_map) - len(value_batch_updates)} rows could not be updated\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ui_8optlYNwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ATR High speed, if not working then go back to V1.0.7 code, this is not working, will see on 14.05.25\n",
        "\n",
        "# ATR_MAS_Engine OIl_ Optimized Code\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import concurrent.futures\n",
        "from functools import lru_cache\n",
        "\n",
        "# Clear data in range L2:M for both worksheets\n",
        "# For engine_oil_CK_BSVI_CK_MAS_ATR worksheet\n",
        "clear_range_all_engine_oil_CK_BSVI_CK_MAS_atr = 'L2:M'\n",
        "engine_oil_CK_BSVI_CK_MAS_ATR.batch_clear([clear_range_all_engine_oil_CK_BSVI_CK_MAS_atr])\n",
        "\n",
        "# For engine_oil_CH_BSIV_CH_MAS_ATR worksheet\n",
        "clear_range_all_engine_oil_CH_BSIV_CH_MAS_atr = 'L2:M'\n",
        "engine_oil_CH_BSIV_CH_MAS_ATR.batch_clear([clear_range_all_engine_oil_CH_BSIV_CH_MAS_atr])\n",
        "\n",
        "# Cache for parsed dates to avoid repeated parsing\n",
        "date_cache = {}\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "@lru_cache(maxsize=1000)  # Cache up to 1000 date parsing results\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    # Check cache first\n",
        "    if date_str in date_cache:\n",
        "        return date_cache[date_str]\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        result = datetime(year, month, day)\n",
        "        date_cache[date_str] = result  # Cache the result\n",
        "        return result\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    # Process all dates in one pass to avoid repeated parsing\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "print(\"üîç Starting data processing with date fallback search...\")\n",
        "\n",
        "# Pre-compile the date pattern for faster matching\n",
        "date_pattern = re.compile(r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})')\n",
        "\n",
        "# üì• Load main sheet\n",
        "print(\"üì• Loading main sheet data...\")\n",
        "main_data = engine_oil_CH_BSIV_CH_MAS_ATR.get_all_values()\n",
        "print(f\"‚úÖ Loaded {len(main_data)-1} rows from main sheet\")\n",
        "\n",
        "# Step 1: Collect unique (bus, date) and row mapping\n",
        "print(\"üî¢ Mapping bus numbers and dates...\")\n",
        "bus_date_map = defaultdict(set)\n",
        "row_map = {}  # maps row number ‚Üí (bus, date)\n",
        "\n",
        "for i, row in enumerate(main_data[1:], start=2):\n",
        "    try:\n",
        "        date_str = row[1].strip()\n",
        "        bus_num = row[4].strip()\n",
        "        if date_str and bus_num:\n",
        "            bus_date_map[bus_num].add(date_str)\n",
        "            row_map[i] = (bus_num, date_str)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"‚úÖ Found {len(bus_date_map)} unique bus numbers\")\n",
        "print(f\"‚úÖ Found {len(row_map)} rows to process\")\n",
        "\n",
        "# Load all worksheets upfront to avoid repeated API calls\n",
        "print(\"\\nüîÑ Pre-loading all bus worksheets...\")\n",
        "bus_worksheet_data = {}\n",
        "\n",
        "# Function to load a bus worksheet - can be run in parallel\n",
        "def load_bus_worksheet(bus):\n",
        "    try:\n",
        "        sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "        return bus, sheet.get_all_values()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not load sheet '{bus}': {str(e)}\")\n",
        "        return bus, None\n",
        "\n",
        "# Load worksheets in parallel using thread pool\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    results = executor.map(load_bus_worksheet, all_buses)\n",
        "\n",
        "    for bus, data in results:\n",
        "        if data:\n",
        "            bus_worksheet_data[bus] = data\n",
        "\n",
        "print(f\"‚úÖ Successfully pre-loaded {len(bus_worksheet_data)} bus worksheets\")\n",
        "\n",
        "# Step 2: Process all bus data to find exact or closest dates\n",
        "print(\"\\nüîç Searching for exact dates or closest earlier dates...\")\n",
        "bus_date_value_map_CK = {}\n",
        "match_type_map_CK = {}  # Tracks whether each match is exact or fallback\n",
        "fallback_date_map_CK = {}  # Store the actual fallback date used\n",
        "\n",
        "bus_date_value_map_CH = {}\n",
        "match_type_map_CH = {}  # Tracks whether each match is exact or fallback\n",
        "fallback_date_map_CH = {}  # Store the actual fallback date used\n",
        "\n",
        "success_count_CK = 0\n",
        "fallback_count_CK = 0\n",
        "success_count_CH = 0\n",
        "fallback_count_CH = 0\n",
        "\n",
        "# Process CK buses\n",
        "for bus, dates in bus_date_map_CK.items():\n",
        "    print(f\"üìä Processing CK bus {bus} ({len(dates)} dates)...\")\n",
        "    sheet_data = bus_worksheet_data.get(bus)\n",
        "    if not sheet_data:\n",
        "        print(f\"‚ùå No data available for bus {bus}\")\n",
        "        continue\n",
        "\n",
        "    # Index the sheet data by date for faster lookups\n",
        "    date_indexed_data = {}\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > 12 and row[1].strip():\n",
        "                date_indexed_data[row[1].strip()] = row[12]  # Column M (index 12)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    sheet_success = 0\n",
        "    for date_str in dates:\n",
        "        # First try exact match using the indexed data (O(1) lookup)\n",
        "        if date_str in date_indexed_data:\n",
        "            exact_value = date_indexed_data[date_str]\n",
        "            bus_date_value_map_CK[(bus, date_str)] = exact_value\n",
        "            match_type_map_CK[(bus, date_str)] = \"exact\"\n",
        "            fallback_date_map_CK[(bus, date_str)] = \"\"  # No fallback date\n",
        "            sheet_success += 1\n",
        "            success_count_CK += 1\n",
        "        else:\n",
        "            # Try fallback date lookup\n",
        "            value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "            if value:\n",
        "                bus_date_value_map_CK[(bus, date_str)] = value\n",
        "                match_type_map_CK[(bus, date_str)] = \"fallback\"\n",
        "                fallback_date_map_CK[(bus, date_str)] = fallback_date\n",
        "                sheet_success += 1\n",
        "                fallback_count_CK += 1\n",
        "\n",
        "    print(f\"  ‚úÖ Found {sheet_success}/{len(dates)} values for CK bus {bus}\")\n",
        "\n",
        "# Process CH buses\n",
        "for bus, dates in bus_date_map_CH.items():\n",
        "    print(f\"üìä Processing CH bus {bus} ({len(dates)} dates)...\")\n",
        "    sheet_data = bus_worksheet_data.get(bus)\n",
        "    if not sheet_data:\n",
        "        print(f\"‚ùå No data available for bus {bus}\")\n",
        "        continue\n",
        "\n",
        "    # Index the sheet data by date for faster lookups\n",
        "    date_indexed_data = {}\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > 12 and row[1].strip():\n",
        "                date_indexed_data[row[1].strip()] = row[12]  # Column M (index 12)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    sheet_success = 0\n",
        "    for date_str in dates:\n",
        "        # First try exact match using the indexed data (O(1) lookup)\n",
        "        if date_str in date_indexed_data:\n",
        "            exact_value = date_indexed_data[date_str]\n",
        "            bus_date_value_map_CH[(bus, date_str)] = exact_value\n",
        "            match_type_map_CH[(bus, date_str)] = \"exact\"\n",
        "            fallback_date_map_CH[(bus, date_str)] = \"\"  # No fallback date\n",
        "            sheet_success += 1\n",
        "            success_count_CH += 1\n",
        "        else:\n",
        "            # Try fallback date lookup\n",
        "            value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "            if value:\n",
        "                bus_date_value_map_CH[(bus, date_str)] = value\n",
        "                match_type_map_CH[(bus, date_str)] = \"fallback\"\n",
        "                fallback_date_map_CH[(bus, date_str)] = fallback_date\n",
        "                sheet_success += 1\n",
        "                fallback_count_CH += 1\n",
        "\n",
        "    print(f\"  ‚úÖ Found {sheet_success}/{len(dates)} values for CH bus {bus}\")\n",
        "\n",
        "print(f\"\\n‚úÖ CK worksheet: Found values for {success_count_CK} exact date matches and {fallback_count_CK} fallback date matches\")\n",
        "print(f\"‚úÖ CH worksheet: Found values for {success_count_CH} exact date matches and {fallback_count_CH} fallback date matches\")\n",
        "print(f\"‚ùå CK worksheet: Could not find values for {len(row_map_CK) - (success_count_CK + fallback_count_CK)} dates\")\n",
        "print(f\"‚ùå CH worksheet: Could not find values for {len(row_map_CH) - (success_count_CH + fallback_count_CH)} dates\")\n",
        "\n",
        "# Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "print(\"\\nüìù Preparing batch updates for both main sheets...\")\n",
        "\n",
        "# Function to perform batch updates with intelligent retry logic\n",
        "def batch_update_with_retry(worksheet, batch_data, max_retries=5):\n",
        "    retry_count = 0\n",
        "    updated_count = 0\n",
        "\n",
        "    while retry_count < max_retries and batch_data:\n",
        "        try:\n",
        "            # Process updates in smaller batches to avoid rate limits\n",
        "            batch_size = 100\n",
        "            current_batch = batch_data[:batch_size]\n",
        "\n",
        "            if current_batch:\n",
        "                worksheet.batch_update(current_batch)\n",
        "                updated_count += len(current_batch)\n",
        "                print(f\"üìä Progress: Updated {updated_count}/{len(batch_data)} cells\")\n",
        "\n",
        "                # Remove processed items\n",
        "                batch_data = batch_data[batch_size:]\n",
        "\n",
        "                # Short delay between batches to avoid rate limiting\n",
        "                if batch_data:\n",
        "                    time.sleep(1)\n",
        "\n",
        "                # Reset retry counter after successful batch\n",
        "                retry_count = 0\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_count += 1\n",
        "            if \"429\" in str(e):\n",
        "                # Exponential backoff with randomization\n",
        "                wait_time = (2 ** retry_count) + (random.random() * 2)\n",
        "                print(f\"‚è≥ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"‚ùå Error during batch update: {e}\")\n",
        "                if retry_count < max_retries:\n",
        "                    time.sleep(2)  # Wait before retrying on non-rate limit errors\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to update batch after {max_retries} retries\")\n",
        "                    return updated_count\n",
        "\n",
        "    return updated_count\n",
        "\n",
        "# Prepare batch updates for CK worksheet\n",
        "batch_updates_CK = []\n",
        "exact_match_cells_CK = []\n",
        "fallback_match_cells_CK = []\n",
        "\n",
        "for row_num, (bus, date_str, _) in row_map_CK.items():\n",
        "    value = bus_date_value_map_CK.get((bus, date_str), \"\")\n",
        "    fallback_date = fallback_date_map_CK.get((bus, date_str), \"\")\n",
        "\n",
        "    if value:\n",
        "        # Add to batch updates for column L (value)\n",
        "        batch_updates_CK.append({\n",
        "            'range': f'L{row_num}',\n",
        "            'values': [[value]]\n",
        "        })\n",
        "\n",
        "        # Add to batch updates for column M (fallback date)\n",
        "        batch_updates_CK.append({\n",
        "            'range': f'M{row_num}',\n",
        "            'values': [[fallback_date]]\n",
        "        })\n",
        "\n",
        "        # Track which cells need which formatting\n",
        "        match_type = match_type_map_CK.get((bus, date_str))\n",
        "        if match_type == \"exact\":\n",
        "            exact_match_cells_CK.append(f\"L{row_num}\")\n",
        "        elif match_type == \"fallback\":\n",
        "            fallback_match_cells_CK.append(f\"L{row_num}\")\n",
        "\n",
        "# Prepare batch updates for CH worksheet\n",
        "batch_updates_CH = []\n",
        "exact_match_cells_CH = []\n",
        "fallback_match_cells_CH = []\n",
        "\n",
        "for row_num, (bus, date_str, _) in row_map_CH.items():\n",
        "    value = bus_date_value_map_CH.get((bus, date_str), \"\")\n",
        "    fallback_date = fallback_date_map_CH.get((bus, date_str), \"\")\n",
        "\n",
        "    if value:\n",
        "        # Add to batch updates for column L (value)\n",
        "        batch_updates_CH.append({\n",
        "            'range': f'L{row_num}',\n",
        "            'values': [[value]]\n",
        "        })\n",
        "\n",
        "        # Add to batch updates for column M (fallback date)\n",
        "        batch_updates_CH.append({\n",
        "            'range': f'M{row_num}',\n",
        "            'values': [[fallback_date]]\n",
        "        })\n",
        "\n",
        "        # Track which cells need which formatting\n",
        "        match_type = match_type_map_CH.get((bus, date_str))\n",
        "        if match_type == \"exact\":\n",
        "            exact_match_cells_CH.append(f\"L{row_num}\")\n",
        "        elif match_type == \"fallback\":\n",
        "            fallback_match_cells_CH.append(f\"L{row_num}\")\n",
        "\n",
        "# Perform the batch updates for CK worksheet\n",
        "print(f\"üìä Updating {len(batch_updates_CK)} cells in CK worksheet columns L and M...\")\n",
        "updated_count_CK = batch_update_with_retry(engine_oil_CK_BSVI_CK_MAS_ATR, batch_updates_CK)\n",
        "\n",
        "# Perform the batch updates for CH worksheet\n",
        "print(f\"üìä Updating {len(batch_updates_CH)} cells in CH worksheet columns L and M...\")\n",
        "updated_count_CH = batch_update_with_retry(engine_oil_CH_BSIV_CH_MAS_ATR, batch_updates_CH)\n",
        "\n",
        "# Apply formatting in batches\n",
        "print(\"\\nüé® Applying formatting to distinguish match types...\")\n",
        "# Formatting logic would go here if implemented\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\n‚úÖ COMPLETE: Updated {updated_count_CK} cells in CK worksheet and {updated_count_CH} cells in CH worksheet\")\n",
        "print(f\"üìä CK Summary: {success_count_CK} exact matches, {fallback_count_CK} fallback matches\")\n",
        "print(f\"üìä CH Summary: {success_count_CH} exact matches, {fallback_count_CH} fallback matches\")\n",
        "print(f\"‚ùå CK: {len(row_map_CK) * 2 - updated_count_CK} cells could not be updated\")\n",
        "print(f\"‚ùå CH: {len(row_map_CH) * 2 - updated_count_CH} cells could not be updated\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xX4H7u05a-I3",
        "outputId": "bbfc22bc-79ac-4963-effc-5290ffa7e42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Starting data processing with date fallback search...\n",
            "üì• Loading main sheet data...\n",
            "‚úÖ Loaded 900 rows from main sheet\n",
            "üî¢ Mapping bus numbers and dates...\n",
            "‚úÖ Found 39 unique bus numbers\n",
            "‚úÖ Found 352 rows to process\n",
            "\n",
            "üîÑ Pre-loading all bus worksheets...\n",
            "‚ùå Could not load sheet '381': 381\n",
            "‚ùå Could not load sheet '576': 576\n",
            "‚ùå Could not load sheet '579': 579\n",
            "‚ùå Could not load sheet '368': 368\n",
            "‚ùå Could not load sheet '565': 565\n",
            "‚ùå Could not load sheet '564': 564\n",
            "‚ùå Could not load sheet '584': 584\n",
            "‚ùå Could not load sheet '319': 319\n",
            "‚ùå Could not load sheet '566': 566\n",
            "‚ùå Could not load sheet '591': 591\n",
            "‚ùå Could not load sheet '560': 560\n",
            "‚ùå Could not load sheet '549': 549\n",
            "‚ùå Could not load sheet '593': 593\n",
            "‚ùå Could not load sheet '545': 545\n",
            "‚ùå Could not load sheet '430': 430\n",
            "‚ùå Could not load sheet '544': 544\n",
            "‚ùå Could not load sheet '553': 553\n",
            "‚ùå Could not load sheet '317': 317\n",
            "‚ùå Could not load sheet '562': 562\n",
            "‚ùå Could not load sheet 'Grease Gun': Grease Gun\n",
            "‚ùå Could not load sheet '433': 433\n",
            "‚ùå Could not load sheet '409': 409\n",
            "‚ùå Could not load sheet '557': 557\n",
            "‚úÖ Successfully pre-loaded 16 bus worksheets\n",
            "\n",
            "üîç Searching for exact dates or closest earlier dates...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'bus_date_map_CK' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-abd5acb0ab0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;31m# Process CK buses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbus_date_map_CK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìä Processing CK bus {bus} ({len(dates)} dates)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0msheet_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbus_worksheet_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bus_date_map_CK' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FG_Working, Best Code\n",
        "\n",
        "# Modified to process multiple engine oil worksheets, Added batch processing done\n",
        "\n",
        "# Modified to process multiple engine oil worksheets with batch processing\n",
        "# With fallback date display in M Column, For ATR_MAS_Engine Oil\n",
        "\n",
        "# Make sure to include these imports at the top of your code\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_FG',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_FG'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Function to process each worksheet\n",
        "def process_worksheet(worksheet_name):\n",
        "    print(f\"\\nüîÑ Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"‚ùå Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "    clear_range = 'L2:M'\n",
        "    print(f\"üßπ Clearing range {clear_range} in {worksheet_name}...\")\n",
        "    worksheet.batch_clear([clear_range])\n",
        "\n",
        "    print(\"üîç Starting data processing with date fallback search...\")\n",
        "\n",
        "    # üì• Load main sheet\n",
        "    print(\"üì• Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"‚úÖ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping\n",
        "    print(\"üî¢ Mapping bus numbers and dates...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number ‚Üí (bus, date)\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            date_str = row[1].strip()\n",
        "            bus_num = row[4].strip()\n",
        "            if date_str and bus_num:\n",
        "                bus_date_map[bus_num].add(date_str)\n",
        "                row_map[i] = (bus_num, date_str)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ Found {len(bus_date_map)} unique bus numbers\")\n",
        "    print(f\"‚úÖ Found {len(row_map)} rows to process\")\n",
        "\n",
        "    # Create a cache for worksheet data to avoid redundant fetches\n",
        "    bus_worksheet_cache = {}\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\nüîç Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Function to process a single bus\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "\n",
        "        try:\n",
        "            # Check if we already have this bus's data in cache\n",
        "            if bus in bus_worksheet_cache:\n",
        "                sheet_data = bus_worksheet_cache[bus]\n",
        "            else:\n",
        "                # Fetch the bus worksheet data and store in cache\n",
        "                sheet = Odometer_spreadsheet_FG.worksheet(bus)\n",
        "                sheet_data = sheet.get_all_values()\n",
        "                bus_worksheet_cache[bus] = sheet_data\n",
        "\n",
        "            # Create a dictionary for faster exact date lookups\n",
        "            date_value_lookup = {}\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12:\n",
        "                        date_str_key = row[1].strip()\n",
        "                        date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for date_str in dates:\n",
        "                # Check for exact match using the lookup dictionary\n",
        "                exact_value = date_value_lookup.get(date_str)\n",
        "\n",
        "                if exact_value:\n",
        "                    results.append((bus, date_str, exact_value, \"exact\", \"\"))\n",
        "                    sheet_success += 1\n",
        "                else:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date))\n",
        "                        sheet_success += 1\n",
        "\n",
        "            print(f\"  ‚úÖ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Could not process sheet '{bus}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Process buses in parallel using ThreadPoolExecutor\n",
        "    print(f\"üöÄ Processing {len(bus_date_map)} buses in parallel...\")\n",
        "\n",
        "    # Convert to list for ThreadPoolExecutor\n",
        "    bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    # Adjust max_workers based on your system's capabilities\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "    # Process all results\n",
        "    for bus_results in all_results:\n",
        "        for bus, date_str, value, match_type, fallback_date in bus_results:\n",
        "            bus_date_value_map[(bus, date_str)] = value\n",
        "            match_type_map[(bus, date_str)] = match_type\n",
        "            fallback_date_map[(bus, date_str)] = fallback_date\n",
        "\n",
        "            if match_type == \"exact\":\n",
        "                success_count += 1\n",
        "            else:\n",
        "                fallback_count += 1\n",
        "\n",
        "    print(f\"\\n‚úÖ Found values for {success_count} exact date matches\")\n",
        "    print(f\"‚úÖ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"‚ùå Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\nüìù Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"‚úÖ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 50  # Adjust based on API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"üîÑ Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update\n",
        "                worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 2  # Divide by 2 because we have 2 cells per row\n",
        "                print(f\"üìä Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//2} rows updated\")\n",
        "\n",
        "                # Add a small delay between chunks to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"‚è≥ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"‚ùå Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\nüé® Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n‚úÖ COMPLETE for {worksheet_name}: Updated {updated_count} rows in column L and column M\")\n",
        "    print(f\"üìä Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"‚ùå {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Main execution\n",
        "print(\"üöÄ Starting to process all engine oil worksheets...\")\n",
        "\n",
        "# Create a global cache for bus worksheets to reuse across all worksheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Process each worksheet in sequence\n",
        "for worksheet_name in engine_oil_worksheets:\n",
        "    print(f\"\\n{'='*80}\\nüìä Processing worksheet: {worksheet_name}\\n{'='*80}\")\n",
        "    process_worksheet(worksheet_name)\n",
        "    # Add a delay between worksheets to avoid rate limiting\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n‚úÖ All worksheets have been processed successfully!\")\n",
        "print(\"üí° Performance Summary:\")\n",
        "print(f\"‚úì Processed {len(engine_oil_worksheets)} worksheets with batch processing\")\n",
        "print(f\"‚úì Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"‚úì Implemented caching to reduce API calls\")\n",
        "print(f\"‚úì Used batch updates to minimize API requests\")\n"
      ],
      "metadata": {
        "id": "ng3__2UKVfZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lean for all units, improved API limiter, #Lean combined all Units Engine Oil and Search all Odometer, but it search only missing values, if it works, then in future just add engine oil sheet and odometer sheet\n",
        "\n",
        "\n",
        "# Lean version - Only processes rows with empty L column\n",
        "# With fallback date display in M Column, For SVP, FG, BT, RT, MB, and DP\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_FG',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_FG',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_ATR',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_ATR',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_rr',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_rr'\n",
        "]\n",
        "\n",
        "# List of all odometer spreadsheets to search\n",
        "odometer_spreadsheets = [\n",
        "    'Odometer_spreadsheet_SVP',\n",
        "    'Odometer_spreadsheet_FG',\n",
        "    'Odometer_spreadsheet_BT',\n",
        "    'Odometer_spreadsheet_RT',\n",
        "    'Odometer_spreadsheet_MB',\n",
        "    'Odometer_spreadsheet_DP'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Global cache for bus worksheets to reuse across all worksheets and spreadsheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Function to process each worksheet - LEAN VERSION (only empty L cells)\n",
        "def process_worksheet_lean(worksheet_name):\n",
        "    print(f\"\\nüîÑ Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"‚ùå Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # NO CLEARING OF RANGES - we're only updating empty cells\n",
        "    print(\"üîç Starting data processing for EMPTY L cells only...\")\n",
        "\n",
        "    # üì• Load main sheet\n",
        "    print(\"üì• Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"‚úÖ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping ONLY FOR EMPTY L CELLS\n",
        "    print(\"üî¢ Mapping bus numbers and dates for empty L cells...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number ‚Üí (bus, date)\n",
        "    empty_cell_count = 0\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            # Check if L column (index 11) is empty\n",
        "            l_value = row[11].strip() if len(row) > 11 else \"\"\n",
        "\n",
        "            if not l_value:  # Only process if L column is empty\n",
        "                date_str = row[1].strip()\n",
        "                bus_num = row[4].strip()\n",
        "                if date_str and bus_num:\n",
        "                    bus_date_map[bus_num].add(date_str)\n",
        "                    row_map[i] = (bus_num, date_str)\n",
        "                    empty_cell_count += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ Found {len(bus_date_map)} unique bus numbers with empty L cells\")\n",
        "    print(f\"‚úÖ Found {empty_cell_count} empty L cells to process\")\n",
        "\n",
        "    # Exit early if no empty cells to process\n",
        "    if empty_cell_count == 0:\n",
        "        print(f\"‚úì No empty L cells to process in {worksheet_name}. Moving to next worksheet.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\nüîç Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Track which spreadsheet contained the match for metadata\n",
        "    spreadsheet_source_map = {}  # Maps (bus, date) to spreadsheet name\n",
        "\n",
        "    # Function to process a single bus across all spreadsheets\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "        dates_to_process = set(dates)  # Create a copy to safely modify\n",
        "\n",
        "        # Try each spreadsheet in sequence\n",
        "        for spreadsheet_name in odometer_spreadsheets:\n",
        "            if not dates_to_process:  # If all dates have been found, skip further processing\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Generate cache key that includes the spreadsheet name\n",
        "                cache_key = f\"{spreadsheet_name}:{bus}\"\n",
        "\n",
        "                # Check if we already have this bus's data in cache\n",
        "                if cache_key in global_bus_worksheet_cache:\n",
        "                    sheet_data = global_bus_worksheet_cache[cache_key]\n",
        "                else:\n",
        "                    # Get the spreadsheet object dynamically\n",
        "                    try:\n",
        "                        spreadsheet = eval(spreadsheet_name)\n",
        "                        # Try to get the worksheet for this bus with retry mechanism\n",
        "                        max_retries = 5\n",
        "                        retry_count = 0\n",
        "                        while retry_count < max_retries:\n",
        "                            try:\n",
        "                                sheet = spreadsheet.worksheet(bus)\n",
        "                                sheet_data = sheet.get_all_values()\n",
        "                                global_bus_worksheet_cache[cache_key] = sheet_data\n",
        "                                break  # Success, exit retry loop\n",
        "                            except gspread.exceptions.APIError as api_error:\n",
        "                                if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                                    retry_count += 1\n",
        "                                    wait_time = (2 ** retry_count) + (random.random() * 2)\n",
        "                                    print(f\"‚è≥ Rate limit hit for bus {bus}, waiting {wait_time:.2f}s (retry {retry_count}/{max_retries})\")\n",
        "                                    time.sleep(wait_time)\n",
        "                                    if retry_count == max_retries:\n",
        "                                        print(f\"‚ö†Ô∏è Max retries reached for bus {bus} in {spreadsheet_name}\")\n",
        "                                        raise\n",
        "                                else:\n",
        "                                    # Not a rate limit issue, the worksheet likely doesn't exist\n",
        "                                    raise\n",
        "                            except Exception as e:\n",
        "                                # The worksheet doesn't exist in this spreadsheet\n",
        "                                if \"Worksheet not found\" in str(e) or \"404\" in str(e):\n",
        "                                    print(f\"  ‚ÑπÔ∏è Bus {bus} not found in {spreadsheet_name}\")\n",
        "                                else:\n",
        "                                    print(f\"  ‚ö†Ô∏è Error accessing {bus} in {spreadsheet_name}: {str(e)}\")\n",
        "                                raise\n",
        "                    except Exception:\n",
        "                        # This bus doesn't exist in this spreadsheet or we couldn't access it\n",
        "                        continue\n",
        "\n",
        "                # Process each date for this bus in this spreadsheet\n",
        "                # Create a dictionary for faster exact date lookups\n",
        "                date_value_lookup = {}\n",
        "                for row in sheet_data[1:]:\n",
        "                    try:\n",
        "                        if len(row) > 12:\n",
        "                            date_str_key = row[1].strip()\n",
        "                            date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                dates_found_in_this_sheet = set()\n",
        "\n",
        "                # First try exact matches which are faster to check\n",
        "                for date_str in dates_to_process:\n",
        "                    # Check for exact match using the lookup dictionary\n",
        "                    exact_value = date_value_lookup.get(date_str)\n",
        "                    if exact_value:\n",
        "                        results.append((bus, date_str, exact_value, \"exact\", \"\", spreadsheet_name))\n",
        "                        sheet_success += 1\n",
        "                        dates_found_in_this_sheet.add(date_str)\n",
        "\n",
        "                # Then try fallback matches for remaining dates\n",
        "                remaining_dates = dates_to_process - dates_found_in_this_sheet\n",
        "                for date_str in remaining_dates:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date, spreadsheet_name))\n",
        "                        sheet_success += 1\n",
        "                        dates_found_in_this_sheet.add(date_str)\n",
        "\n",
        "                # Remove all processed dates\n",
        "                dates_to_process -= dates_found_in_this_sheet\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"Worksheet not found\" not in str(e) and \"404\" not in str(e):\n",
        "                    print(f\"‚ùå Could not process sheet '{bus}' in spreadsheet '{spreadsheet_name}': {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if sheet_success > 0:\n",
        "            print(f\"  ‚úÖ Found {sheet_success} values for bus {bus}\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è No values found for bus {bus} in any spreadsheet\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    # Only process if we have buses to check\n",
        "    if len(bus_date_map) > 0:\n",
        "        # Convert to list for ThreadPoolExecutor\n",
        "        bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        max_workers = min(5, len(bus_dates_list))  # Adjust based on data size\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "        # Process all results\n",
        "        for bus_results in all_results:\n",
        "            for bus, date_str, value, match_type, fallback_date, source_spreadsheet in bus_results:\n",
        "                bus_date_value_map[(bus, date_str)] = value\n",
        "                match_type_map[(bus, date_str)] = match_type\n",
        "                fallback_date_map[(bus, date_str)] = fallback_date\n",
        "                spreadsheet_source_map[(bus, date_str)] = source_spreadsheet\n",
        "\n",
        "                if match_type == \"exact\":\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    fallback_count += 1\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No buses to process - all L cells have values already\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Found values for {success_count} exact date matches\")\n",
        "    print(f\"‚úÖ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"‚ùå Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\nüìù Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "        spreadsheet_source = spreadsheet_source_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date if fallback_date else \"\"\n",
        "            })\n",
        "\n",
        "            # Add source spreadsheet info in Column N for tracking\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 14,  # Column N\n",
        "                'value': spreadsheet_source\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"‚úÖ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Skip if no updates needed\n",
        "    if len(batch_updates) == 0:\n",
        "        print(\"‚úì No updates needed for this worksheet\")\n",
        "        return\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 25  # Reduced chunk size to avoid API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"üîÑ Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 8  # Increased retries\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update using our rate-limited function\n",
        "                def do_update():\n",
        "                    return worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                rate_limited_request(do_update)\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 3  # Divide by 3 because we have 3 cells per row (L, M, N)\n",
        "                print(f\"üìä Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//3} rows updated\")\n",
        "\n",
        "                # Add a variable delay between chunks to avoid rate limits\n",
        "                # More aggressive backoff with increasing chunk index\n",
        "                wait_time = 2 + (chunk_index % 3) + (random.random() * 3)\n",
        "                print(f\"‚è≥ Waiting {wait_time:.2f}s before next chunk...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            except gspread.exceptions.APIError as api_error:\n",
        "                retry_count += 1\n",
        "                if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                    # Exponential backoff with jitter for rate limits\n",
        "                    wait_time = (2 ** retry_count) + (random.random() * 5)\n",
        "                    # Cap maximum wait time at 2 minutes\n",
        "                    wait_time = min(wait_time, 120)\n",
        "                    print(f\"‚è≥ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"‚ùå API Error on chunk {chunk_index+1}: {api_error}\")\n",
        "                    if retry_count < 3:  # Only retry a few times for non-rate-limit errors\n",
        "                        time.sleep(5)\n",
        "                    else:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                print(f\"‚ùå Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                if retry_count < 3:  # Only retry a few times for general errors\n",
        "                    time.sleep(5)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"‚ùå Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "            # Add extra delay when a chunk completely fails before moving to next chunk\n",
        "            wait_time = 15 + (random.random() * 10)\n",
        "            print(f\"‚è≥ Taking a longer break: {wait_time:.2f}s before continuing...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\nüé® Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n‚úÖ COMPLETE for {worksheet_name}: Updated {updated_count} rows in columns L, M, and N\")\n",
        "    print(f\"üìä Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"‚ùå {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Global rate limiting function\n",
        "def rate_limited_request(func, *args, **kwargs):\n",
        "    \"\"\"Execute a function with rate limiting and retries for API errors\"\"\"\n",
        "    max_retries = 8  # More retries for critical operations\n",
        "    retry_count = 0\n",
        "    base_wait_time = 2  # seconds\n",
        "\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except gspread.exceptions.APIError as api_error:\n",
        "            if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                retry_count += 1\n",
        "                # Exponential backoff with jitter\n",
        "                wait_time = (base_wait_time ** retry_count) + (random.random() * 5)\n",
        "                # Cap the wait time at 3 minutes\n",
        "                wait_time = min(wait_time, 180)\n",
        "                print(f\"‚è≥ Rate limit hit! Waiting {wait_time:.2f}s (retry {retry_count}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                # Re-raise non-rate-limit errors\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            # For other exceptions, retry a few times but with less patience\n",
        "            if retry_count < 3:\n",
        "                retry_count += 1\n",
        "                wait_time = base_wait_time + (random.random() * 2)\n",
        "                print(f\"‚ö†Ô∏è Error occurred: {str(e)}\")\n",
        "                print(f\"Retrying in {wait_time:.2f}s (retry {retry_count}/3)\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    # If we've exhausted all retries\n",
        "    raise Exception(f\"Failed after {max_retries} retries due to persistent rate limiting\")\n",
        "\n",
        "# Main execution - LEAN VERSION with improved rate limiting\n",
        "print(\"üöÄ Starting LEAN processing - only empty L cells...\")\n",
        "print(f\"üîç Will search across {len(odometer_spreadsheets)} odometer spreadsheets: {', '.join(odometer_spreadsheets)}\")\n",
        "\n",
        "# Process each worksheet with rate limiting and proper delays\n",
        "for i, worksheet_name in enumerate(engine_oil_worksheets):\n",
        "    print(f\"\\n{'='*80}\\nüìä Processing worksheet: {worksheet_name} ({i+1}/{len(engine_oil_worksheets)})\\n{'='*80}\")\n",
        "\n",
        "    # Add a longer delay between worksheets to avoid rate limiting\n",
        "    if i > 0:\n",
        "        wait_time = 5 + (random.random() * 5)  # 5-10 second delay between worksheets\n",
        "        print(f\"‚è≥ Waiting {wait_time:.2f}s before processing next worksheet...\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "    # Process the worksheet with proper rate limiting\n",
        "    try:\n",
        "        process_worksheet_lean(worksheet_name)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing worksheet {worksheet_name}: {str(e)}\")\n",
        "        print(\"Continuing with next worksheet after a delay...\")\n",
        "        time.sleep(15)  # Longer delay after an error\n",
        "\n",
        "print(\"\\n‚úÖ All worksheets have been processed successfully!\")\n",
        "print(\"üí° Performance Summary:\")\n",
        "print(f\"‚úì Processed {len(engine_oil_worksheets)} worksheets with LEAN mode (empty L cells only)\")\n",
        "print(f\"‚úì Searched across {len(odometer_spreadsheets)} odometer spreadsheets\")\n",
        "print(f\"‚úì Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"‚úì Implemented caching to reduce API calls\")\n",
        "print(f\"‚úì Used batch updates to minimize API requests\")\n",
        "\n"
      ],
      "metadata": {
        "id": "azCz95R7KQK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # High Speed, Lean for all units, improved API limiter, #Lean combined all Units Engine Oil and Search all Odometer, but it search only missing values, if it works, then in future just add engine oil sheet and odometer sheet\n",
        "\n",
        "\n",
        "\n",
        "# The above code is returning value in Column N, i dont need it, also incorporate batch processing to increase speed"
      ],
      "metadata": {
        "id": "J09_k_HVL0ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Highe speed Report Generation, The Report1 address to be changed, in this, Tuesday work, Report in seperate file, Engine_Oil_Report_TD\n",
        "\n",
        "# Highe speed Report Generation\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"üöÄ Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# Cached function to get worksheet data\n",
        "@lru_cache_with_expiry(maxsize=100, ttl=300)  # Cache for 5 minutes\n",
        "def get_worksheet_data(spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Get worksheet data with caching\"\"\"\n",
        "    cache_key = f\"{spreadsheet_name}:{worksheet_name}\"\n",
        "\n",
        "    with cache_lock:\n",
        "        if cache_key in data_cache:\n",
        "            print(f\"üìã Cache hit for {cache_key}\")\n",
        "            return data_cache[cache_key]\n",
        "\n",
        "    # Function to find the actual worksheet object\n",
        "    def find_worksheet(spreadsheet_list, worksheet_name, spreadsheet_names):\n",
        "        for idx, spreadsheet in enumerate(spreadsheet_list):\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(worksheet_name)\n",
        "                print(f\"üìë Found worksheet {worksheet_name} in {spreadsheet_names[idx]}\")\n",
        "                return sheet, spreadsheet_names[idx]\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None, None\n",
        "\n",
        "    # Cache miss, need to retrieve data\n",
        "    try:\n",
        "        sheet, source = find_worksheet(odometer_spreadsheets, worksheet_name, spreadsheet_names)\n",
        "        if sheet:\n",
        "            data = sheet.get_all_values()\n",
        "\n",
        "            with cache_lock:\n",
        "                data_cache[cache_key] = (data, source)\n",
        "\n",
        "            return data, source\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error retrieving data for {worksheet_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# === STEP 1: CLEAR DATA ===\n",
        "print(\"\\nüßπ STEP 1: Clearing data...\")\n",
        "clear_range_all_OdometersheetID = 'A5:R200'\n",
        "clear_range_all_Report_EO1_Worksheet = ['B2:B200', 'AC2:AC200']\n",
        "\n",
        "OdometersheetID.batch_clear([clear_range_all_OdometersheetID])\n",
        "Report_EO1_Worksheet.batch_clear(clear_range_all_Report_EO1_Worksheet)\n",
        "\n",
        "# === STEP 2: PROCESS BUS DATA ===\n",
        "print(\"\\nüìä STEP 2: Processing bus data...\")\n",
        "\n",
        "# Function to process spreadsheet data in parallel\n",
        "def process_spreadsheet(cell_ref, position_index):\n",
        "    \"\"\"Process a single spreadsheet and return its data\"\"\"\n",
        "    odometer_id = OdometersheetID.acell(cell_ref).value\n",
        "    spreadsheet = gc.open_by_key(odometer_id)\n",
        "    bus_numbers = sorted([worksheet.title for worksheet in spreadsheet])\n",
        "    spreadsheet_name = spreadsheet.title\n",
        "    return {\n",
        "        'spreadsheet': spreadsheet,\n",
        "        'name': spreadsheet_name,\n",
        "        'buses': bus_numbers,\n",
        "        'position': position_index\n",
        "    }\n",
        "\n",
        "# Process all spreadsheets in parallel\n",
        "spreadsheet_info = {\n",
        "    'SVP': {'cell': 'A2', 'position': 0},\n",
        "    'FG': {'cell': 'B2', 'position': 1},\n",
        "    'BT': {'cell': 'C2', 'position': 2},\n",
        "    'RT': {'cell': 'D2', 'position': 3},\n",
        "    'MB': {'cell': 'E2', 'position': 4},\n",
        "    'DP': {'cell': 'F2', 'position': 5}\n",
        "}\n",
        "\n",
        "spreadsheet_data = {}\n",
        "odometer_spreadsheets = [None] * 6\n",
        "spreadsheet_names = [None] * 6\n",
        "\n",
        "# For Google Sheets API, we can't fully parallelize due to rate limits\n",
        "# But we can optimize the processing steps\n",
        "for key, info in spreadsheet_info.items():\n",
        "    data = process_spreadsheet(info['cell'], info['position'])\n",
        "    spreadsheet_data[key] = data\n",
        "    odometer_spreadsheets[info['position']] = data['spreadsheet']\n",
        "    spreadsheet_names[info['position']] = data['name']\n",
        "    print(f\"‚úÖ Processed {key} spreadsheet: {data['name']} with {len(data['buses'])} buses\")\n",
        "\n",
        "# Calculate starting rows for each section\n",
        "start_row_svp = 2\n",
        "start_row_fg = start_row_svp + len(spreadsheet_data['SVP']['buses'])\n",
        "start_row_bt = start_row_fg + len(spreadsheet_data['FG']['buses'])\n",
        "start_row_rt = start_row_bt + len(spreadsheet_data['BT']['buses'])\n",
        "start_row_mb = start_row_rt + len(spreadsheet_data['RT']['buses'])\n",
        "start_row_dp = start_row_mb + len(spreadsheet_data['MB']['buses'])\n",
        "\n",
        "# === STEP 3: UPDATE ODOMETERSHEETID WITH BUS NUMBERS ===\n",
        "print(\"\\nüìù STEP 3: Updating OdometersheetID with bus numbers...\")\n",
        "\n",
        "# Optimized batch update function\n",
        "def optimized_batch_update(worksheet, updates):\n",
        "    \"\"\"Process updates in optimal batch sizes\"\"\"\n",
        "    BATCH_SIZE = 100  # Adjust based on API limits\n",
        "\n",
        "    # Split updates into manageable chunks\n",
        "    chunked_updates = chunk_list(updates, BATCH_SIZE)\n",
        "\n",
        "    for chunk in chunked_updates:\n",
        "        try:\n",
        "            worksheet.batch_update(chunk)\n",
        "            time.sleep(0.5)  # Slight delay to avoid rate limiting\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Batch update error: {e}\")\n",
        "            # Fall back to smaller chunks if needed\n",
        "            for update in chunk:\n",
        "                try:\n",
        "                    worksheet.update(update['range'], update['values'])\n",
        "                    time.sleep(0.1)\n",
        "                except Exception as e2:\n",
        "                    print(f\"‚ö†Ô∏è Individual update error at {update['range']}: {e2}\")\n",
        "\n",
        "# Update spreadsheet names in row 4 (batch update)\n",
        "name_updates = [\n",
        "    {'range': 'A4', 'values': [[spreadsheet_data['SVP']['name']]]},\n",
        "    {'range': 'B4', 'values': [[spreadsheet_data['FG']['name']]]},\n",
        "    {'range': 'C4', 'values': [[spreadsheet_data['BT']['name']]]},\n",
        "    {'range': 'D4', 'values': [[spreadsheet_data['RT']['name']]]},\n",
        "    {'range': 'E4', 'values': [[spreadsheet_data['MB']['name']]]},\n",
        "    {'range': 'F4', 'values': [[spreadsheet_data['DP']['name']]]}\n",
        "]\n",
        "OdometersheetID.batch_update(name_updates)\n",
        "print(\"‚úÖ Updated all spreadsheet names in row 4\")\n",
        "\n",
        "# Generate all updates for bus numbers\n",
        "all_bus_updates = []\n",
        "\n",
        "# Helper function to create batch updates for a column\n",
        "def create_column_updates(column_letter, start_row, buses):\n",
        "    updates = []\n",
        "    for i, bus in enumerate(buses):\n",
        "        row = start_row + i\n",
        "        # Try to convert numeric strings to integers\n",
        "        try:\n",
        "            if isinstance(bus, str) and bus.isdigit():\n",
        "                bus = int(bus)\n",
        "        except:\n",
        "            pass\n",
        "        updates.append({\n",
        "            'range': f'{column_letter}{row}',\n",
        "            'values': [[bus]]\n",
        "        })\n",
        "    return updates\n",
        "\n",
        "# Create all updates for bus numbers\n",
        "all_bus_updates.extend(create_column_updates('A', 5, spreadsheet_data['SVP']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('B', 5, spreadsheet_data['FG']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('C', 5, spreadsheet_data['BT']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('D', 5, spreadsheet_data['RT']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('E', 5, spreadsheet_data['MB']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('F', 5, spreadsheet_data['DP']['buses']))\n",
        "\n",
        "# Batch update OdometersheetID with bus numbers\n",
        "optimized_batch_update(OdometersheetID, all_bus_updates)\n",
        "print(f\"‚úÖ Updated {len(all_bus_updates)} bus entries in OdometersheetID\")\n",
        "\n",
        "# === STEP 4: UPDATE REPORT_EO1 WORKSHEET WITH ALL BUS NUMBERS ===\n",
        "print(\"\\nüìä STEP 4: Updating Report_EO1 with all bus numbers...\")\n",
        "\n",
        "# Combine all bus lists for ReportA_EO1\n",
        "all_buses_with_sources = []\n",
        "for key, data in spreadsheet_data.items():\n",
        "    for bus in data['buses']:\n",
        "        all_buses_with_sources.append((bus, data['name']))\n",
        "\n",
        "# Generate batch updates for Report_EO1\n",
        "report_EO1_updates = []\n",
        "for i, (bus, source) in enumerate(all_buses_with_sources):\n",
        "    row = i + 2  # Start from row 2\n",
        "    # Try to convert numeric strings to integers\n",
        "    try:\n",
        "        if isinstance(bus, str) and bus.isdigit():\n",
        "            bus = int(bus)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    report_EO1_updates.append({\n",
        "        'range': f'B{row}',\n",
        "        'values': [[bus]]\n",
        "    })\n",
        "    report_EO1_updates.append({\n",
        "        'range': f'AC{row}',\n",
        "        'values': [[source]]\n",
        "    })\n",
        "\n",
        "# Batch update Report_EO1\n",
        "optimized_batch_update(Report_EO1_Worksheet, report_EO1_updates)\n",
        "print(f\"‚úÖ Updated {len(all_buses_with_sources)} bus entries in Report_EO1 worksheet\")\n",
        "\n",
        "print(\"üéâ Bus data preparation completed successfully!\")\n",
        "\n",
        "# === STEP 5: SEARCH FOR ODOMETER VALUES ===\n",
        "print(\"\\nüîç STEP 5: Searching for odometer values...\")\n",
        "\n",
        "# Clear columns C and D\n",
        "Report_EO1_Worksheet.batch_clear([\"C2:D\"])\n",
        "\n",
        "# Get current date in expected format (e.g., \"05,May25\")\n",
        "current_date_str = datetime.now().strftime('%d,%b%y')\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus + date (using today's date)\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = (bus, current_date_str)\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"üîé Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize lookup maps with thread safety\n",
        "bus_date_value_map = {}\n",
        "match_type_map = {}\n",
        "fallback_date_map = {}\n",
        "source_spreadsheet_map = {}\n",
        "map_lock = threading.Lock()\n",
        "\n",
        "# Function to find closest earlier date value\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    \"\"\"Find closest earlier date with a value in the specified column\"\"\"\n",
        "    if not sheet_data:\n",
        "        return None, \"\"\n",
        "\n",
        "    try:\n",
        "        target_date = datetime.strptime(target_date_str, '%d,%b%y')\n",
        "        valid_rows = []\n",
        "\n",
        "        for row in sheet_data[1:]:  # Skip header\n",
        "            if len(row) <= value_column_index:\n",
        "                continue\n",
        "\n",
        "            date_cell = row[1].strip() if len(row) > 1 else \"\"\n",
        "            value_cell = row[value_column_index].strip() if len(row) > value_column_index else \"\"\n",
        "\n",
        "            if not date_cell or not value_cell:\n",
        "                continue\n",
        "\n",
        "            # Try multiple date formats\n",
        "            parsed_date = None\n",
        "            for fmt in ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']:\n",
        "                try:\n",
        "                    parsed_date = datetime.strptime(date_cell, fmt)\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if parsed_date and parsed_date <= target_date and value_cell:\n",
        "                valid_rows.append((parsed_date, date_cell, value_cell))\n",
        "\n",
        "        if valid_rows:\n",
        "            valid_rows.sort(key=lambda x: x[0], reverse=True)  # Newest date first\n",
        "            best = valid_rows[0]\n",
        "            return best[2], best[1]  # value, fallback_date\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error in fallback for {target_date_str}: {e}\")\n",
        "\n",
        "    return None, \"\"\n",
        "\n",
        "# Process a single bus to find odometer value\n",
        "def process_bus_odometer(bus):\n",
        "    \"\"\"Process odometer data for a single bus\"\"\"\n",
        "    found = False\n",
        "\n",
        "    # Use the cached function to get worksheet data\n",
        "    data, source = get_worksheet_data(\"All\", bus)\n",
        "\n",
        "    if data:\n",
        "        found = True\n",
        "        # Check for exact date match\n",
        "        exact_match = False\n",
        "        for row in data[1:]:  # Skip header\n",
        "            if len(row) > 12 and row[1].strip() == current_date_str and row[12].strip():\n",
        "                odometer_val = row[12].strip()\n",
        "                if odometer_val and odometer_val.isdigit():  # Validate numeric\n",
        "                    with map_lock:\n",
        "                        bus_date_value_map[(bus, current_date_str)] = odometer_val\n",
        "                        match_type_map[(bus, current_date_str)] = \"exact\"\n",
        "                        fallback_date_map[(bus, current_date_str)] = \"\"\n",
        "                        source_spreadsheet_map[(bus, current_date_str)] = source\n",
        "                    exact_match = True\n",
        "                    print(f\"üìå Bus {bus}: Exact match: {odometer_val} km on {current_date_str}\")\n",
        "                    break\n",
        "\n",
        "        if not exact_match:\n",
        "            # Fallback to closest date\n",
        "            val, fallback_date = find_closest_date_value(data, current_date_str, 12)\n",
        "            if val:\n",
        "                with map_lock:\n",
        "                    bus_date_value_map[(bus, current_date_str)] = val\n",
        "                    match_type_map[(bus, current_date_str)] = \"fallback\"\n",
        "                    fallback_date_map[(bus, current_date_str)] = fallback_date\n",
        "                    source_spreadsheet_map[(bus, current_date_str)] = source\n",
        "                print(f\"üîÑ Bus {bus}: Fallback match: {val} km on {fallback_date}\")\n",
        "\n",
        "    if not found:\n",
        "        print(f\"üö® Bus {bus} not found in ANY spreadsheet!\")\n",
        "\n",
        "    return found\n",
        "\n",
        "# Process buses in parallel batches\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(process_bus_odometer, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"‚úÖ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# Update Report_EO1 with logging\n",
        "print(\"\\nüìù Writing data to Report_EO1...\")\n",
        "updated_rows = []\n",
        "for row_idx, (bus, date_str) in row_map.items():\n",
        "    key = (bus, date_str)\n",
        "    val = bus_date_value_map.get(key, \"\")\n",
        "    fallback = fallback_date_map.get(key, \"\")\n",
        "    match_type = match_type_map.get(key, \"\")\n",
        "    source = source_spreadsheet_map.get(key, \"\")\n",
        "\n",
        "    # Build updated values for Column C and D\n",
        "    col_c = fallback if match_type == \"fallback\" else (source if match_type == \"exact\" else \"\")\n",
        "    col_d = val if val else \"\"\n",
        "\n",
        "    # Add to batch update\n",
        "    updated_rows.append([col_c, col_d])\n",
        "\n",
        "# Prepare batch update\n",
        "if updated_rows:\n",
        "    # Split into manageable chunks\n",
        "    MAX_ROWS_PER_UPDATE = 1000  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(updated_rows, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"C{start_row}:D{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"‚úÖ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"‚úÖ All {len(updated_rows)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"‚ùå No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"üéâ Complete script execution finished successfully!\")"
      ],
      "metadata": {
        "id": "3rjx7MTuCAKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI8hA/aE1Lj35ywcbAnKwP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}