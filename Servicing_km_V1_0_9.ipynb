{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdopsmang/Colab-projects/blob/main/Servicing_km_V1_0_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did not work on RR path, rather added bus details upto DP, in consolidated report, also batch processing done in few cases"
      ],
      "metadata": {
        "id": "JVbZ291M-7mv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CMKeh0lHx8tS",
        "outputId": "886b3da4-0793-4486-9f79-ae2a5de74741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.1)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.32.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.2.1)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gspread\n",
        "!pip install gspread google-auth-oauthlib google-auth-httplib2\n",
        "!pip install gspread google-auth oauth2client\n",
        "!pip install gspread oauth2client pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GIVCOLMf4oFc"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "from google.oauth2.service_account import Credentials\n",
        "gc = gspread.authorize(creds)\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import concurrent.futures\n",
        "from functools import lru_cache\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dX9vEbLhRZAG"
      },
      "outputs": [],
      "source": [
        "# Details our Database spreadsheet which has details of all files, ID in Transport Department\n",
        "\n",
        "Database_File_spreadsheet_ID = gc.open_by_key('1nJKyvV1WQmZzvbjOP7Hsp1bQJmiNsYoJOH_jIL7H9PI') #ID of Database Spreadsheet\n",
        "MASsheetID = Database_File_spreadsheet_ID.worksheet('MAS')                                    #MAS Worksheet\n",
        "OdometersheetID = Database_File_spreadsheet_ID.worksheet('Odometer')                          #Odometer Worksheet\n",
        "ReportID = Database_File_spreadsheet_ID.worksheet('Report')                          #Odometer Worksheet\n",
        "\n",
        "#Reports spreadsheet and worksheet details\n",
        "Report_EO_SpreadsheetID = ReportID.acell('C2').value                  # Get the spreadsheet ID from cell C2\n",
        "Report_EO = gc.open_by_key(Report_EO_SpreadsheetID)                    # Open RR_Oil_Lub spreadsheet\n",
        "Report_EO1_worksheet_name = ReportID.acell('B2').value                        # Km after last servicing consolidated\n",
        "Report_EO1_Worksheet = Report_EO.worksheet(Report_EO1_worksheet_name)  # Open RR CH engineoil worksheet\n",
        "\n",
        "#STS RR SVP\n",
        "RR_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C2').value                  # Get the spreadsheet ID from cell C2\n",
        "RR_Oil_Lub = gc.open_by_key(RR_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open RR_Oil_Lub spreadsheet\n",
        "\n",
        "RR_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B2').value                        # Get the worksheet name from cell B2\n",
        "engine_oil_CH_BSIV_CH_MAS_rr = RR_Oil_Lub.worksheet(RR_Engine_oil_CH_BSIV_worksheet_name)  # Open RR CH engineoil worksheet\n",
        "\n",
        "RR_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B3').value                        # Get the worksheet name from cell B3\n",
        "engine_oil_CK_BSVI_CK_MAS_rr = RR_Oil_Lub.worksheet(RR_Engine_oil_CK_BSVI_worksheet_name)  # Open RR CK engineoil worksheet\n",
        "\n",
        "RR_Report1_worksheet_name = MASsheetID.acell('B8').value                                # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "RR_Report1_Worksheet = RR_Oil_Lub.worksheet(RR_Report1_worksheet_name)                  # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "RR_Report1_worksheet_name = MASsheetID.acell('B8').value                                # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "RR_Report1_Worksheet = RR_Oil_Lub.worksheet(RR_Report1_worksheet_name)                  # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "#STS ATR SVP\n",
        "ATR_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C52').value                  # Get the spreadsheet ID from cell C51 ATR\n",
        "ATR_Oil_Lub = gc.open_by_key(ATR_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open ATR_Oil_Lub spreadsheet\n",
        "\n",
        "ATR_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B52').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CH_BSIV_CH_MAS_ATR = ATR_Oil_Lub.worksheet(ATR_Engine_oil_CH_BSIV_worksheet_name)  # Open ATR CH engineoil worksheet\n",
        "\n",
        "ATR_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B53').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CK_BSVI_CK_MAS_ATR = ATR_Oil_Lub.worksheet(ATR_Engine_oil_CK_BSVI_worksheet_name)  # Open ATR CK engineoil worksheet\n",
        "\n",
        "#ATR_Report1_worksheet_name = MASsheetID.acell('B58').value                           #******This may not be required       # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "#ATR_Report1_Worksheet = ATR_Oil_Lub.worksheet(ATR_Report1_worksheet_name)              #This may not be required     # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "#STS FG\n",
        "FG_Oil_Lub_Coolant_DEF_spreadsheet_id = MASsheetID.acell('C102').value                  # Get the spreadsheet ID from cell C51 ATR\n",
        "FG_Oil_Lub = gc.open_by_key(FG_Oil_Lub_Coolant_DEF_spreadsheet_id)                    # Open ATR_Oil_Lub spreadsheet\n",
        "\n",
        "FG_Engine_oil_CH_BSIV_worksheet_name = MASsheetID.acell('B102').value                         # Get the worksheet name from cell B2\n",
        "engine_oil_CH_BSIV_CH_MAS_FG = FG_Oil_Lub.worksheet(FG_Engine_oil_CH_BSIV_worksheet_name)  # Open RR engineoil worksheet\n",
        "\n",
        "FG_Engine_oil_CK_BSVI_worksheet_name = MASsheetID.acell('B103').value                         # Get the worksheet name from cell B52\n",
        "engine_oil_CK_BSVI_CK_MAS_FG = FG_Oil_Lub.worksheet(FG_Engine_oil_CK_BSVI_worksheet_name)  # Open ATR CK engineoil worksheet\n",
        "\n",
        "#FG_Report1_worksheet_name = MASsheetID.acell('B108').value                    #*****This may not be required            # Get the worksheet name from report1, RR Km between and after last servicing\n",
        "#FG_Report1_Worksheet = FG_Oil_Lub.worksheet(FG_Report1_worksheet_name)          #This may not be required         # Open RR Km between and after last servicing worksheet\n",
        "\n",
        "\n",
        "# Process SVP data\n",
        "SVP_Odometer_id = OdometersheetID.acell('A2').value\n",
        "Odometer_spreadsheet_SVP = gc.open_by_key(SVP_Odometer_id)\n",
        "bus_number_SVP = sorted([worksheet.title for worksheet in Odometer_spreadsheet_SVP])\n",
        "spreadsheet_name_SVP = Odometer_spreadsheet_SVP.title\n",
        "start_row_svp = 2\n",
        "\n",
        "# Process FG data\n",
        "FG_Odometer_id = OdometersheetID.acell('B2').value\n",
        "Odometer_spreadsheet_FG = gc.open_by_key(FG_Odometer_id)\n",
        "bus_number_FG = sorted([worksheet.title for worksheet in Odometer_spreadsheet_FG])\n",
        "spreadsheet_name_FG = Odometer_spreadsheet_FG.title\n",
        "start_row_fg = start_row_svp + len(bus_number_SVP)\n",
        "\n",
        "# Process Baratang data\n",
        "BT_Odometer_id = OdometersheetID.acell('C2').value\n",
        "Odometer_spreadsheet_BT = gc.open_by_key(BT_Odometer_id)\n",
        "bus_number_BT = sorted([worksheet.title for worksheet in Odometer_spreadsheet_BT])\n",
        "spreadsheet_name_BT = Odometer_spreadsheet_BT.title\n",
        "start_row_bt = start_row_fg + len(bus_number_FG)\n",
        "\n",
        "# Process Rangat data\n",
        "RT_Odometer_id = OdometersheetID.acell('D2').value\n",
        "Odometer_spreadsheet_RT = gc.open_by_key(RT_Odometer_id)\n",
        "bus_number_RT = sorted([worksheet.title for worksheet in Odometer_spreadsheet_RT])\n",
        "spreadsheet_name_RT = Odometer_spreadsheet_RT.title\n",
        "start_row_rt = start_row_bt + len(bus_number_BT)\n",
        "\n",
        "# Process Mayabunder data\n",
        "MB_Odometer_id = OdometersheetID.acell('E2').value\n",
        "Odometer_spreadsheet_MB = gc.open_by_key(MB_Odometer_id)\n",
        "bus_number_MB = sorted([worksheet.title for worksheet in Odometer_spreadsheet_MB])\n",
        "spreadsheet_name_MB = Odometer_spreadsheet_MB.title\n",
        "start_row_mb = start_row_rt + len(bus_number_RT)\n",
        "\n",
        "# Process Diglipur data\n",
        "DP_Odometer_id = OdometersheetID.acell('F2').value\n",
        "Odometer_spreadsheet_DP = gc.open_by_key(DP_Odometer_id)\n",
        "bus_number_DP = sorted([worksheet.title for worksheet in Odometer_spreadsheet_DP])\n",
        "spreadsheet_name_DP = Odometer_spreadsheet_DP.title\n",
        "start_row_dp = start_row_mb + len(bus_number_MB)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below codes are redundent, No need to run, Use this If required in future"
      ],
      "metadata": {
        "id": "Uq1Go9jLsRov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# High speed, RR, code of 1.0.7 is fast but only for CH, if we incorporate CK, it will be super fast\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_rr',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_rr'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Function to process each worksheet\n",
        "def process_worksheet(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "    clear_range = 'L2:M'\n",
        "    print(f\"🧹 Clearing range {clear_range} in {worksheet_name}...\")\n",
        "    worksheet.batch_clear([clear_range])\n",
        "\n",
        "    print(\"🔍 Starting data processing with date fallback search...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping\n",
        "    print(\"🔢 Mapping bus numbers and dates...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            date_str = row[1].strip()\n",
        "            bus_num = row[4].strip()\n",
        "            if date_str and bus_num:\n",
        "                bus_date_map[bus_num].add(date_str)\n",
        "                row_map[i] = (bus_num, date_str)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers\")\n",
        "    print(f\"✅ Found {len(row_map)} rows to process\")\n",
        "\n",
        "    # Create a cache for worksheet data to avoid redundant fetches\n",
        "    bus_worksheet_cache = {}\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Function to process a single bus\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "\n",
        "        try:\n",
        "            # Check if we already have this bus's data in cache\n",
        "            if bus in bus_worksheet_cache:\n",
        "                sheet_data = bus_worksheet_cache[bus]\n",
        "            else:\n",
        "                # Fetch the bus worksheet data and store in cache\n",
        "                sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "                sheet_data = sheet.get_all_values()\n",
        "                bus_worksheet_cache[bus] = sheet_data\n",
        "\n",
        "            # Create a dictionary for faster exact date lookups\n",
        "            date_value_lookup = {}\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12:\n",
        "                        date_str_key = row[1].strip()\n",
        "                        date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for date_str in dates:\n",
        "                # Check for exact match using the lookup dictionary\n",
        "                exact_value = date_value_lookup.get(date_str)\n",
        "\n",
        "                if exact_value:\n",
        "                    results.append((bus, date_str, exact_value, \"exact\", \"\"))\n",
        "                    sheet_success += 1\n",
        "                else:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date))\n",
        "                        sheet_success += 1\n",
        "\n",
        "            print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Process buses in parallel using ThreadPoolExecutor\n",
        "    print(f\"🚀 Processing {len(bus_date_map)} buses in parallel...\")\n",
        "\n",
        "    # Convert to list for ThreadPoolExecutor\n",
        "    bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    # Adjust max_workers based on your system's capabilities\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "    # Process all results\n",
        "    for bus_results in all_results:\n",
        "        for bus, date_str, value, match_type, fallback_date in bus_results:\n",
        "            bus_date_value_map[(bus, date_str)] = value\n",
        "            match_type_map[(bus, date_str)] = match_type\n",
        "            fallback_date_map[(bus, date_str)] = fallback_date\n",
        "\n",
        "            if match_type == \"exact\":\n",
        "                success_count += 1\n",
        "            else:\n",
        "                fallback_count += 1\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 50  # Adjust based on API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update\n",
        "                worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 2  # Divide by 2 because we have 2 cells per row\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//2} rows updated\")\n",
        "\n",
        "                # Add a small delay between chunks to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in column L and column M\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Main execution\n",
        "print(\"🚀 Starting to process all engine oil worksheets...\")\n",
        "\n",
        "# Create a global cache for bus worksheets to reuse across all worksheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Process each worksheet in sequence\n",
        "for worksheet_name in engine_oil_worksheets:\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name}\\n{'='*80}\")\n",
        "    process_worksheet(worksheet_name)\n",
        "    # Add a delay between worksheets to avoid rate limiting\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with batch processing\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")\n"
      ],
      "metadata": {
        "id": "Ui_8optlYNwb",
        "outputId": "236399cb-66ba-4a4d-d7c4-b7798de5203e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting to process all engine oil worksheets...\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_rr\n",
            "================================================================================\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_rr\n",
            "🧹 Clearing range L2:M in engine_oil_CH_BSIV_CH_MAS_rr...\n",
            "🔍 Starting data processing with date fallback search...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 946 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates...\n",
            "✅ Found 76 unique bus numbers\n",
            "✅ Found 931 rows to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "🚀 Processing 76 buses in parallel...\n",
            "  ✅ Found 17/18 values for bus 504\n",
            "  ✅ Found 12/13 values for bus 442\n",
            "  ✅ Found 9/10 values for bus 530\n",
            "  ✅ Found 10/11 values for bus 459\n",
            "  ✅ Found 5/6 values for bus 521\n",
            "❌ Could not process sheet '477': 477\n",
            "❌ Could not process sheet '326': 326\n",
            "❌ Could not process sheet '322': 322\n",
            "❌ Could not process sheet '317': 317\n",
            "❌ Could not process sheet '342': 342\n",
            "  ✅ Found 33/34 values for bus 469\n",
            "  ✅ Found 17/17 values for bus 508\n",
            "  ✅ Found 15/15 values for bus 486\n",
            "❌ Could not process sheet '381': 381\n",
            "❌ Could not process sheet '493': 493\n",
            "  ✅ Found 40/40 values for bus 585\n",
            "  ✅ Found 28/28 values for bus 460\n",
            "  ✅ Found 2/2 values for bus 546\n",
            "  ✅ Found 42/42 values for bus 474\n",
            "❌ Could not process sheet '319': 319\n",
            "❌ Could not process sheet '443': 443\n",
            "  ✅ Found 47/47 values for bus 462\n",
            "❌ Could not process sheet '576': 576\n",
            "  ✅ Found 1/1 values for bus 595\n",
            "  ✅ Found 14/14 values for bus 473\n",
            "  ✅ Found 22/22 values for bus 534\n",
            "❌ Could not process sheet 'RCS': RCS\n",
            "  ✅ Found 14/14 values for bus 535\n",
            "❌ Could not process sheet '560': 560\n",
            "  ✅ Found 26/26 values for bus 444\n",
            "  ✅ Found 18/18 values for bus 440\n",
            "  ✅ Found 14/14 values for bus 523\n",
            "  ✅ Found 2/2 values for bus 574\n",
            "  ✅ Found 48/48 values for bus 457\n",
            "❌ Could not process sheet '419': 419\n",
            "  ✅ Found 6/6 values for bus 590\n",
            "  ✅ Found 1/1 values for bus 548\n",
            "  ✅ Found 31/31 values for bus 515\n",
            "❌ Could not process sheet '341': 341\n",
            "  ✅ Found 19/19 values for bus 476\n",
            "  ✅ Found 26/26 values for bus 536\n",
            "  ✅ Found 12/12 values for bus 475\n",
            "❌ Could not process sheet '553': 553\n",
            "  ✅ Found 19/19 values for bus 514\n",
            "  ✅ Found 1/1 values for bus 578\n",
            "  ✅ Found 14/14 values for bus 528\n",
            "❌ Could not process sheet '314': 314\n",
            "  ✅ Found 31/31 values for bus 472\n",
            "  ✅ Found 13/13 values for bus 538\n",
            "  ✅ Found 8/8 values for bus 441\n",
            "  ✅ Found 2/2 values for bus 589\n",
            "❌ Could not process sheet '513': 513\n",
            "❌ Could not process sheet '352': 352\n",
            "❌ Could not process sheet '544': 544\n",
            "❌ Could not process sheet '487': 487\n",
            "❌ Could not process sheet 'ES': ES\n",
            "❌ Could not process sheet '368': 368\n",
            "  ✅ Found 15/15 values for bus 511\n",
            "  ✅ Found 25/25 values for bus 471\n",
            "❌ Could not process sheet '438': 438\n",
            "❌ Could not process sheet 'D.G.SET': D.G.SET\n",
            "❌ Could not process sheet '435': 435\n",
            "❌ Could not process sheet '409': 409\n",
            "❌ Could not process sheet '593': 593\n",
            "  ✅ Found 1/1 values for bus 563\n",
            "  ✅ Found 3/3 values for bus 587\n",
            "  ✅ Found 2/2 values for bus 550\n",
            "  ✅ Found 24/24 values for bus 495\n",
            "❌ Could not process sheet 'AirComp.': AirComp.\n",
            "  ✅ Found 1/1 values for bus 575\n",
            "❌ Could not process sheet '482': 482\n",
            "❌ Could not process sheet '552': 552\n",
            "❌ Could not process sheet 'B/T': B/T\n",
            "  ✅ Found 1/1 values for bus 613\n",
            "  ✅ Found 2/2 values for bus 547\n",
            "  ✅ Found 1/1 values for bus 518\n",
            "\n",
            "✅ Found values for 496 exact date matches\n",
            "✅ Found values for 198 fallback date matches\n",
            "❌ Could not find values for 237 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CH_BSIV_CH_MAS_rr...\n",
            "✅ Prepared 1400 cell updates\n",
            "🔄 Executing batch updates in 28 chunks...\n",
            "📊 Progress: Chunk 1/28 complete - 25/700 rows updated\n",
            "📊 Progress: Chunk 2/28 complete - 50/700 rows updated\n",
            "📊 Progress: Chunk 3/28 complete - 75/700 rows updated\n",
            "⏳ Rate limit hit, waiting for 2.60 seconds (retry 1/5)\n",
            "⏳ Rate limit hit, waiting for 4.44 seconds (retry 2/5)\n",
            "⏳ Rate limit hit, waiting for 8.23 seconds (retry 3/5)\n",
            "⏳ Rate limit hit, waiting for 16.45 seconds (retry 4/5)\n",
            "📊 Progress: Chunk 4/28 complete - 100/700 rows updated\n",
            "📊 Progress: Chunk 5/28 complete - 125/700 rows updated\n",
            "📊 Progress: Chunk 6/28 complete - 150/700 rows updated\n",
            "📊 Progress: Chunk 7/28 complete - 175/700 rows updated\n",
            "📊 Progress: Chunk 8/28 complete - 200/700 rows updated\n",
            "📊 Progress: Chunk 9/28 complete - 225/700 rows updated\n",
            "📊 Progress: Chunk 10/28 complete - 250/700 rows updated\n",
            "📊 Progress: Chunk 11/28 complete - 275/700 rows updated\n",
            "📊 Progress: Chunk 12/28 complete - 300/700 rows updated\n",
            "⏳ Rate limit hit, waiting for 2.54 seconds (retry 1/5)\n",
            "⏳ Rate limit hit, waiting for 4.32 seconds (retry 2/5)\n",
            "⏳ Rate limit hit, waiting for 8.26 seconds (retry 3/5)\n",
            "📊 Progress: Chunk 13/28 complete - 325/700 rows updated\n",
            "📊 Progress: Chunk 14/28 complete - 350/700 rows updated\n",
            "📊 Progress: Chunk 15/28 complete - 375/700 rows updated\n",
            "📊 Progress: Chunk 16/28 complete - 400/700 rows updated\n",
            "📊 Progress: Chunk 17/28 complete - 425/700 rows updated\n",
            "📊 Progress: Chunk 18/28 complete - 450/700 rows updated\n",
            "⏳ Rate limit hit, waiting for 2.85 seconds (retry 1/5)\n",
            "⏳ Rate limit hit, waiting for 4.41 seconds (retry 2/5)\n",
            "📊 Progress: Chunk 19/28 complete - 475/700 rows updated\n",
            "📊 Progress: Chunk 20/28 complete - 500/700 rows updated\n",
            "📊 Progress: Chunk 21/28 complete - 525/700 rows updated\n",
            "📊 Progress: Chunk 22/28 complete - 550/700 rows updated\n",
            "📊 Progress: Chunk 23/28 complete - 575/700 rows updated\n",
            "📊 Progress: Chunk 24/28 complete - 600/700 rows updated\n",
            "⏳ Rate limit hit, waiting for 2.25 seconds (retry 1/5)\n",
            "⏳ Rate limit hit, waiting for 4.41 seconds (retry 2/5)\n",
            "📊 Progress: Chunk 25/28 complete - 625/700 rows updated\n",
            "📊 Progress: Chunk 26/28 complete - 650/700 rows updated\n",
            "📊 Progress: Chunk 27/28 complete - 675/700 rows updated\n",
            "📊 Progress: Chunk 28/28 complete - 700/700 rows updated\n",
            "\n",
            "🎨 Applying formatting to distinguish match types...\n",
            "\n",
            "✅ COMPLETE for engine_oil_CH_BSIV_CH_MAS_rr: Updated 700 rows in column L and column M\n",
            "📊 Summary: 496 exact matches (normal format), 198 fallback matches (italic and right-aligned)\n",
            "❌ 231 rows could not be updated\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_rr\n",
            "================================================================================\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_rr\n",
            "🧹 Clearing range L2:M in engine_oil_CK_BSVI_CK_MAS_rr...\n",
            "🔍 Starting data processing with date fallback search...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 12 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates...\n",
            "✅ Found 4 unique bus numbers\n",
            "✅ Found 7 rows to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "🚀 Processing 4 buses in parallel...\n",
            "❌ Could not process sheet '610': 610\n",
            "  ✅ Found 3/3 values for bus 613\n",
            "  ✅ Found 2/2 values for bus 612\n",
            "  ✅ Found 1/1 values for bus 614\n",
            "\n",
            "✅ Found values for 4 exact date matches\n",
            "✅ Found values for 2 fallback date matches\n",
            "❌ Could not find values for 1 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CK_BSVI_CK_MAS_rr...\n",
            "✅ Prepared 12 cell updates\n",
            "🔄 Executing batch updates in 1 chunks...\n",
            "⏳ Rate limit hit, waiting for 2.74 seconds (retry 1/5)\n",
            "📊 Progress: Chunk 1/1 complete - 6/6 rows updated\n",
            "\n",
            "🎨 Applying formatting to distinguish match types...\n",
            "\n",
            "✅ COMPLETE for engine_oil_CK_BSVI_CK_MAS_rr: Updated 6 rows in column L and column M\n",
            "📊 Summary: 4 exact matches (normal format), 2 fallback matches (italic and right-aligned)\n",
            "❌ 1 rows could not be updated\n",
            "\n",
            "✅ All worksheets have been processed successfully!\n",
            "💡 Performance Summary:\n",
            "✓ Processed 2 worksheets with batch processing\n",
            "✓ Used parallel processing for bus data with ThreadPoolExecutor\n",
            "✓ Implemented caching to reduce API calls\n",
            "✓ Used batch updates to minimize API requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ATR , (RR, code of 1.0.7 is fast but only for CH, if we incorporate CK, and adopt for ATR and FG it will be super fast)\n",
        "\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_ATR',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_ATR'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Function to process each worksheet\n",
        "def process_worksheet(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "    clear_range = 'L2:M'\n",
        "    print(f\"🧹 Clearing range {clear_range} in {worksheet_name}...\")\n",
        "    worksheet.batch_clear([clear_range])\n",
        "\n",
        "    print(\"🔍 Starting data processing with date fallback search...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping\n",
        "    print(\"🔢 Mapping bus numbers and dates...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            date_str = row[1].strip()\n",
        "            bus_num = row[4].strip()\n",
        "            if date_str and bus_num:\n",
        "                bus_date_map[bus_num].add(date_str)\n",
        "                row_map[i] = (bus_num, date_str)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers\")\n",
        "    print(f\"✅ Found {len(row_map)} rows to process\")\n",
        "\n",
        "    # Create a cache for worksheet data to avoid redundant fetches\n",
        "    bus_worksheet_cache = {}\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Function to process a single bus\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "\n",
        "        try:\n",
        "            # Check if we already have this bus's data in cache\n",
        "            if bus in bus_worksheet_cache:\n",
        "                sheet_data = bus_worksheet_cache[bus]\n",
        "            else:\n",
        "                # Fetch the bus worksheet data and store in cache\n",
        "                sheet = Odometer_spreadsheet_SVP.worksheet(bus)\n",
        "                sheet_data = sheet.get_all_values()\n",
        "                bus_worksheet_cache[bus] = sheet_data\n",
        "\n",
        "            # Create a dictionary for faster exact date lookups\n",
        "            date_value_lookup = {}\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12:\n",
        "                        date_str_key = row[1].strip()\n",
        "                        date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for date_str in dates:\n",
        "                # Check for exact match using the lookup dictionary\n",
        "                exact_value = date_value_lookup.get(date_str)\n",
        "\n",
        "                if exact_value:\n",
        "                    results.append((bus, date_str, exact_value, \"exact\", \"\"))\n",
        "                    sheet_success += 1\n",
        "                else:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date))\n",
        "                        sheet_success += 1\n",
        "\n",
        "            print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Process buses in parallel using ThreadPoolExecutor\n",
        "    print(f\"🚀 Processing {len(bus_date_map)} buses in parallel...\")\n",
        "\n",
        "    # Convert to list for ThreadPoolExecutor\n",
        "    bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    # Adjust max_workers based on your system's capabilities\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "    # Process all results\n",
        "    for bus_results in all_results:\n",
        "        for bus, date_str, value, match_type, fallback_date in bus_results:\n",
        "            bus_date_value_map[(bus, date_str)] = value\n",
        "            match_type_map[(bus, date_str)] = match_type\n",
        "            fallback_date_map[(bus, date_str)] = fallback_date\n",
        "\n",
        "            if match_type == \"exact\":\n",
        "                success_count += 1\n",
        "            else:\n",
        "                fallback_count += 1\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 50  # Adjust based on API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update\n",
        "                worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 2  # Divide by 2 because we have 2 cells per row\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//2} rows updated\")\n",
        "\n",
        "                # Add a small delay between chunks to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in column L and column M\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Main execution\n",
        "print(\"🚀 Starting to process all engine oil worksheets...\")\n",
        "\n",
        "# Create a global cache for bus worksheets to reuse across all worksheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Process each worksheet in sequence\n",
        "for worksheet_name in engine_oil_worksheets:\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name}\\n{'='*80}\")\n",
        "    process_worksheet(worksheet_name)\n",
        "    # Add a delay between worksheets to avoid rate limiting\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with batch processing\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")\n"
      ],
      "metadata": {
        "id": "xX4H7u05a-I3",
        "outputId": "25b5f566-7f5f-421b-b1f6-24bb6b3b4659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting to process all engine oil worksheets...\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_ATR\n",
            "================================================================================\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_ATR\n",
            "🧹 Clearing range L2:M in engine_oil_CH_BSIV_CH_MAS_ATR...\n",
            "🔍 Starting data processing with date fallback search...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 900 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates...\n",
            "✅ Found 39 unique bus numbers\n",
            "✅ Found 352 rows to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "🚀 Processing 39 buses in parallel...\n",
            "❌ Could not process sheet '381': 381\n",
            "❌ Could not process sheet '579': 579\n",
            "❌ Could not process sheet '576': 576\n",
            "❌ Could not process sheet '368': 368\n",
            "  ✅ Found 16/16 values for bus 574\n",
            "  ✅ Found 5/5 values for bus 575\n",
            "  ✅ Found 14/14 values for bus 590\n",
            "  ✅ Found 31/31 values for bus 546\n",
            "❌ Could not process sheet '565': 565\n",
            "❌ Could not process sheet '319': 319\n",
            "  ✅ Found 16/16 values for bus 550\n",
            "  ✅ Found 29/29 values for bus 548\n",
            "  ✅ Found 3/3 values for bus 584\n",
            "  ✅ Found 3/3 values for bus 564\n",
            "❌ Could not process sheet '566': 566\n",
            "❌ Could not process sheet '560': 560\n",
            "  ✅ Found 22/22 values for bus 547\n",
            "❌ Could not process sheet '545': 545\n",
            "❌ Could not process sheet '591': 591\n",
            "  ✅ Found 19/19 values for bus 595\n",
            "  ✅ Found 10/10 values for bus 495\n",
            "❌ Could not process sheet '593': 593\n",
            "  ✅ Found 32/32 values for bus 578\n",
            "❌ Could not process sheet '549': 549\n",
            "❌ Could not process sheet '553': 553\n",
            "❌ Could not process sheet '544': 544\n",
            "❌ Could not process sheet '430': 430\n",
            "  ✅ Found 6/6 values for bus 589\n",
            "  ✅ Found 30/30 values for bus 563\n",
            "❌ Could not process sheet '317': 317\n",
            "❌ Could not process sheet '562': 562\n",
            "❌ Could not process sheet '433': 433\n",
            "❌ Could not process sheet '409': 409\n",
            "  ✅ Found 24/24 values for bus 588\n",
            "  ✅ Found 1/1 values for bus 474\n",
            "  ✅ Found 34/34 values for bus 577\n",
            "❌ Could not process sheet 'Grease Gun': Grease Gun\n",
            "❌ Could not process sheet '557': 557\n",
            "  ✅ Found 13/13 values for bus 587\n",
            "\n",
            "✅ Found values for 72 exact date matches\n",
            "✅ Found values for 236 fallback date matches\n",
            "❌ Could not find values for 44 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CH_BSIV_CH_MAS_ATR...\n",
            "✅ Prepared 616 cell updates\n",
            "🔄 Executing batch updates in 13 chunks...\n",
            "📊 Progress: Chunk 1/13 complete - 25/308 rows updated\n",
            "📊 Progress: Chunk 2/13 complete - 50/308 rows updated\n",
            "📊 Progress: Chunk 3/13 complete - 75/308 rows updated\n",
            "📊 Progress: Chunk 4/13 complete - 100/308 rows updated\n",
            "📊 Progress: Chunk 5/13 complete - 125/308 rows updated\n",
            "📊 Progress: Chunk 6/13 complete - 150/308 rows updated\n",
            "📊 Progress: Chunk 7/13 complete - 175/308 rows updated\n",
            "📊 Progress: Chunk 8/13 complete - 200/308 rows updated\n",
            "⏳ Rate limit hit, waiting for 2.71 seconds (retry 1/5)\n",
            "⏳ Rate limit hit, waiting for 4.49 seconds (retry 2/5)\n",
            "⏳ Rate limit hit, waiting for 8.47 seconds (retry 3/5)\n",
            "⏳ Rate limit hit, waiting for 16.31 seconds (retry 4/5)\n",
            "📊 Progress: Chunk 9/13 complete - 225/308 rows updated\n",
            "📊 Progress: Chunk 10/13 complete - 250/308 rows updated\n",
            "📊 Progress: Chunk 11/13 complete - 275/308 rows updated\n",
            "📊 Progress: Chunk 12/13 complete - 300/308 rows updated\n",
            "📊 Progress: Chunk 13/13 complete - 308/308 rows updated\n",
            "\n",
            "🎨 Applying formatting to distinguish match types...\n",
            "\n",
            "✅ COMPLETE for engine_oil_CH_BSIV_CH_MAS_ATR: Updated 308 rows in column L and column M\n",
            "📊 Summary: 72 exact matches (normal format), 236 fallback matches (italic and right-aligned)\n",
            "❌ 44 rows could not be updated\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_ATR\n",
            "================================================================================\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_ATR\n",
            "🧹 Clearing range L2:M in engine_oil_CK_BSVI_CK_MAS_ATR...\n",
            "🔍 Starting data processing with date fallback search...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 35 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates...\n",
            "✅ Found 8 unique bus numbers\n",
            "✅ Found 25 rows to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "🚀 Processing 8 buses in parallel...\n",
            "❌ Could not process sheet '600': 600\n",
            "  ✅ Found 4/4 values for bus 597\n",
            "  ✅ Found 4/4 values for bus 596\n",
            "  ✅ Found 3/3 values for bus 598\n",
            "  ✅ Found 4/4 values for bus 603\n",
            "  ✅ Found 3/3 values for bus 599\n",
            "  ✅ Found 3/3 values for bus 604\n",
            "  ✅ Found 3/3 values for bus 605\n",
            "\n",
            "✅ Found values for 1 exact date matches\n",
            "✅ Found values for 23 fallback date matches\n",
            "❌ Could not find values for 1 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CK_BSVI_CK_MAS_ATR...\n",
            "✅ Prepared 48 cell updates\n",
            "🔄 Executing batch updates in 1 chunks...\n",
            "📊 Progress: Chunk 1/1 complete - 24/24 rows updated\n",
            "\n",
            "🎨 Applying formatting to distinguish match types...\n",
            "\n",
            "✅ COMPLETE for engine_oil_CK_BSVI_CK_MAS_ATR: Updated 24 rows in column L and column M\n",
            "📊 Summary: 1 exact matches (normal format), 23 fallback matches (italic and right-aligned)\n",
            "❌ 1 rows could not be updated\n",
            "\n",
            "✅ All worksheets have been processed successfully!\n",
            "💡 Performance Summary:\n",
            "✓ Processed 2 worksheets with batch processing\n",
            "✓ Used parallel processing for bus data with ThreadPoolExecutor\n",
            "✓ Implemented caching to reduce API calls\n",
            "✓ Used batch updates to minimize API requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FG_Working, Best Code\n",
        "\n",
        "# Modified to process multiple engine oil worksheets, Added batch processing done\n",
        "\n",
        "# Modified to process multiple engine oil worksheets with batch processing\n",
        "# With fallback date display in M Column, For ATR_MAS_Engine Oil\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_FG',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_FG'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Function to process each worksheet\n",
        "def process_worksheet(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # Clear data in range L2:M, Odometer value in Engine Oil and Fallback date removal\n",
        "    clear_range = 'L2:M'\n",
        "    print(f\"🧹 Clearing range {clear_range} in {worksheet_name}...\")\n",
        "    worksheet.batch_clear([clear_range])\n",
        "\n",
        "    print(\"🔍 Starting data processing with date fallback search...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping\n",
        "    print(\"🔢 Mapping bus numbers and dates...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            date_str = row[1].strip()\n",
        "            bus_num = row[4].strip()\n",
        "            if date_str and bus_num:\n",
        "                bus_date_map[bus_num].add(date_str)\n",
        "                row_map[i] = (bus_num, date_str)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers\")\n",
        "    print(f\"✅ Found {len(row_map)} rows to process\")\n",
        "\n",
        "    # Create a cache for worksheet data to avoid redundant fetches\n",
        "    bus_worksheet_cache = {}\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Function to process a single bus\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "\n",
        "        try:\n",
        "            # Check if we already have this bus's data in cache\n",
        "            if bus in bus_worksheet_cache:\n",
        "                sheet_data = bus_worksheet_cache[bus]\n",
        "            else:\n",
        "                # Fetch the bus worksheet data and store in cache\n",
        "                sheet = Odometer_spreadsheet_FG.worksheet(bus)\n",
        "                sheet_data = sheet.get_all_values()\n",
        "                bus_worksheet_cache[bus] = sheet_data\n",
        "\n",
        "            # Create a dictionary for faster exact date lookups\n",
        "            date_value_lookup = {}\n",
        "            for row in sheet_data[1:]:\n",
        "                try:\n",
        "                    if len(row) > 12:\n",
        "                        date_str_key = row[1].strip()\n",
        "                        date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            for date_str in dates:\n",
        "                # Check for exact match using the lookup dictionary\n",
        "                exact_value = date_value_lookup.get(date_str)\n",
        "\n",
        "                if exact_value:\n",
        "                    results.append((bus, date_str, exact_value, \"exact\", \"\"))\n",
        "                    sheet_success += 1\n",
        "                else:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date))\n",
        "                        sheet_success += 1\n",
        "\n",
        "            print(f\"  ✅ Found {sheet_success}/{len(dates)} values for bus {bus}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not process sheet '{bus}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Process buses in parallel using ThreadPoolExecutor\n",
        "    print(f\"🚀 Processing {len(bus_date_map)} buses in parallel...\")\n",
        "\n",
        "    # Convert to list for ThreadPoolExecutor\n",
        "    bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    # Adjust max_workers based on your system's capabilities\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "    # Process all results\n",
        "    for bus_results in all_results:\n",
        "        for bus, date_str, value, match_type, fallback_date in bus_results:\n",
        "            bus_date_value_map[(bus, date_str)] = value\n",
        "            match_type_map[(bus, date_str)] = match_type\n",
        "            fallback_date_map[(bus, date_str)] = fallback_date\n",
        "\n",
        "            if match_type == \"exact\":\n",
        "                success_count += 1\n",
        "            else:\n",
        "                fallback_count += 1\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date\n",
        "            })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 50  # Adjust based on API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update\n",
        "                worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 2  # Divide by 2 because we have 2 cells per row\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//2} rows updated\")\n",
        "\n",
        "                # Add a small delay between chunks to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = (2 ** retry_count) + random.random()\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in column L and column M\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Main execution\n",
        "print(\"🚀 Starting to process all engine oil worksheets...\")\n",
        "\n",
        "# Create a global cache for bus worksheets to reuse across all worksheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Process each worksheet in sequence\n",
        "for worksheet_name in engine_oil_worksheets:\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name}\\n{'='*80}\")\n",
        "    process_worksheet(worksheet_name)\n",
        "    # Add a delay between worksheets to avoid rate limiting\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with batch processing\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")\n"
      ],
      "metadata": {
        "id": "ng3__2UKVfZL",
        "outputId": "4b890543-a8fd-401a-e436-ccd3bc587ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting to process all engine oil worksheets...\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_FG\n",
            "================================================================================\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CH_BSIV_CH_MAS_FG\n",
            "🧹 Clearing range L2:M in engine_oil_CH_BSIV_CH_MAS_FG...\n",
            "🔍 Starting data processing with date fallback search...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 999 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates...\n",
            "✅ Found 37 unique bus numbers\n",
            "✅ Found 978 rows to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "🚀 Processing 37 buses in parallel...\n",
            "❌ Could not process sheet 'tyreplant': tyreplant\n",
            "❌ Could not process sheet '388': 388\n",
            "❌ Could not process sheet '419': 419\n",
            "❌ Could not process sheet '389': 389\n",
            "  ✅ Found 1/1 values for bus 463\n",
            "❌ Could not process sheet '415': 415\n",
            "  ✅ Found 34/34 values for bus 560\n",
            "❌ Could not process sheet '435': 435\n",
            "  ✅ Found 29/29 values for bus 507\n",
            "❌ Could not process sheet 'workshop': workshop\n",
            "❌ Could not process sheet '370': 370\n",
            "  ✅ Found 67/67 values for bus 516\n",
            "❌ Could not process sheet '386': 386\n",
            "  ✅ Found 41/41 values for bus 573\n",
            "❌ Could not process sheet '328': 328\n",
            "❌ Could not process sheet '202': 202\n",
            "  ✅ Found 43/43 values for bus 438\n",
            "❌ Could not process sheet '436': 436\n",
            "  ✅ Found 53/53 values for bus 529\n",
            "  ✅ Found 30/30 values for bus 479\n",
            "  ✅ Found 35/35 values for bus 461\n",
            "❌ Could not process sheet '344': 344\n",
            "  ✅ Found 32/32 values for bus 561\n",
            "❌ Could not process sheet 'tyre plant': tyre plant\n",
            "❌ Could not process sheet '350': 350\n",
            "  ✅ Found 23/24 values for bus 437\n",
            "❌ Could not process sheet '378': 378\n",
            "❌ Could not process sheet 'Tyre Plant': Tyre Plant\n",
            "❌ Could not process sheet 'others': others\n",
            "  ✅ Found 21/21 values for bus 593\n",
            "  ✅ Found 24/24 values for bus 592\n",
            "  ✅ Found 27/27 values for bus 478\n",
            "  ✅ Found 14/17 values for bus 572\n",
            "❌ Could not process sheet 'OTHER': OTHER\n",
            "❌ Could not process sheet 'TYRE PLANT': TYRE PLANT\n",
            "  ✅ Found 5/5 values for bus 493\n",
            "  ✅ Found 9/9 values for bus 562\n",
            "\n",
            "✅ Found values for 480 exact date matches\n",
            "✅ Found values for 8 fallback date matches\n",
            "❌ Could not find values for 490 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CH_BSIV_CH_MAS_FG...\n",
            "✅ Prepared 982 cell updates\n",
            "🔄 Executing batch updates in 20 chunks...\n",
            "📊 Progress: Chunk 1/20 complete - 25/491 rows updated\n",
            "📊 Progress: Chunk 2/20 complete - 50/491 rows updated\n",
            "📊 Progress: Chunk 3/20 complete - 75/491 rows updated\n",
            "⏳ Rate limit hit, waiting for 2.84 seconds (retry 1/5)\n",
            "⏳ Rate limit hit, waiting for 4.24 seconds (retry 2/5)\n",
            "⏳ Rate limit hit, waiting for 8.06 seconds (retry 3/5)\n",
            "⏳ Rate limit hit, waiting for 16.06 seconds (retry 4/5)\n",
            "📊 Progress: Chunk 4/20 complete - 100/491 rows updated\n",
            "📊 Progress: Chunk 5/20 complete - 125/491 rows updated\n",
            "📊 Progress: Chunk 6/20 complete - 150/491 rows updated\n",
            "📊 Progress: Chunk 7/20 complete - 175/491 rows updated\n",
            "📊 Progress: Chunk 8/20 complete - 200/491 rows updated\n",
            "📊 Progress: Chunk 9/20 complete - 225/491 rows updated\n",
            "📊 Progress: Chunk 10/20 complete - 250/491 rows updated\n",
            "📊 Progress: Chunk 11/20 complete - 275/491 rows updated\n",
            "📊 Progress: Chunk 12/20 complete - 300/491 rows updated\n",
            "📊 Progress: Chunk 13/20 complete - 325/491 rows updated\n",
            "📊 Progress: Chunk 14/20 complete - 350/491 rows updated\n",
            "⏳ Rate limit hit, waiting for 2.06 seconds (retry 1/5)\n",
            "⏳ Rate limit hit, waiting for 4.45 seconds (retry 2/5)\n",
            "⏳ Rate limit hit, waiting for 8.25 seconds (retry 3/5)\n",
            "⏳ Rate limit hit, waiting for 16.80 seconds (retry 4/5)\n",
            "📊 Progress: Chunk 15/20 complete - 375/491 rows updated\n",
            "📊 Progress: Chunk 16/20 complete - 400/491 rows updated\n",
            "📊 Progress: Chunk 17/20 complete - 425/491 rows updated\n",
            "📊 Progress: Chunk 18/20 complete - 450/491 rows updated\n",
            "📊 Progress: Chunk 19/20 complete - 475/491 rows updated\n",
            "📊 Progress: Chunk 20/20 complete - 491/491 rows updated\n",
            "\n",
            "🎨 Applying formatting to distinguish match types...\n",
            "\n",
            "✅ COMPLETE for engine_oil_CH_BSIV_CH_MAS_FG: Updated 491 rows in column L and column M\n",
            "📊 Summary: 480 exact matches (normal format), 8 fallback matches (italic and right-aligned)\n",
            "❌ 487 rows could not be updated\n",
            "\n",
            "================================================================================\n",
            "📊 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_FG\n",
            "================================================================================\n",
            "\n",
            "🔄 Processing worksheet: engine_oil_CK_BSVI_CK_MAS_FG\n",
            "🧹 Clearing range L2:M in engine_oil_CK_BSVI_CK_MAS_FG...\n",
            "🔍 Starting data processing with date fallback search...\n",
            "📥 Loading worksheet data...\n",
            "✅ Loaded 7 rows from worksheet\n",
            "🔢 Mapping bus numbers and dates...\n",
            "✅ Found 3 unique bus numbers\n",
            "✅ Found 3 rows to process\n",
            "\n",
            "🔍 Searching for exact dates or closest earlier dates...\n",
            "🚀 Processing 3 buses in parallel...\n",
            "  ✅ Found 1/1 values for bus 617\n",
            "  ✅ Found 1/1 values for bus 615\n",
            "  ✅ Found 1/1 values for bus 616\n",
            "\n",
            "✅ Found values for 3 exact date matches\n",
            "✅ Found values for 0 fallback date matches\n",
            "❌ Could not find values for 0 dates\n",
            "\n",
            "📝 Preparing batch updates for engine_oil_CK_BSVI_CK_MAS_FG...\n",
            "✅ Prepared 6 cell updates\n",
            "🔄 Executing batch updates in 1 chunks...\n",
            "📊 Progress: Chunk 1/1 complete - 3/3 rows updated\n",
            "\n",
            "🎨 Applying formatting to distinguish match types...\n",
            "\n",
            "✅ COMPLETE for engine_oil_CK_BSVI_CK_MAS_FG: Updated 3 rows in column L and column M\n",
            "📊 Summary: 3 exact matches (normal format), 0 fallback matches (italic and right-aligned)\n",
            "❌ 0 rows could not be updated\n",
            "\n",
            "✅ All worksheets have been processed successfully!\n",
            "💡 Performance Summary:\n",
            "✓ Processed 2 worksheets with batch processing\n",
            "✓ Used parallel processing for bus data with ThreadPoolExecutor\n",
            "✓ Implemented caching to reduce API calls\n",
            "✓ Used batch updates to minimize API requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lean for all units, improved API limiter, #Lean combined all Units Engine Oil and Search all Odometer, but it search only missing values, if it works, then in future just add engine oil sheet and odometer sheet\n",
        "\n",
        "\n",
        "# Lean version - Only processes rows with empty L column\n",
        "# With fallback date display in M Column, For SVP, FG, BT, RT, MB, and DP\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import gspread\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# List of all worksheets to process\n",
        "engine_oil_worksheets = [\n",
        "    'engine_oil_CH_BSIV_CH_MAS_FG',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_FG',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_ATR',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_ATR',\n",
        "    'engine_oil_CH_BSIV_CH_MAS_rr',\n",
        "    'engine_oil_CK_BSVI_CK_MAS_rr'\n",
        "]\n",
        "\n",
        "# List of all odometer spreadsheets to search\n",
        "odometer_spreadsheets = [\n",
        "    'Odometer_spreadsheet_SVP',\n",
        "    'Odometer_spreadsheet_FG',\n",
        "    'Odometer_spreadsheet_BT',\n",
        "    'Odometer_spreadsheet_RT',\n",
        "    'Odometer_spreadsheet_MB',\n",
        "    'Odometer_spreadsheet_DP'\n",
        "]\n",
        "\n",
        "# Function to parse custom date format 'DD,MMMyy' like '13,Apr24'\n",
        "def parse_custom_date(date_str):\n",
        "    if not date_str or date_str == '':\n",
        "        return None\n",
        "\n",
        "    pattern = r'(\\d{1,2}),(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(\\d{2})'\n",
        "    match = re.match(pattern, date_str)\n",
        "\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month_str = match.group(2)\n",
        "        year = int(match.group(3)) + 2000  # Assuming 20xx for the year\n",
        "\n",
        "        month_map = {\n",
        "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
        "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
        "        }\n",
        "\n",
        "        month = month_map[month_str]\n",
        "        return datetime(year, month, day)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Modified function to return both value and fallback date\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    # Parse all dates in the sheet\n",
        "    dates = []\n",
        "    parsed_dates = []\n",
        "    values = []\n",
        "\n",
        "    for row in sheet_data[1:]:  # Skip header\n",
        "        try:\n",
        "            if len(row) > value_column_index and len(row) > 1:\n",
        "                date_str = row[1].strip()\n",
        "                value = row[value_column_index]\n",
        "\n",
        "                parsed_date = parse_custom_date(date_str)\n",
        "                if parsed_date:\n",
        "                    dates.append(date_str)\n",
        "                    parsed_dates.append(parsed_date)\n",
        "                    values.append(value)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Parse target date\n",
        "    target_date = parse_custom_date(target_date_str)\n",
        "    if not target_date:\n",
        "        return None, None\n",
        "\n",
        "    # First check for exact match\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date == target_date:\n",
        "            return values[i], None  # Return value with no fallback date for exact matches\n",
        "\n",
        "    # If no exact match, find closest earlier date\n",
        "    closest_date = None\n",
        "    closest_value = None\n",
        "    max_date = None\n",
        "\n",
        "    for i, parsed_date in enumerate(parsed_dates):\n",
        "        if parsed_date < target_date and (max_date is None or parsed_date > max_date):\n",
        "            max_date = parsed_date\n",
        "            closest_date = dates[i]\n",
        "            closest_value = values[i]\n",
        "\n",
        "    return closest_value, closest_date  # Return both value and fallback date\n",
        "\n",
        "# Global cache for bus worksheets to reuse across all worksheets and spreadsheets\n",
        "global_bus_worksheet_cache = {}\n",
        "\n",
        "# Function to process each worksheet - LEAN VERSION (only empty L cells)\n",
        "def process_worksheet_lean(worksheet_name):\n",
        "    print(f\"\\n🔄 Processing worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Get the worksheet object\n",
        "    worksheet = None\n",
        "    try:\n",
        "        # Assuming these are defined in your notebook environment\n",
        "        worksheet = eval(worksheet_name)  # This will use the already defined variable in your notebook\n",
        "    except:\n",
        "        print(f\"❌ Error: Worksheet '{worksheet_name}' not found or not accessible\")\n",
        "        return\n",
        "\n",
        "    # NO CLEARING OF RANGES - we're only updating empty cells\n",
        "    print(\"🔍 Starting data processing for EMPTY L cells only...\")\n",
        "\n",
        "    # 📥 Load main sheet\n",
        "    print(\"📥 Loading worksheet data...\")\n",
        "    main_data = worksheet.get_all_values()\n",
        "    print(f\"✅ Loaded {len(main_data)-1} rows from worksheet\")\n",
        "\n",
        "    # Step 1: Collect unique (bus, date) and row mapping ONLY FOR EMPTY L CELLS\n",
        "    print(\"🔢 Mapping bus numbers and dates for empty L cells...\")\n",
        "    bus_date_map = defaultdict(set)\n",
        "    row_map = {}  # maps row number → (bus, date)\n",
        "    empty_cell_count = 0\n",
        "\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            # Check if L column (index 11) is empty\n",
        "            l_value = row[11].strip() if len(row) > 11 else \"\"\n",
        "\n",
        "            if not l_value:  # Only process if L column is empty\n",
        "                date_str = row[1].strip()\n",
        "                bus_num = row[4].strip()\n",
        "                if date_str and bus_num:\n",
        "                    bus_date_map[bus_num].add(date_str)\n",
        "                    row_map[i] = (bus_num, date_str)\n",
        "                    empty_cell_count += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Found {len(bus_date_map)} unique bus numbers with empty L cells\")\n",
        "    print(f\"✅ Found {empty_cell_count} empty L cells to process\")\n",
        "\n",
        "    # Exit early if no empty cells to process\n",
        "    if empty_cell_count == 0:\n",
        "        print(f\"✓ No empty L cells to process in {worksheet_name}. Moving to next worksheet.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Fetch each bus worksheet once, collect all data and find exact or closest earlier date\n",
        "    print(\"\\n🔍 Searching for exact dates or closest earlier dates...\")\n",
        "    bus_date_value_map = {}\n",
        "    match_type_map = {}  # Tracks whether each match is exact or fallback\n",
        "    fallback_date_map = {}  # Store the actual fallback date used\n",
        "    success_count = 0\n",
        "    fallback_count = 0\n",
        "\n",
        "    # Track which spreadsheet contained the match for metadata\n",
        "    spreadsheet_source_map = {}  # Maps (bus, date) to spreadsheet name\n",
        "\n",
        "    # Function to process a single bus across all spreadsheets\n",
        "    def process_bus(bus_dates_tuple):\n",
        "        bus, dates = bus_dates_tuple\n",
        "        sheet_success = 0\n",
        "        results = []\n",
        "        dates_to_process = set(dates)  # Create a copy to safely modify\n",
        "\n",
        "        # Try each spreadsheet in sequence\n",
        "        for spreadsheet_name in odometer_spreadsheets:\n",
        "            if not dates_to_process:  # If all dates have been found, skip further processing\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Generate cache key that includes the spreadsheet name\n",
        "                cache_key = f\"{spreadsheet_name}:{bus}\"\n",
        "\n",
        "                # Check if we already have this bus's data in cache\n",
        "                if cache_key in global_bus_worksheet_cache:\n",
        "                    sheet_data = global_bus_worksheet_cache[cache_key]\n",
        "                else:\n",
        "                    # Get the spreadsheet object dynamically\n",
        "                    try:\n",
        "                        spreadsheet = eval(spreadsheet_name)\n",
        "                        # Try to get the worksheet for this bus with retry mechanism\n",
        "                        max_retries = 5\n",
        "                        retry_count = 0\n",
        "                        while retry_count < max_retries:\n",
        "                            try:\n",
        "                                sheet = spreadsheet.worksheet(bus)\n",
        "                                sheet_data = sheet.get_all_values()\n",
        "                                global_bus_worksheet_cache[cache_key] = sheet_data\n",
        "                                break  # Success, exit retry loop\n",
        "                            except gspread.exceptions.APIError as api_error:\n",
        "                                if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                                    retry_count += 1\n",
        "                                    wait_time = (2 ** retry_count) + (random.random() * 2)\n",
        "                                    print(f\"⏳ Rate limit hit for bus {bus}, waiting {wait_time:.2f}s (retry {retry_count}/{max_retries})\")\n",
        "                                    time.sleep(wait_time)\n",
        "                                    if retry_count == max_retries:\n",
        "                                        print(f\"⚠️ Max retries reached for bus {bus} in {spreadsheet_name}\")\n",
        "                                        raise\n",
        "                                else:\n",
        "                                    # Not a rate limit issue, the worksheet likely doesn't exist\n",
        "                                    raise\n",
        "                            except Exception as e:\n",
        "                                # The worksheet doesn't exist in this spreadsheet\n",
        "                                if \"Worksheet not found\" in str(e) or \"404\" in str(e):\n",
        "                                    print(f\"  ℹ️ Bus {bus} not found in {spreadsheet_name}\")\n",
        "                                else:\n",
        "                                    print(f\"  ⚠️ Error accessing {bus} in {spreadsheet_name}: {str(e)}\")\n",
        "                                raise\n",
        "                    except Exception:\n",
        "                        # This bus doesn't exist in this spreadsheet or we couldn't access it\n",
        "                        continue\n",
        "\n",
        "                # Process each date for this bus in this spreadsheet\n",
        "                # Create a dictionary for faster exact date lookups\n",
        "                date_value_lookup = {}\n",
        "                for row in sheet_data[1:]:\n",
        "                    try:\n",
        "                        if len(row) > 12:\n",
        "                            date_str_key = row[1].strip()\n",
        "                            date_value_lookup[date_str_key] = row[12]  # Column M\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                dates_found_in_this_sheet = set()\n",
        "\n",
        "                # First try exact matches which are faster to check\n",
        "                for date_str in dates_to_process:\n",
        "                    # Check for exact match using the lookup dictionary\n",
        "                    exact_value = date_value_lookup.get(date_str)\n",
        "                    if exact_value:\n",
        "                        results.append((bus, date_str, exact_value, \"exact\", \"\", spreadsheet_name))\n",
        "                        sheet_success += 1\n",
        "                        dates_found_in_this_sheet.add(date_str)\n",
        "\n",
        "                # Then try fallback matches for remaining dates\n",
        "                remaining_dates = dates_to_process - dates_found_in_this_sheet\n",
        "                for date_str in remaining_dates:\n",
        "                    # Try to find closest earlier date\n",
        "                    value, fallback_date = find_closest_date_value(sheet_data, date_str, value_column_index=12)\n",
        "                    if value:\n",
        "                        results.append((bus, date_str, value, \"fallback\", fallback_date, spreadsheet_name))\n",
        "                        sheet_success += 1\n",
        "                        dates_found_in_this_sheet.add(date_str)\n",
        "\n",
        "                # Remove all processed dates\n",
        "                dates_to_process -= dates_found_in_this_sheet\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"Worksheet not found\" not in str(e) and \"404\" not in str(e):\n",
        "                    print(f\"❌ Could not process sheet '{bus}' in spreadsheet '{spreadsheet_name}': {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if sheet_success > 0:\n",
        "            print(f\"  ✅ Found {sheet_success} values for bus {bus}\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ No values found for bus {bus} in any spreadsheet\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    # Only process if we have buses to check\n",
        "    if len(bus_date_map) > 0:\n",
        "        # Convert to list for ThreadPoolExecutor\n",
        "        bus_dates_list = list(bus_date_map.items())\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        max_workers = min(5, len(bus_dates_list))  # Adjust based on data size\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            all_results = list(executor.map(process_bus, bus_dates_list))\n",
        "\n",
        "        # Process all results\n",
        "        for bus_results in all_results:\n",
        "            for bus, date_str, value, match_type, fallback_date, source_spreadsheet in bus_results:\n",
        "                bus_date_value_map[(bus, date_str)] = value\n",
        "                match_type_map[(bus, date_str)] = match_type\n",
        "                fallback_date_map[(bus, date_str)] = fallback_date\n",
        "                spreadsheet_source_map[(bus, date_str)] = source_spreadsheet\n",
        "\n",
        "                if match_type == \"exact\":\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    fallback_count += 1\n",
        "    else:\n",
        "        print(\"⚠️ No buses to process - all L cells have values already\")\n",
        "\n",
        "    print(f\"\\n✅ Found values for {success_count} exact date matches\")\n",
        "    print(f\"✅ Found values for {fallback_count} fallback date matches\")\n",
        "    print(f\"❌ Could not find values for {len(row_map) - (success_count + fallback_count)} dates\")\n",
        "\n",
        "    # Step 3: Update values into Column L of main sheet and fallback dates into Column M\n",
        "    print(f\"\\n📝 Preparing batch updates for {worksheet_name}...\")\n",
        "\n",
        "    # Prepare batch update for all cells at once\n",
        "    batch_updates = []\n",
        "    exact_match_cells = []\n",
        "    fallback_match_cells = []\n",
        "\n",
        "    for row_num, (bus, date_str) in row_map.items():\n",
        "        value = bus_date_value_map.get((bus, date_str), \"\")\n",
        "        fallback_date = fallback_date_map.get((bus, date_str), \"\")\n",
        "        spreadsheet_source = spreadsheet_source_map.get((bus, date_str), \"\")\n",
        "\n",
        "        if value:\n",
        "            # Add to batch updates list (row, col, value)\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 12,  # Column L\n",
        "                'value': value\n",
        "            })\n",
        "\n",
        "            # Add fallback date update\n",
        "            batch_updates.append({\n",
        "                'row': row_num,\n",
        "                'col': 13,  # Column M\n",
        "                'value': fallback_date if fallback_date else \"\"\n",
        "            })\n",
        "\n",
        "            # Add source spreadsheet info in Column N for tracking\n",
        "     #       batch_updates.append({\n",
        "      #          'row': row_num,\n",
        "      #          'col': 14,  # Column N\n",
        "      #          'value': spreadsheet_source\n",
        "      #      })\n",
        "\n",
        "            # Track which cells need which formatting\n",
        "            match_type = match_type_map.get((bus, date_str))\n",
        "            if match_type == \"exact\":\n",
        "                exact_match_cells.append(f\"L{row_num}\")\n",
        "            elif match_type == \"fallback\":\n",
        "                fallback_match_cells.append(f\"L{row_num}\")\n",
        "\n",
        "    print(f\"✅ Prepared {len(batch_updates)} cell updates\")\n",
        "\n",
        "    # Skip if no updates needed\n",
        "    if len(batch_updates) == 0:\n",
        "        print(\"✓ No updates needed for this worksheet\")\n",
        "        return\n",
        "\n",
        "    # Execute batch updates in chunks to avoid rate limits\n",
        "    updated_count = 0\n",
        "    chunk_size = 25  # Reduced chunk size to avoid API limits\n",
        "    chunks = math.ceil(len(batch_updates) / chunk_size)\n",
        "\n",
        "    print(f\"🔄 Executing batch updates in {chunks} chunks...\")\n",
        "\n",
        "    for chunk_index in range(chunks):\n",
        "        start_idx = chunk_index * chunk_size\n",
        "        end_idx = min(start_idx + chunk_size, len(batch_updates))\n",
        "        current_chunk = batch_updates[start_idx:end_idx]\n",
        "\n",
        "        max_retries = 8  # Increased retries\n",
        "        retry_count = 0\n",
        "        update_successful = False\n",
        "\n",
        "        while not update_successful and retry_count < max_retries:\n",
        "            try:\n",
        "                # Prepare the batch update request\n",
        "                cell_list = []\n",
        "\n",
        "                for update in current_chunk:\n",
        "                    cell = worksheet.cell(update['row'], update['col'])\n",
        "                    cell.value = update['value']\n",
        "                    cell_list.append(cell)\n",
        "\n",
        "                # Execute the batch update using our rate-limited function\n",
        "                def do_update():\n",
        "                    return worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
        "\n",
        "                rate_limited_request(do_update)\n",
        "\n",
        "                update_successful = True\n",
        "                updated_count += len(current_chunk) // 3  # Divide by 3 because we have 3 cells per row (L, M, N)\n",
        "                print(f\"📊 Progress: Chunk {chunk_index+1}/{chunks} complete - {updated_count}/{len(batch_updates)//3} rows updated\")\n",
        "\n",
        "                # Add a variable delay between chunks to avoid rate limits\n",
        "                # More aggressive backoff with increasing chunk index\n",
        "                wait_time = 2 + (chunk_index % 3) + (random.random() * 3)\n",
        "                print(f\"⏳ Waiting {wait_time:.2f}s before next chunk...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            except gspread.exceptions.APIError as api_error:\n",
        "                retry_count += 1\n",
        "                if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                    # Exponential backoff with jitter for rate limits\n",
        "                    wait_time = (2 ** retry_count) + (random.random() * 5)\n",
        "                    # Cap maximum wait time at 2 minutes\n",
        "                    wait_time = min(wait_time, 120)\n",
        "                    print(f\"⏳ Rate limit hit, waiting for {wait_time:.2f} seconds (retry {retry_count}/{max_retries})\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"❌ API Error on chunk {chunk_index+1}: {api_error}\")\n",
        "                    if retry_count < 3:  # Only retry a few times for non-rate-limit errors\n",
        "                        time.sleep(5)\n",
        "                    else:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                print(f\"❌ Failed to update chunk {chunk_index+1}: {e}\")\n",
        "                if retry_count < 3:  # Only retry a few times for general errors\n",
        "                    time.sleep(5)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        if not update_successful:\n",
        "            print(f\"❌ Failed to update chunk {chunk_index+1} after {max_retries} retries\")\n",
        "            # Add extra delay when a chunk completely fails before moving to next chunk\n",
        "            wait_time = 15 + (random.random() * 10)\n",
        "            print(f\"⏳ Taking a longer break: {wait_time:.2f}s before continuing...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    # Apply formatting in batches (currently commented out)\n",
        "    print(\"\\n🎨 Applying formatting to distinguish match types...\")\n",
        "    # You can uncomment and implement batch formatting logic if needed.\n",
        "\n",
        "    print(f\"\\n✅ COMPLETE for {worksheet_name}: Updated {updated_count} rows in columns L, M, and N\")\n",
        "    print(f\"📊 Summary: {success_count} exact matches (normal format), {fallback_count} fallback matches (italic and right-aligned)\")\n",
        "    print(f\"❌ {len(row_map) - updated_count} rows could not be updated\")\n",
        "\n",
        "# Global rate limiting function\n",
        "def rate_limited_request(func, *args, **kwargs):\n",
        "    \"\"\"Execute a function with rate limiting and retries for API errors\"\"\"\n",
        "    max_retries = 8  # More retries for critical operations\n",
        "    retry_count = 0\n",
        "    base_wait_time = 2  # seconds\n",
        "\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except gspread.exceptions.APIError as api_error:\n",
        "            if hasattr(api_error, 'response') and api_error.response.status_code == 429:\n",
        "                retry_count += 1\n",
        "                # Exponential backoff with jitter\n",
        "                wait_time = (base_wait_time ** retry_count) + (random.random() * 5)\n",
        "                # Cap the wait time at 3 minutes\n",
        "                wait_time = min(wait_time, 180)\n",
        "                print(f\"⏳ Rate limit hit! Waiting {wait_time:.2f}s (retry {retry_count}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                # Re-raise non-rate-limit errors\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            # For other exceptions, retry a few times but with less patience\n",
        "            if retry_count < 3:\n",
        "                retry_count += 1\n",
        "                wait_time = base_wait_time + (random.random() * 2)\n",
        "                print(f\"⚠️ Error occurred: {str(e)}\")\n",
        "                print(f\"Retrying in {wait_time:.2f}s (retry {retry_count}/3)\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    # If we've exhausted all retries\n",
        "    raise Exception(f\"Failed after {max_retries} retries due to persistent rate limiting\")\n",
        "\n",
        "# Main execution - LEAN VERSION with improved rate limiting\n",
        "print(\"🚀 Starting LEAN processing - only empty L cells...\")\n",
        "print(f\"🔍 Will search across {len(odometer_spreadsheets)} odometer spreadsheets: {', '.join(odometer_spreadsheets)}\")\n",
        "\n",
        "# Process each worksheet with rate limiting and proper delays\n",
        "for i, worksheet_name in enumerate(engine_oil_worksheets):\n",
        "    print(f\"\\n{'='*80}\\n📊 Processing worksheet: {worksheet_name} ({i+1}/{len(engine_oil_worksheets)})\\n{'='*80}\")\n",
        "\n",
        "    # Add a longer delay between worksheets to avoid rate limiting\n",
        "    if i > 0:\n",
        "        wait_time = 5 + (random.random() * 5)  # 5-10 second delay between worksheets\n",
        "        print(f\"⏳ Waiting {wait_time:.2f}s before processing next worksheet...\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "    # Process the worksheet with proper rate limiting\n",
        "    try:\n",
        "        process_worksheet_lean(worksheet_name)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing worksheet {worksheet_name}: {str(e)}\")\n",
        "        print(\"Continuing with next worksheet after a delay...\")\n",
        "        time.sleep(15)  # Longer delay after an error\n",
        "\n",
        "print(\"\\n✅ All worksheets have been processed successfully!\")\n",
        "print(\"💡 Performance Summary:\")\n",
        "print(f\"✓ Processed {len(engine_oil_worksheets)} worksheets with LEAN mode (empty L cells only)\")\n",
        "print(f\"✓ Searched across {len(odometer_spreadsheets)} odometer spreadsheets\")\n",
        "print(f\"✓ Used parallel processing for bus data with ThreadPoolExecutor\")\n",
        "print(f\"✓ Implemented caching to reduce API calls\")\n",
        "print(f\"✓ Used batch updates to minimize API requests\")\n",
        "\n"
      ],
      "metadata": {
        "id": "azCz95R7KQK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # High Speed, Lean for all units, improved API limiter, #Lean combined all Units Engine Oil and Search all Odometer, but it search only missing values, if it works, then in future just add engine oil sheet and odometer sheet\n",
        "\n",
        "\n",
        "\n",
        "# The above code is returning value in Column N, i dont need it, also incorporate batch processing to increase speed"
      ],
      "metadata": {
        "id": "J09_k_HVL0ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If date is current date then it is returning the spreadsheet name, trying to correct, It is wokring, i will delete above code in version 1.0.9\n",
        "\n",
        "# High speed Report Generation, in Engine Oil Report Spreadsheet\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# Cached function to get worksheet data\n",
        "@lru_cache_with_expiry(maxsize=100, ttl=300)  # Cache for 5 minutes\n",
        "def get_worksheet_data(spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Get worksheet data with caching\"\"\"\n",
        "    cache_key = f\"{spreadsheet_name}:{worksheet_name}\"\n",
        "\n",
        "    with cache_lock:\n",
        "        if cache_key in data_cache:\n",
        "            print(f\"📋 Cache hit for {cache_key}\")\n",
        "            return data_cache[cache_key]\n",
        "\n",
        "    # Function to find the actual worksheet object\n",
        "    def find_worksheet(spreadsheet_list, worksheet_name, spreadsheet_names):\n",
        "        for idx, spreadsheet in enumerate(spreadsheet_list):\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(worksheet_name)\n",
        "                print(f\"📑 Found worksheet {worksheet_name} in {spreadsheet_names[idx]}\")\n",
        "                return sheet, spreadsheet_names[idx]\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None, None\n",
        "\n",
        "    # Cache miss, need to retrieve data\n",
        "    try:\n",
        "        sheet, source = find_worksheet(odometer_spreadsheets, worksheet_name, spreadsheet_names)\n",
        "        if sheet:\n",
        "            data = sheet.get_all_values()\n",
        "\n",
        "            with cache_lock:\n",
        "                data_cache[cache_key] = (data, source)\n",
        "\n",
        "            return data, source\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error retrieving data for {worksheet_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# === STEP 1: CLEAR DATA ===\n",
        "print(\"\\n🧹 STEP 1: Clearing data...\")\n",
        "clear_range_all_OdometersheetID = 'A5:R200'\n",
        "clear_range_all_Report_EO1_Worksheet = ['B2:B200', 'AC2:AC200']\n",
        "\n",
        "OdometersheetID.batch_clear([clear_range_all_OdometersheetID])\n",
        "Report_EO1_Worksheet.batch_clear(clear_range_all_Report_EO1_Worksheet)\n",
        "\n",
        "# === STEP 2: PROCESS BUS DATA ===\n",
        "print(\"\\n📊 STEP 2: Processing bus data...\")\n",
        "\n",
        "# Function to process spreadsheet data in parallel\n",
        "def process_spreadsheet(cell_ref, position_index):\n",
        "    \"\"\"Process a single spreadsheet and return its data\"\"\"\n",
        "    odometer_id = OdometersheetID.acell(cell_ref).value\n",
        "    spreadsheet = gc.open_by_key(odometer_id)\n",
        "    bus_numbers = sorted([worksheet.title for worksheet in spreadsheet])\n",
        "    spreadsheet_name = spreadsheet.title\n",
        "    return {\n",
        "        'spreadsheet': spreadsheet,\n",
        "        'name': spreadsheet_name,\n",
        "        'buses': bus_numbers,\n",
        "        'position': position_index\n",
        "    }\n",
        "\n",
        "# Process all spreadsheets in parallel\n",
        "spreadsheet_info = {\n",
        "    'SVP': {'cell': 'A2', 'position': 0},\n",
        "    'FG': {'cell': 'B2', 'position': 1},\n",
        "    'BT': {'cell': 'C2', 'position': 2},\n",
        "    'RT': {'cell': 'D2', 'position': 3},\n",
        "    'MB': {'cell': 'E2', 'position': 4},\n",
        "    'DP': {'cell': 'F2', 'position': 5}\n",
        "}\n",
        "\n",
        "spreadsheet_data = {}\n",
        "odometer_spreadsheets = [None] * 6\n",
        "spreadsheet_names = [None] * 6\n",
        "\n",
        "# For Google Sheets API, we can't fully parallelize due to rate limits\n",
        "# But we can optimize the processing steps\n",
        "for key, info in spreadsheet_info.items():\n",
        "    data = process_spreadsheet(info['cell'], info['position'])\n",
        "    spreadsheet_data[key] = data\n",
        "    odometer_spreadsheets[info['position']] = data['spreadsheet']\n",
        "    spreadsheet_names[info['position']] = data['name']\n",
        "    print(f\"✅ Processed {key} spreadsheet: {data['name']} with {len(data['buses'])} buses\")\n",
        "\n",
        "# Calculate starting rows for each section\n",
        "start_row_svp = 2\n",
        "start_row_fg = start_row_svp + len(spreadsheet_data['SVP']['buses'])\n",
        "start_row_bt = start_row_fg + len(spreadsheet_data['FG']['buses'])\n",
        "start_row_rt = start_row_bt + len(spreadsheet_data['BT']['buses'])\n",
        "start_row_mb = start_row_rt + len(spreadsheet_data['RT']['buses'])\n",
        "start_row_dp = start_row_mb + len(spreadsheet_data['MB']['buses'])\n",
        "\n",
        "# === STEP 3: UPDATE ODOMETERSHEETID WITH BUS NUMBERS ===\n",
        "print(\"\\n📝 STEP 3: Updating OdometersheetID with bus numbers...\")\n",
        "\n",
        "# Optimized batch update function\n",
        "def optimized_batch_update(worksheet, updates):\n",
        "    \"\"\"Process updates in optimal batch sizes\"\"\"\n",
        "    BATCH_SIZE = 100  # Adjust based on API limits\n",
        "\n",
        "    # Split updates into manageable chunks\n",
        "    chunked_updates = chunk_list(updates, BATCH_SIZE)\n",
        "\n",
        "    for chunk in chunked_updates:\n",
        "        try:\n",
        "            worksheet.batch_update(chunk)\n",
        "            time.sleep(0.5)  # Slight delay to avoid rate limiting\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Batch update error: {e}\")\n",
        "            # Fall back to smaller chunks if needed\n",
        "            for update in chunk:\n",
        "                try:\n",
        "                    worksheet.update(update['range'], update['values'])\n",
        "                    time.sleep(0.1)\n",
        "                except Exception as e2:\n",
        "                    print(f\"⚠️ Individual update error at {update['range']}: {e2}\")\n",
        "\n",
        "# Update spreadsheet names in row 4 (batch update)\n",
        "name_updates = [\n",
        "    {'range': 'A4', 'values': [[spreadsheet_data['SVP']['name']]]},\n",
        "    {'range': 'B4', 'values': [[spreadsheet_data['FG']['name']]]},\n",
        "    {'range': 'C4', 'values': [[spreadsheet_data['BT']['name']]]},\n",
        "    {'range': 'D4', 'values': [[spreadsheet_data['RT']['name']]]},\n",
        "    {'range': 'E4', 'values': [[spreadsheet_data['MB']['name']]]},\n",
        "    {'range': 'F4', 'values': [[spreadsheet_data['DP']['name']]]}\n",
        "]\n",
        "OdometersheetID.batch_update(name_updates)\n",
        "print(\"✅ Updated all spreadsheet names in row 4\")\n",
        "\n",
        "# Generate all updates for bus numbers\n",
        "all_bus_updates = []\n",
        "\n",
        "# Helper function to create batch updates for a column\n",
        "def create_column_updates(column_letter, start_row, buses):\n",
        "    updates = []\n",
        "    for i, bus in enumerate(buses):\n",
        "        row = start_row + i\n",
        "        # Try to convert numeric strings to integers\n",
        "        try:\n",
        "            if isinstance(bus, str) and bus.isdigit():\n",
        "                bus = int(bus)\n",
        "        except:\n",
        "            pass\n",
        "        updates.append({\n",
        "            'range': f'{column_letter}{row}',\n",
        "            'values': [[bus]]\n",
        "        })\n",
        "    return updates\n",
        "\n",
        "# Create all updates for bus numbers\n",
        "all_bus_updates.extend(create_column_updates('A', 5, spreadsheet_data['SVP']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('B', 5, spreadsheet_data['FG']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('C', 5, spreadsheet_data['BT']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('D', 5, spreadsheet_data['RT']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('E', 5, spreadsheet_data['MB']['buses']))\n",
        "all_bus_updates.extend(create_column_updates('F', 5, spreadsheet_data['DP']['buses']))\n",
        "\n",
        "# Batch update OdometersheetID with bus numbers\n",
        "optimized_batch_update(OdometersheetID, all_bus_updates)\n",
        "print(f\"✅ Updated {len(all_bus_updates)} bus entries in OdometersheetID\")\n",
        "\n",
        "# === STEP 4: UPDATE REPORT_EO1 WORKSHEET WITH ALL BUS NUMBERS ===\n",
        "print(\"\\n📊 STEP 4: Updating Report_EO1 with all bus numbers...\")\n",
        "\n",
        "# Combine all bus lists for ReportA_EO1\n",
        "all_buses_with_sources = []\n",
        "for key, data in spreadsheet_data.items():\n",
        "    for bus in data['buses']:\n",
        "        all_buses_with_sources.append((bus, data['name']))\n",
        "\n",
        "# Generate batch updates for Report_EO1\n",
        "report_EO1_updates = []\n",
        "for i, (bus, source) in enumerate(all_buses_with_sources):\n",
        "    row = i + 2  # Start from row 2\n",
        "    # Try to convert numeric strings to integers\n",
        "    try:\n",
        "        if isinstance(bus, str) and bus.isdigit():\n",
        "            bus = int(bus)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    report_EO1_updates.append({\n",
        "        'range': f'B{row}',\n",
        "        'values': [[bus]]\n",
        "    })\n",
        "    report_EO1_updates.append({\n",
        "        'range': f'AC{row}',\n",
        "        'values': [[source]]\n",
        "    })\n",
        "\n",
        "# Batch update Report_EO1\n",
        "optimized_batch_update(Report_EO1_Worksheet, report_EO1_updates)\n",
        "print(f\"✅ Updated {len(all_buses_with_sources)} bus entries in Report_EO1 worksheet\")\n",
        "\n",
        "print(\"🎉 Bus data preparation completed successfully!\")\n",
        "\n",
        "# === STEP 5: SEARCH FOR ODOMETER VALUES ===\n",
        "print(\"\\n🔍 STEP 5: Searching for odometer values...\")\n",
        "\n",
        "# Clear columns C and D\n",
        "Report_EO1_Worksheet.batch_clear([\"C2:D\"])\n",
        "\n",
        "# Get current date in expected format (e.g., \"05,May25\")\n",
        "current_date_str = datetime.now().strftime('%d,%b%y')\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus + date (using today's date)\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = (bus, current_date_str)\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize lookup maps with thread safety\n",
        "bus_date_value_map = {}\n",
        "match_type_map = {}\n",
        "fallback_date_map = {}\n",
        "source_spreadsheet_map = {}\n",
        "map_lock = threading.Lock()\n",
        "\n",
        "# Function to find closest earlier date value\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=12):\n",
        "    \"\"\"Find closest earlier date with a value in the specified column\"\"\"\n",
        "    if not sheet_data:\n",
        "        return None, \"\"\n",
        "\n",
        "    try:\n",
        "        target_date = datetime.strptime(target_date_str, '%d,%b%y')\n",
        "        valid_rows = []\n",
        "\n",
        "        for row in sheet_data[1:]:  # Skip header\n",
        "            if len(row) <= value_column_index:\n",
        "                continue\n",
        "\n",
        "            date_cell = row[1].strip() if len(row) > 1 else \"\"\n",
        "            value_cell = row[value_column_index].strip() if len(row) > value_column_index else \"\"\n",
        "\n",
        "            if not date_cell or not value_cell:\n",
        "                continue\n",
        "\n",
        "            # Try multiple date formats\n",
        "            parsed_date = None\n",
        "            for fmt in ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']:\n",
        "                try:\n",
        "                    parsed_date = datetime.strptime(date_cell, fmt)\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if parsed_date and parsed_date <= target_date and value_cell:\n",
        "                valid_rows.append((parsed_date, date_cell, value_cell))\n",
        "\n",
        "        if valid_rows:\n",
        "            valid_rows.sort(key=lambda x: x[0], reverse=True)  # Newest date first\n",
        "            best = valid_rows[0]\n",
        "            return best[2], best[1]  # value, fallback_date\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error in fallback for {target_date_str}: {e}\")\n",
        "\n",
        "    return None, \"\"\n",
        "\n",
        "# Process a single bus to find odometer value\n",
        "def process_bus_odometer(bus):\n",
        "    \"\"\"Process odometer data for a single bus\"\"\"\n",
        "    found = False\n",
        "\n",
        "    # Use the cached function to get worksheet data\n",
        "    data, source = get_worksheet_data(\"All\", bus)\n",
        "\n",
        "    if data:\n",
        "        found = True\n",
        "        # Check for exact date match\n",
        "        exact_match = False\n",
        "        for row in data[1:]:  # Skip header\n",
        "            if len(row) > 12 and row[1].strip() == current_date_str and row[12].strip():\n",
        "                odometer_val = row[12].strip()\n",
        "                if odometer_val and odometer_val.isdigit():  # Validate numeric\n",
        "                    with map_lock:\n",
        "                        bus_date_value_map[(bus, current_date_str)] = odometer_val\n",
        "                        match_type_map[(bus, current_date_str)] = \"exact\"\n",
        "                        fallback_date_map[(bus, current_date_str)] = current_date_str  # Store current date\n",
        "                        source_spreadsheet_map[(bus, current_date_str)] = source\n",
        "                    exact_match = True\n",
        "                    print(f\"📌 Bus {bus}: Exact match: {odometer_val} km on {current_date_str}\")\n",
        "                    break\n",
        "\n",
        "        if not exact_match:\n",
        "            # Fallback to closest date\n",
        "            val, fallback_date = find_closest_date_value(data, current_date_str, 12)\n",
        "            if val:\n",
        "                with map_lock:\n",
        "                    bus_date_value_map[(bus, current_date_str)] = val\n",
        "                    match_type_map[(bus, current_date_str)] = \"fallback\"\n",
        "                    fallback_date_map[(bus, current_date_str)] = fallback_date\n",
        "                    source_spreadsheet_map[(bus, current_date_str)] = source\n",
        "                print(f\"🔄 Bus {bus}: Fallback match: {val} km on {fallback_date}\")\n",
        "\n",
        "    if not found:\n",
        "        print(f\"🚨 Bus {bus} not found in ANY spreadsheet!\")\n",
        "\n",
        "    return found\n",
        "\n",
        "# Process buses in parallel batches\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(process_bus_odometer, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# Update Report_EO1 with logging\n",
        "print(\"\\n📝 Writing data to Report_EO1...\")\n",
        "updated_rows = []\n",
        "for row_idx, (bus, date_str) in row_map.items():\n",
        "    key = (bus, date_str)\n",
        "    val = bus_date_value_map.get(key, \"\")\n",
        "    fallback = fallback_date_map.get(key, \"\")\n",
        "    match_type = match_type_map.get(key, \"\")\n",
        "    source = source_spreadsheet_map.get(key, \"\")\n",
        "\n",
        "    # Improved logic for Column C (date) and Column D (value)\n",
        "    if match_type == \"exact\":\n",
        "        # For exact matches, use the current date (date_str)\n",
        "        col_c = date_str\n",
        "        col_d = val if val else \"\"\n",
        "    elif match_type == \"fallback\":\n",
        "        # For fallback matches, use the fallback date we found\n",
        "        col_c = fallback if fallback else \"\"\n",
        "        col_d = val if val else \"\"\n",
        "    else:\n",
        "        # No match found\n",
        "        col_c = \"\"\n",
        "        col_d = \"\"\n",
        "\n",
        "    # Add to batch update\n",
        "    updated_rows.append([col_c, col_d])\n",
        "\n",
        "# Prepare batch update\n",
        "if updated_rows:\n",
        "    # Split into manageable chunks\n",
        "    MAX_ROWS_PER_UPDATE = 1000  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(updated_rows, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"C{start_row}:D{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(updated_rows)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"🎉 Complete script execution finished successfully!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sBULhR5luXmc",
        "outputId": "ee198aef-15a2-4b5a-bd10-8e7eeb687acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing performance optimizations...\n",
            "\n",
            "🧹 STEP 1: Clearing data...\n",
            "\n",
            "📊 STEP 2: Processing bus data...\n",
            "✅ Processed SVP spreadsheet: Odometer_SVP with 59 buses\n",
            "✅ Processed FG spreadsheet: Odometer_FG with 20 buses\n",
            "✅ Processed BT spreadsheet: Odometer_BT with 1 buses\n",
            "✅ Processed RT spreadsheet: Odometer_RT with 1 buses\n",
            "✅ Processed MB spreadsheet: Odometer_MB with 1 buses\n",
            "✅ Processed DP spreadsheet: Odometer_DP with 1 buses\n",
            "\n",
            "📝 STEP 3: Updating OdometersheetID with bus numbers...\n",
            "✅ Updated all spreadsheet names in row 4\n",
            "✅ Updated 83 bus entries in OdometersheetID\n",
            "\n",
            "📊 STEP 4: Updating Report_EO1 with all bus numbers...\n",
            "✅ Updated 83 bus entries in Report_EO1 worksheet\n",
            "🎉 Bus data preparation completed successfully!\n",
            "\n",
            "🔍 STEP 5: Searching for odometer values...\n",
            "🔎 Total buses to process: 83\n",
            "Sample buses: ['587', '475', '515', '486', '999']\n",
            "📑 Found worksheet 587 in Odometer_SVP\n",
            "🔄 Bus 587: Fallback match: 296935 km on 14,May25\n",
            "📑 Found worksheet 475 in Odometer_SVP\n",
            "🔄 Bus 475: Fallback match: 327218 km on 12,Apr25\n",
            "📑 Found worksheet 515 in Odometer_SVP\n",
            "🔄 Bus 515: Fallback match: 474742 km on 14,Feb25\n",
            "📑 Found worksheet 486 in Odometer_SVP\n",
            "🔄 Bus 486: Fallback match: 379760 km on 28,Mar25\n",
            "📑 Found worksheet 999 in Odometer_BT\n",
            "🔄 Bus 999: Fallback match: 345158 km on 29,Mar25\n",
            "📑 Found worksheet 578 in Odometer_SVP\n",
            "🔄 Bus 578: Fallback match: 473385 km on 12,May25\n",
            "📑 Found worksheet 440 in Odometer_SVP\n",
            "🔄 Bus 440: Fallback match: 259274 km on 23,Apr25\n",
            "📑 Found worksheet 508 in Odometer_SVP\n",
            "🔄 Bus 508: Fallback match: 379982 km on 13,May25\n",
            "📑 Found worksheet 561 in Odometer_FG\n",
            "🔄 Bus 561: Fallback match: 571846 km on 14,May25\n",
            "📑 Found worksheet 471 in Odometer_SVP\n",
            "🔄 Bus 471: Fallback match: 402452 km on 13,May25\n",
            "Progress: 10/83 buses processed (12.0%)\n",
            "📑 Found worksheet 462 in Odometer_SVP\n",
            "🔄 Bus 462: Fallback match: 394338 km on 14,May25\n",
            "📑 Found worksheet 463 in Odometer_FG\n",
            "🔄 Bus 463: Fallback match: 254649 km on 13,May25\n",
            "📑 Found worksheet 460 in Odometer_SVP\n",
            "🔄 Bus 460: Fallback match: 367360 km on 06,Feb25\n",
            "📑 Found worksheet 613 in Odometer_SVP\n",
            "🔄 Bus 613: Fallback match: 53915 km on 17,Mar25\n",
            "📑 Found worksheet 604 in Odometer_SVP\n",
            "🔄 Bus 604: Fallback match: 115816 km on 14,May25\n",
            "📑 Found worksheet 529 in Odometer_FG\n",
            "🔄 Bus 529: Fallback match: 360784 km on 13,May25\n",
            "📑 Found worksheet 437 in Odometer_FG\n",
            "🔄 Bus 437: Fallback match: 251769 km on 10,May25\n",
            "📑 Found worksheet 493 in Odometer_FG\n",
            "🔄 Bus 493: Fallback match: 284199 km on 13,May25\n",
            "📑 Found worksheet 489 in Odometer_SVP\n",
            "🔄 Bus 489: Fallback match: 103604 km on 11,Oct16\n",
            "📑 Found worksheet 563 in Odometer_SVP\n",
            "🔄 Bus 563: Fallback match: 733577 km on 12,May25\n",
            "Progress: 20/83 buses processed (24.1%)\n",
            "📑 Found worksheet 521 in Odometer_SVP\n",
            "🔄 Bus 521: Fallback match: 307526 km on 13,May25\n",
            "📑 Found worksheet 534 in Odometer_SVP\n",
            "🔄 Bus 534: Fallback match: 309529 km on 15,May25\n",
            "📑 Found worksheet 538 in Odometer_SVP\n",
            "🔄 Bus 538: Fallback match: 341993 km on 13,May25\n",
            "📑 Found worksheet 596 in Odometer_SVP\n",
            "🔄 Bus 596: Fallback match: 97469 km on 14,May25\n",
            "📑 Found worksheet 444 in Odometer_SVP\n",
            "🔄 Bus 444: Fallback match: 303633 km on 15,May25\n",
            "📑 Found worksheet 614 in Odometer_SVP\n",
            "🔄 Bus 614: Fallback match: 61985 km on 14,May25\n",
            "📑 Found worksheet 536 in Odometer_SVP\n",
            "🔄 Bus 536: Fallback match: 328571 km on 13,May25\n",
            "📑 Found worksheet 535 in Odometer_SVP\n",
            "🔄 Bus 535: Fallback match: 348104 km on 13,May25\n",
            "📑 Found worksheet 546 in Odometer_SVP\n",
            "🔄 Bus 546: Fallback match: 534433 km on 13,May25\n",
            "📑 Found worksheet 990 in Odometer_RT\n",
            "🔄 Bus 990: Fallback match: 278164 km on 03,Mar25\n",
            "Progress: 30/83 buses processed (36.1%)\n",
            "📑 Found worksheet 615 in Odometer_FG\n",
            "🔄 Bus 615: Fallback match: 58303 km on 13,May25\n",
            "📑 Found worksheet 473 in Odometer_SVP\n",
            "🔄 Bus 473: Fallback match: 335615 km on 31,Jan25\n",
            "📑 Found worksheet 595 in Odometer_SVP\n",
            "🔄 Bus 595: Fallback match: 280689 km on 13,May25\n",
            "📑 Found worksheet 617 in Odometer_FG\n",
            "🔄 Bus 617: Fallback match: 67418 km on 13,May25\n",
            "📑 Found worksheet 589 in Odometer_SVP\n",
            "🔄 Bus 589: Fallback match: 197762 km on 04,Mar25\n",
            "📑 Found worksheet 573 in Odometer_FG\n",
            "🔄 Bus 573: Fallback match: 540225 km on 13,May25\n",
            "📑 Found worksheet 472 in Odometer_SVP\n",
            "🔄 Bus 472: Fallback match: 395620 km on 14,May25\n",
            "📑 Found worksheet 459 in Odometer_SVP\n",
            "🔄 Bus 459: Fallback match: 159987 km on 13,May25\n",
            "📑 Found worksheet 518 in Odometer_SVP\n",
            "🔄 Bus 518: Fallback match: 257082 km on 15,Apr20\n",
            "📑 Found worksheet 577 in Odometer_SVP\n",
            "🔄 Bus 577: Fallback match: 616055 km on 13,May25\n",
            "Progress: 40/83 buses processed (48.2%)\n",
            "📑 Found worksheet 560 in Odometer_FG\n",
            "🔄 Bus 560: Fallback match: 580991 km on 14,May25\n",
            "📑 Found worksheet 548 in Odometer_SVP\n",
            "🔄 Bus 548: Fallback match: 588521 km on 14,May25\n",
            "📑 Found worksheet 442 in Odometer_SVP\n",
            "🔄 Bus 442: Fallback match: 276983 km on 14,May25\n",
            "📑 Found worksheet 616 in Odometer_FG\n",
            "🔄 Bus 616: Fallback match: 72416 km on 13,May25\n",
            "📑 Found worksheet 547 in Odometer_SVP\n",
            "🔄 Bus 547: Fallback match: 704578 km on 14,May25\n",
            "📑 Found worksheet 504 in Odometer_SVP\n",
            "🔄 Bus 504: Fallback match: 378458 km on 14,May25\n",
            "📑 Found worksheet 479 in Odometer_FG\n",
            "🔄 Bus 479: Fallback match: 425352 km on 13,May25\n",
            "📑 Found worksheet 457 in Odometer_SVP\n",
            "🔄 Bus 457: Fallback match: 411536 km on 14,May25\n",
            "📑 Found worksheet 441 in Odometer_SVP\n",
            "🔄 Bus 441: Fallback match: 278164 km on 12,Apr25\n",
            "📑 Found worksheet 572 in Odometer_FG\n",
            "🔄 Bus 572: Fallback match: 464882 km on 14,May25\n",
            "Progress: 50/83 buses processed (60.2%)\n",
            "📑 Found worksheet 588 in Odometer_SVP\n",
            "🔄 Bus 588: Fallback match: 287946 km on 14,May25\n",
            "📑 Found worksheet 476 in Odometer_SVP\n",
            "🔄 Bus 476: Fallback match: 388548 km on 15,May25\n",
            "📑 Found worksheet 528 in Odometer_SVP\n",
            "🔄 Bus 528: Fallback match: 302399 km on 25,Apr25\n",
            "📑 Found worksheet 970 in Odometer_DP\n",
            "🔄 Bus 970: Fallback match: 383845 km on 29,Mar25\n",
            "📑 Found worksheet 593 in Odometer_FG\n",
            "🔄 Bus 593: Fallback match: 396223 km on 13,May25\n",
            "📑 Found worksheet 598 in Odometer_SVP\n",
            "🔄 Bus 598: Fallback match: 115299 km on 14,May25\n",
            "📑 Found worksheet 612 in Odometer_SVP\n",
            "🔄 Bus 612: Fallback match: 61623 km on 15,May25\n",
            "📑 Found worksheet 461 in Odometer_FG\n",
            "🔄 Bus 461: Fallback match: 332346 km on 13,May25\n",
            "📑 Found worksheet 474 in Odometer_SVP\n",
            "🔄 Bus 474: Fallback match: 373479 km on 15,May25\n",
            "📑 Found worksheet 590 in Odometer_SVP\n",
            "🔄 Bus 590: Fallback match: 434243 km on 01,May25\n",
            "Progress: 60/83 buses processed (72.3%)\n",
            "📑 Found worksheet 478 in Odometer_FG\n",
            "🔄 Bus 478: Fallback match: 290062 km on 13,May25\n",
            "📑 Found worksheet 516 in Odometer_FG\n",
            "🔄 Bus 516: Fallback match: 289618 km on 13,May25\n",
            "📑 Found worksheet 495 in Odometer_SVP\n",
            "🔄 Bus 495: Fallback match: 585951 km on 14,May25\n",
            "📑 Found worksheet 574 in Odometer_SVP\n",
            "🔄 Bus 574: Fallback match: 548919 km on 02,Dec24\n",
            "📑 Found worksheet 575 in Odometer_SVP\n",
            "🔄 Bus 575: Fallback match: 391243 km on 13,Nov24\n",
            "📑 Found worksheet 438 in Odometer_FG\n",
            "🔄 Bus 438: Fallback match: 318951 km on 13,May25\n",
            "📑 Found worksheet 599 in Odometer_SVP\n",
            "🔄 Bus 599: Fallback match: 102140 km on 15,May25\n",
            "📑 Found worksheet 562 in Odometer_FG\n",
            "🔄 Bus 562: Fallback match: 591226 km on 14,May25\n",
            "📑 Found worksheet 507 in Odometer_FG\n",
            "🔄 Bus 507: Fallback match: 400449 km on 13,May25\n",
            "📑 Found worksheet 550 in Odometer_SVP\n",
            "🔄 Bus 550: Fallback match: 680832 km on 14,Oct24\n",
            "Progress: 70/83 buses processed (84.3%)\n",
            "📑 Found worksheet 523 in Odometer_SVP\n",
            "🔄 Bus 523: Fallback match: 281016 km on 23,Apr25\n",
            "📑 Found worksheet 597 in Odometer_SVP\n",
            "🔄 Bus 597: Fallback match: 122045 km on 12,May25\n",
            "📑 Found worksheet 584 in Odometer_SVP\n",
            "🔄 Bus 584: Fallback match: 190961 km on 10,Aug24\n",
            "📑 Found worksheet 469 in Odometer_SVP\n",
            "🔄 Bus 469: Fallback match: 236807 km on 29,Apr25\n",
            "📑 Found worksheet 592 in Odometer_FG\n",
            "🔄 Bus 592: Fallback match: 370006 km on 13,May25\n",
            "📑 Found worksheet 511 in Odometer_SVP\n",
            "🔄 Bus 511: Fallback match: 250716 km on 13,May25\n",
            "📑 Found worksheet 605 in Odometer_SVP\n",
            "🔄 Bus 605: Fallback match: 101277 km on 14,May25\n",
            "📑 Found worksheet 564 in Odometer_SVP\n",
            "🔄 Bus 564: Fallback match: 573079 km on 16,May24\n",
            "📑 Found worksheet 585 in Odometer_SVP\n",
            "🔄 Bus 585: Fallback match: 361074 km on 14,May25\n",
            "📑 Found worksheet 980 in Odometer_MB\n",
            "🔄 Bus 980: Fallback match: 361040 km on 05,Apr25\n",
            "Progress: 80/83 buses processed (96.4%)\n",
            "📑 Found worksheet 514 in Odometer_SVP\n",
            "🔄 Bus 514: Fallback match: 182345 km on 14,May25\n",
            "📑 Found worksheet 603 in Odometer_SVP\n",
            "🔄 Bus 603: Fallback match: 100003 km on 14,May25\n",
            "📑 Found worksheet 530 in Odometer_SVP\n",
            "🔄 Bus 530: Fallback match: 398198 km on 13,May25\n",
            "Progress: 83/83 buses processed (100.0%)\n",
            "✅ Completed processing all 83 buses\n",
            "\n",
            "📝 Writing data to Report_EO1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-307866c0d17e>:447: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(range_to_update, chunk)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated rows 2-84 in Report_EO1\n",
            "✅ All 83 rows updated successfully in Report_EO1!\n",
            "🎉 Complete script execution finished successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here i will try to get the HSD balance on current date\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# Cached function to get worksheet data\n",
        "@lru_cache_with_expiry(maxsize=100, ttl=300)  # Cache for 5 minutes\n",
        "def get_worksheet_data(spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Get worksheet data with caching\"\"\"\n",
        "    cache_key = f\"{spreadsheet_name}:{worksheet_name}\"\n",
        "\n",
        "    with cache_lock:\n",
        "        if cache_key in data_cache:\n",
        "            print(f\"📋 Cache hit for {cache_key}\")\n",
        "            return data_cache[cache_key]\n",
        "\n",
        "    # Function to find the actual worksheet object\n",
        "    def find_worksheet(spreadsheet_list, worksheet_name, spreadsheet_names):\n",
        "        for idx, spreadsheet in enumerate(spreadsheet_list):\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(worksheet_name)\n",
        "                print(f\"📑 Found worksheet {worksheet_name} in {spreadsheet_names[idx]}\")\n",
        "                return sheet, spreadsheet_names[idx]\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None, None\n",
        "\n",
        "    # Cache miss, need to retrieve data\n",
        "    try:\n",
        "        sheet, source = find_worksheet(odometer_spreadsheets, worksheet_name, spreadsheet_names)\n",
        "        if sheet:\n",
        "            data = sheet.get_all_values()\n",
        "\n",
        "            with cache_lock:\n",
        "                data_cache[cache_key] = (data, source)\n",
        "\n",
        "            return data, source\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error retrieving data for {worksheet_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# === STEP 1: CLEAR DATA HSD from AF Coloumn\n",
        "print(\"\\n🧹 STEP 1: Clearing data...\")\n",
        "\n",
        "\n",
        "clear_range_HSD_Report_EO1_Worksheet = ['AF2:AF200', 'AG2:AG200']\n",
        "\n",
        "Report_EO1_Worksheet.batch_clear(clear_range_HSD_Report_EO1_Worksheet)\n",
        "\n",
        "\n",
        "# === STEP 5: SEARCH FOR HSD Balance VALUES ===\n",
        "print(\"\\n🔍 STEP 5: Searching for HSD Balance values...\")\n",
        "\n",
        "\n",
        "# Get current date in expected format (e.g., \"05,May25\")\n",
        "current_date_str = datetime.now().strftime('%d,%b%y')\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus + date (using today's date)\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = (bus, current_date_str)\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize lookup maps with thread safety\n",
        "bus_date_value_map = {}\n",
        "match_type_map = {}\n",
        "fallback_date_map = {}\n",
        "source_spreadsheet_map = {}\n",
        "map_lock = threading.Lock()\n",
        "\n",
        "# Function to find closest earlier date value\n",
        "def find_closest_date_value(sheet_data, target_date_str, value_column_index=6):\n",
        "    \"\"\"Find closest earlier date with a value in the specified column\"\"\"\n",
        "    if not sheet_data:\n",
        "        return None, \"\"\n",
        "\n",
        "    try:\n",
        "        target_date = datetime.strptime(target_date_str, '%d,%b%y')\n",
        "        valid_rows = []\n",
        "\n",
        "        for row in sheet_data[1:]:  # Skip header\n",
        "            if len(row) <= value_column_index:\n",
        "                continue\n",
        "\n",
        "            date_cell = row[1].strip() if len(row) > 1 else \"\"\n",
        "            value_cell = row[value_column_index].strip() if len(row) > value_column_index else \"\"\n",
        "\n",
        "            if not date_cell or not value_cell:\n",
        "                continue\n",
        "\n",
        "            # Try multiple date formats\n",
        "            parsed_date = None\n",
        "            for fmt in ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']:\n",
        "                try:\n",
        "                    parsed_date = datetime.strptime(date_cell, fmt)\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if parsed_date and parsed_date <= target_date and value_cell:\n",
        "                valid_rows.append((parsed_date, date_cell, value_cell))\n",
        "\n",
        "        if valid_rows:\n",
        "            valid_rows.sort(key=lambda x: x[0], reverse=True)  # Newest date first\n",
        "            best = valid_rows[0]\n",
        "            return best[2], best[1]  # value, fallback_date\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error in fallback for {target_date_str}: {e}\")\n",
        "\n",
        "    return None, \"\"\n",
        "\n",
        "# Process a single bus to find odometer value\n",
        "def process_bus_odometer(bus):\n",
        "    \"\"\"Process odometer data for a single bus\"\"\"\n",
        "    found = False\n",
        "\n",
        "    # Use the cached function to get worksheet data\n",
        "    data, source = get_worksheet_data(\"All\", bus)\n",
        "\n",
        "    if data:\n",
        "        found = True\n",
        "        # Check for exact date match\n",
        "        exact_match = False\n",
        "        for row in data[1:]:  # Skip header\n",
        "            if len(row) > 12 and row[1].strip() == current_date_str and row[12].strip():\n",
        "                odometer_val = row[12].strip()\n",
        "                if odometer_val and odometer_val.isdigit():  # Validate numeric\n",
        "                    with map_lock:\n",
        "                        bus_date_value_map[(bus, current_date_str)] = odometer_val\n",
        "                        match_type_map[(bus, current_date_str)] = \"exact\"\n",
        "                        fallback_date_map[(bus, current_date_str)] = current_date_str  # Store current date\n",
        "                        source_spreadsheet_map[(bus, current_date_str)] = source\n",
        "                    exact_match = True\n",
        "                    print(f\"📌 Bus {bus}: Exact match: {odometer_val} km on {current_date_str}\")\n",
        "                    break\n",
        "\n",
        "        if not exact_match:\n",
        "            # Fallback to closest date\n",
        "            val, fallback_date = find_closest_date_value(data, current_date_str, 6)\n",
        "            if val:\n",
        "                with map_lock:\n",
        "                    bus_date_value_map[(bus, current_date_str)] = val\n",
        "                    match_type_map[(bus, current_date_str)] = \"fallback\"\n",
        "                    fallback_date_map[(bus, current_date_str)] = fallback_date\n",
        "                    source_spreadsheet_map[(bus, current_date_str)] = source\n",
        "                print(f\"🔄 Bus {bus}: Fallback match: {val} km on {fallback_date}\")\n",
        "\n",
        "    if not found:\n",
        "        print(f\"🚨 Bus {bus} not found in ANY spreadsheet!\")\n",
        "\n",
        "    return found\n",
        "\n",
        "# Process buses in parallel batches\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(process_bus_odometer, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# Update Report_EO1 with logging\n",
        "print(\"\\n📝 Writing data to Report_EO1...\")\n",
        "updated_rows = []\n",
        "for row_idx, (bus, date_str) in row_map.items():\n",
        "    key = (bus, date_str)\n",
        "    val = bus_date_value_map.get(key, \"\")\n",
        "    fallback = fallback_date_map.get(key, \"\")\n",
        "    match_type = match_type_map.get(key, \"\")\n",
        "    source = source_spreadsheet_map.get(key, \"\")\n",
        "\n",
        "    # Improved logic for Column C (date) and Column D (value)\n",
        "    if match_type == \"exact\":\n",
        "        # For exact matches, use the current date (date_str)\n",
        "        col_c = date_str\n",
        "        col_d = val if val else \"\"\n",
        "    elif match_type == \"fallback\":\n",
        "        # For fallback matches, use the fallback date we found\n",
        "        col_c = fallback if fallback else \"\"\n",
        "        col_d = val if val else \"\"\n",
        "    else:\n",
        "        # No match found\n",
        "        col_c = \"\"\n",
        "        col_d = \"\"\n",
        "\n",
        "    # Add to batch update\n",
        "    updated_rows.append([col_c, col_d])\n",
        "\n",
        "# Prepare batch update\n",
        "if updated_rows:\n",
        "    # Split into manageable chunks\n",
        "    MAX_ROWS_PER_UPDATE = 1000  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(updated_rows, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AF{start_row}:AG{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(updated_rows)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"🎉 Complete script execution finished successfully!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7H4g7iF1fZDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on your requirements, I've created a new script that calculates non-operating days for buses by analyzing odometer readings. The script follows your process flow and implements the specific logic needed to detect when buses are not operating.\n",
        "Key Features of This Implementation:\n",
        "1. Clear Existing Data\n",
        "\n",
        "Clears columns AH, AI, and AJ in rows 2-200 of the Report_EO1_Worksheet\n",
        "\n",
        "2. Get Current Date\n",
        "\n",
        "Gets the current date and formats it as DD,MonYY\n",
        "Calculates reference dates for different time periods (week, month, year)\n",
        "\n",
        "3. Process Buses\n",
        "\n",
        "For each bus, the script:\n",
        "\n",
        "Retrieves all available date and odometer data\n",
        "Sorts the data chronologically\n",
        "Identifies non-operating days through two methods:\n",
        "\n",
        "When odometer readings don't change between consecutive dates (indicating the bus didn't run)\n",
        "When there are gaps between dates (missing days)\n",
        "\n",
        "\n",
        "Counts non-operating days in three time periods:\n",
        "\n",
        "Last week\n",
        "Last month (30 days)\n",
        "Last year (365 days)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Update Report\n",
        "\n",
        "Updates columns AH, AI, and AJ with the calculated non-operating days\n",
        "Uses batch processing to efficiently update the worksheet\n",
        "\n",
        "How the Non-Operating Days Detection Works\n",
        "The script identifies non-operating days in two ways:\n",
        "\n",
        "Same odometer value on consecutive dates: When the odometer reading doesn't change between two dates, it means the bus didn't operate on the later date.\n",
        "Missing dates: When there's a gap between two dates in the data, the script assumes the bus might not have operated on those missing days.\n",
        "\n",
        "For example, with the data you provided:\n",
        "03,Jan24 - 488487\n",
        "05,Jan24 - 488815\n",
        "06,Jan24 - 488815\n",
        "08,Jan24 - 489011\n",
        "09,Jan24 - 489339\n",
        "10,Jan24 - 489643\n",
        "The script would detect:\n",
        "\n",
        "Non-operating on 04,Jan24 (missing date)\n",
        "Non-operating on 06,Jan24 (same odometer as 05,Jan24)\n",
        "Non-operating on 07,Jan24 (missing date)\n",
        "\n",
        "Would you like me to explain any specific part of the implementation in more detail?"
      ],
      "metadata": {
        "id": "k75yAO65bGAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No of days bus operated in last One week, One Month and One year\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# Cached function to get worksheet data\n",
        "@lru_cache_with_expiry(maxsize=100, ttl=300)  # Cache for 5 minutes\n",
        "def get_worksheet_data(spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Get worksheet data with caching\"\"\"\n",
        "    cache_key = f\"{spreadsheet_name}:{worksheet_name}\"\n",
        "\n",
        "    with cache_lock:\n",
        "        if cache_key in data_cache:\n",
        "            print(f\"📋 Cache hit for {cache_key}\")\n",
        "            return data_cache[cache_key]\n",
        "\n",
        "    # Function to find the actual worksheet object\n",
        "    def find_worksheet(spreadsheet_list, worksheet_name, spreadsheet_names):\n",
        "        for idx, spreadsheet in enumerate(spreadsheet_list):\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(worksheet_name)\n",
        "                print(f\"📑 Found worksheet {worksheet_name} in {spreadsheet_names[idx]}\")\n",
        "                return sheet, spreadsheet_names[idx]\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None, None\n",
        "\n",
        "    # Cache miss, need to retrieve data\n",
        "    try:\n",
        "        sheet, source = find_worksheet(odometer_spreadsheets, worksheet_name, spreadsheet_names)\n",
        "        if sheet:\n",
        "            data = sheet.get_all_values()\n",
        "\n",
        "            with cache_lock:\n",
        "                data_cache[cache_key] = (data, source)\n",
        "\n",
        "            return data, source\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error retrieving data for {worksheet_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# === STEP 1: CLEAR DATA from AH, AI, AJ Columns ===\n",
        "print(\"\\n🧹 STEP 1: Clearing data...\")\n",
        "\n",
        "clear_range_Report_EO1_Worksheet = ['AH2:AJ200']\n",
        "Report_EO1_Worksheet.batch_clear(clear_range_Report_EO1_Worksheet)\n",
        "\n",
        "# === STEP 2: GET CURRENT DATE ===\n",
        "print(\"\\n📅 STEP 2: Getting current date...\")\n",
        "current_date = datetime.now()\n",
        "current_date_str = current_date.strftime('%d,%b%y')\n",
        "\n",
        "# Calculate dates for different time periods\n",
        "one_week_ago = current_date - timedelta(days=7)\n",
        "one_month_ago = current_date - timedelta(days=30)\n",
        "one_year_ago = current_date - timedelta(days=365)\n",
        "\n",
        "# === STEP 3: PROCESS BUSES ===\n",
        "print(\"\\n🚌 STEP 3: Processing buses...\")\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize results with thread safety\n",
        "bus_results = {}\n",
        "results_lock = threading.Lock()\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse date string with multiple format support\"\"\"\n",
        "    for fmt in ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def calculate_non_operating_days(bus):\n",
        "    \"\"\"Calculate non-operating days for a bus across different time periods\"\"\"\n",
        "    try:\n",
        "        # Get the worksheet data for this bus\n",
        "        data, source = get_worksheet_data(\"All\", bus)\n",
        "\n",
        "        if not data or len(data) < 2:  # Need at least header + 1 data row\n",
        "            print(f\"⚠️ No data found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Extract date and odometer data\n",
        "        # Assuming column B (index 1) is date and column M (index 12) is odometer\n",
        "        date_odometer_data = []\n",
        "\n",
        "        for row in data[1:]:  # Skip header\n",
        "            if len(row) <= 12:  # Skip rows that don't have enough columns\n",
        "                continue\n",
        "\n",
        "            date_str = row[1].strip()\n",
        "            odometer_str = row[12].strip()\n",
        "\n",
        "            if not date_str or not odometer_str:\n",
        "                continue\n",
        "\n",
        "            parsed_date = parse_date(date_str)\n",
        "            if not parsed_date:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                odometer_value = int(odometer_str)\n",
        "                date_odometer_data.append((parsed_date, odometer_value))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        if not date_odometer_data:\n",
        "            print(f\"⚠️ No valid date-odometer pairs found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Sort by date in ascending order\n",
        "        date_odometer_data.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Identify non-operating days\n",
        "        non_operating_days = []\n",
        "\n",
        "        for i in range(1, len(date_odometer_data)):\n",
        "            current_date, current_odo = date_odometer_data[i]\n",
        "            prev_date, prev_odo = date_odometer_data[i-1]\n",
        "\n",
        "            # If odometer didn't change, the bus didn't operate\n",
        "            if current_odo == prev_odo:\n",
        "                non_operating_days.append(current_date)\n",
        "                continue\n",
        "\n",
        "            # Check for missing days between consecutive dates\n",
        "            date_diff = (current_date - prev_date).days\n",
        "            if date_diff > 1:\n",
        "                # For all days in between, determine if they were likely non-operating\n",
        "                for day_offset in range(1, date_diff):\n",
        "                    missing_date = prev_date + timedelta(days=day_offset)\n",
        "                    non_operating_days.append(missing_date)\n",
        "\n",
        "        # Count non-operating days in different periods\n",
        "        week_non_op = 0\n",
        "        month_non_op = 0\n",
        "        year_non_op = 0\n",
        "\n",
        "        for non_op_date in non_operating_days:\n",
        "            if non_op_date >= one_week_ago:\n",
        "                week_non_op += 1\n",
        "            if non_op_date >= one_month_ago:\n",
        "                month_non_op += 1\n",
        "            if non_op_date >= one_year_ago:\n",
        "                year_non_op += 1\n",
        "\n",
        "        # Calculate results\n",
        "        with results_lock:\n",
        "            bus_results[bus] = {\n",
        "                'week_non_op': week_non_op,\n",
        "                'month_non_op': month_non_op,\n",
        "                'year_non_op': year_non_op\n",
        "            }\n",
        "\n",
        "        print(f\"✅ Bus {bus}: Week({week_non_op}), Month({month_non_op}), Year({year_non_op}) non-operating days\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing bus {bus}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process buses in parallel batches\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(calculate_non_operating_days, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# === STEP 4: UPDATE REPORT ===\n",
        "print(\"\\n📝 STEP 4: Updating report...\")\n",
        "\n",
        "# Prepare data for batch update\n",
        "updated_rows = []\n",
        "for row_idx, bus in row_map.items():\n",
        "    result = bus_results.get(bus, {})\n",
        "\n",
        "    week_non_op = result.get('week_non_op', '')\n",
        "    month_non_op = result.get('month_non_op', '')\n",
        "    year_non_op = result.get('year_non_op', '')\n",
        "\n",
        "    updated_rows.append([week_non_op, month_non_op, year_non_op])\n",
        "\n",
        "# Prepare batch update\n",
        "if updated_rows:\n",
        "    # Split into manageable chunks\n",
        "    MAX_ROWS_PER_UPDATE = 1000  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(updated_rows, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AH{start_row}:AJ{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(updated_rows)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"🎉 Process finished successfully!\")\n"
      ],
      "metadata": {
        "id": "meqquMoIP_H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R2\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# Cached function to get worksheet data\n",
        "@lru_cache_with_expiry(maxsize=100, ttl=300)  # Cache for 5 minutes\n",
        "def get_worksheet_data(spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Get worksheet data with caching\"\"\"\n",
        "    cache_key = f\"{spreadsheet_name}:{worksheet_name}\"\n",
        "\n",
        "    with cache_lock:\n",
        "        if cache_key in data_cache:\n",
        "            print(f\"📋 Cache hit for {cache_key}\")\n",
        "            return data_cache[cache_key]\n",
        "\n",
        "    # Function to find the actual worksheet object\n",
        "    def find_worksheet(spreadsheet_list, worksheet_name, spreadsheet_names):\n",
        "        for idx, spreadsheet in enumerate(spreadsheet_list):\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(worksheet_name)\n",
        "                print(f\"📑 Found worksheet {worksheet_name} in {spreadsheet_names[idx]}\")\n",
        "                return sheet, spreadsheet_names[idx]\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None, None\n",
        "\n",
        "    # Cache miss, need to retrieve data\n",
        "    try:\n",
        "        sheet, source = find_worksheet(odometer_spreadsheets, worksheet_name, spreadsheet_names)\n",
        "        if sheet:\n",
        "            data = sheet.get_all_values()\n",
        "\n",
        "            with cache_lock:\n",
        "                data_cache[cache_key] = (data, source)\n",
        "\n",
        "            return data, source\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error retrieving data for {worksheet_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# === STEP 1: CLEAR DATA from AH, AI, AJ Columns ===\n",
        "print(\"\\n🧹 STEP 1: Clearing data...\")\n",
        "\n",
        "clear_range_Report_EO1_Worksheet = ['AH2:AJ200']\n",
        "Report_EO1_Worksheet.batch_clear(clear_range_Report_EO1_Worksheet)\n",
        "\n",
        "# === STEP 2: GET CURRENT DATE ===\n",
        "print(\"\\n📅 STEP 2: Getting current date...\")\n",
        "current_date = datetime.now()\n",
        "current_date_str = current_date.strftime('%d,%b%y')\n",
        "\n",
        "# Calculate dates for different time periods\n",
        "one_week_ago = current_date - timedelta(days=7)\n",
        "one_month_ago = current_date - timedelta(days=30)\n",
        "one_year_ago = current_date - timedelta(days=365)\n",
        "\n",
        "# === STEP 3: PROCESS BUSES ===\n",
        "print(\"\\n🚌 STEP 3: Processing buses...\")\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize results with thread safety\n",
        "bus_results = {}\n",
        "results_lock = threading.Lock()\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse date string with multiple format support\"\"\"\n",
        "    for fmt in ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def calculate_non_operating_days(bus):\n",
        "    \"\"\"Calculate non-operating days for a bus across different time periods\"\"\"\n",
        "    try:\n",
        "        # Get the worksheet data for this bus\n",
        "        data, source = get_worksheet_data(\"All\", bus)\n",
        "\n",
        "        if not data or len(data) < 2:  # Need at least header + 1 data row\n",
        "            print(f\"⚠️ No data found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Extract date and odometer data\n",
        "        # Column B (index 1) is date and column M (index 12) is odometer\n",
        "        date_odometer_map = {}\n",
        "\n",
        "        for row in data[1:]:  # Skip header\n",
        "            if len(row) <= 12:  # Skip rows that don't have enough columns\n",
        "                continue\n",
        "\n",
        "            date_str = row[1].strip()\n",
        "            odometer_str = row[12].strip()\n",
        "\n",
        "            if not date_str or not odometer_str:\n",
        "                continue\n",
        "\n",
        "            parsed_date = parse_date(date_str)\n",
        "            if not parsed_date:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                odometer_value = int(odometer_str)\n",
        "                # Use date as key (without time) to group multiple entries on same date\n",
        "                date_key = parsed_date.date()\n",
        "\n",
        "                # For multiple entries on same date, keep the highest odometer value\n",
        "                if date_key in date_odometer_map:\n",
        "                    date_odometer_map[date_key] = max(date_odometer_map[date_key], odometer_value)\n",
        "                else:\n",
        "                    date_odometer_map[date_key] = odometer_value\n",
        "\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        if not date_odometer_map:\n",
        "            print(f\"⚠️ No valid date-odometer pairs found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Sort unique dates in ascending order\n",
        "        sorted_dates = sorted(date_odometer_map.keys())\n",
        "\n",
        "        # Identify non-operating days (only between dates, not on the same date)\n",
        "        non_operating_days = []\n",
        "\n",
        "        # Find date ranges where bus was not operating\n",
        "        for i in range(1, len(sorted_dates)):\n",
        "            current_date = sorted_dates[i]\n",
        "            prev_date = sorted_dates[i-1]\n",
        "\n",
        "            # Get odometer values\n",
        "            current_odo = date_odometer_map[current_date]\n",
        "            prev_odo = date_odometer_map[prev_date]\n",
        "\n",
        "            # If odometer value didn't change between different dates, bus didn't operate on current date\n",
        "            if current_odo == prev_odo and current_date != prev_date:\n",
        "                non_operating_days.append(current_date)\n",
        "\n",
        "            # Check for missing days between consecutive dates\n",
        "            date_diff = (current_date - prev_date).days\n",
        "            if date_diff > 1:\n",
        "                # For all days in between, mark as non-operating\n",
        "                for day_offset in range(1, date_diff):\n",
        "                    missing_date = prev_date + timedelta(days=day_offset)\n",
        "                    non_operating_days.append(missing_date.date())\n",
        "\n",
        "        # Count non-operating days in different periods\n",
        "        week_non_op = 0\n",
        "        month_non_op = 0\n",
        "        year_non_op = 0\n",
        "\n",
        "        for non_op_date in non_operating_days:\n",
        "            # Convert to datetime for comparison with time periods\n",
        "            non_op_datetime = datetime.combine(non_op_date, datetime.min.time())\n",
        "\n",
        "            if non_op_datetime >= one_week_ago:\n",
        "                week_non_op += 1\n",
        "            if non_op_datetime >= one_month_ago:\n",
        "                month_non_op += 1\n",
        "            if non_op_datetime >= one_year_ago:\n",
        "                year_non_op += 1\n",
        "\n",
        "        # Calculate results\n",
        "        with results_lock:\n",
        "            bus_results[bus] = {\n",
        "                'week_non_op': week_non_op,\n",
        "                'month_non_op': month_non_op,\n",
        "                'year_non_op': year_non_op\n",
        "            }\n",
        "\n",
        "        print(f\"✅ Bus {bus}: Week({week_non_op}), Month({month_non_op}), Year({year_non_op}) non-operating days\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing bus {bus}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process buses in parallel batches\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(calculate_non_operating_days, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# === STEP 4: UPDATE REPORT ===\n",
        "print(\"\\n📝 STEP 4: Updating report...\")\n",
        "\n",
        "# Prepare data for batch update\n",
        "updated_rows = []\n",
        "for row_idx, bus in row_map.items():\n",
        "    result = bus_results.get(bus, {})\n",
        "\n",
        "    week_non_op = result.get('week_non_op', '')\n",
        "    month_non_op = result.get('month_non_op', '')\n",
        "    year_non_op = result.get('year_non_op', '')\n",
        "\n",
        "    updated_rows.append([week_non_op, month_non_op, year_non_op])\n",
        "\n",
        "# Prepare batch update\n",
        "if updated_rows:\n",
        "    # Split into manageable chunks\n",
        "    MAX_ROWS_PER_UPDATE = 1000  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(updated_rows, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AH{start_row}:AJ{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(updated_rows)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"🎉 Process finished successfully!\")"
      ],
      "metadata": {
        "id": "DRH0_iHWd7wy",
        "outputId": "8728b73d-40cf-4e0b-9522-ac54ab42c868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing performance optimizations...\n",
            "\n",
            "🧹 STEP 1: Clearing data...\n",
            "\n",
            "📅 STEP 2: Getting current date...\n",
            "\n",
            "🚌 STEP 3: Processing buses...\n",
            "🔎 Total buses to process: 83\n",
            "Sample buses: ['587', '475', '515', '486', '999']\n",
            "📑 Found worksheet 587 in Odometer_SVP\n",
            "❌ Error processing bus 587: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 475 in Odometer_SVP\n",
            "❌ Error processing bus 475: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 515 in Odometer_SVP\n",
            "❌ Error processing bus 515: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 486 in Odometer_SVP\n",
            "❌ Error processing bus 486: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 999 in Odometer_BT\n",
            "❌ Error processing bus 999: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 578 in Odometer_SVP\n",
            "❌ Error processing bus 578: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 440 in Odometer_SVP\n",
            "❌ Error processing bus 440: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 508 in Odometer_SVP\n",
            "❌ Error processing bus 508: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 561 in Odometer_FG\n",
            "❌ Error processing bus 561: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 471 in Odometer_SVP\n",
            "❌ Error processing bus 471: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 10/83 buses processed (12.0%)\n",
            "📑 Found worksheet 462 in Odometer_SVP\n",
            "❌ Error processing bus 462: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 463 in Odometer_FG\n",
            "❌ Error processing bus 463: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 460 in Odometer_SVP\n",
            "❌ Error processing bus 460: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 613 in Odometer_SVP\n",
            "❌ Error processing bus 613: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 604 in Odometer_SVP\n",
            "❌ Error processing bus 604: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 529 in Odometer_FG\n",
            "❌ Error processing bus 529: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 437 in Odometer_FG\n",
            "❌ Error processing bus 437: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 493 in Odometer_FG\n",
            "❌ Error processing bus 493: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 489 in Odometer_SVP\n",
            "✅ Bus 489: Week(0), Month(0), Year(0) non-operating days\n",
            "📑 Found worksheet 563 in Odometer_SVP\n",
            "❌ Error processing bus 563: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 20/83 buses processed (24.1%)\n",
            "📑 Found worksheet 521 in Odometer_SVP\n",
            "❌ Error processing bus 521: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 534 in Odometer_SVP\n",
            "❌ Error processing bus 534: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 538 in Odometer_SVP\n",
            "❌ Error processing bus 538: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 596 in Odometer_SVP\n",
            "❌ Error processing bus 596: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 444 in Odometer_SVP\n",
            "❌ Error processing bus 444: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 614 in Odometer_SVP\n",
            "❌ Error processing bus 614: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 536 in Odometer_SVP\n",
            "❌ Error processing bus 536: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 535 in Odometer_SVP\n",
            "❌ Error processing bus 535: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 546 in Odometer_SVP\n",
            "❌ Error processing bus 546: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 990 in Odometer_RT\n",
            "❌ Error processing bus 990: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 30/83 buses processed (36.1%)\n",
            "📑 Found worksheet 615 in Odometer_FG\n",
            "❌ Error processing bus 615: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 473 in Odometer_SVP\n",
            "❌ Error processing bus 473: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 595 in Odometer_SVP\n",
            "❌ Error processing bus 595: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 617 in Odometer_FG\n",
            "✅ Bus 617: Week(1), Month(1), Year(38) non-operating days\n",
            "📑 Found worksheet 589 in Odometer_SVP\n",
            "❌ Error processing bus 589: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 573 in Odometer_FG\n",
            "❌ Error processing bus 573: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 472 in Odometer_SVP\n",
            "❌ Error processing bus 472: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 459 in Odometer_SVP\n",
            "❌ Error processing bus 459: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 518 in Odometer_SVP\n",
            "✅ Bus 518: Week(0), Month(0), Year(0) non-operating days\n",
            "📑 Found worksheet 577 in Odometer_SVP\n",
            "❌ Error processing bus 577: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 40/83 buses processed (48.2%)\n",
            "📑 Found worksheet 560 in Odometer_FG\n",
            "❌ Error processing bus 560: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 548 in Odometer_SVP\n",
            "❌ Error processing bus 548: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 442 in Odometer_SVP\n",
            "❌ Error processing bus 442: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 616 in Odometer_FG\n",
            "✅ Bus 616: Week(0), Month(2), Year(9) non-operating days\n",
            "📑 Found worksheet 547 in Odometer_SVP\n",
            "❌ Error processing bus 547: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 504 in Odometer_SVP\n",
            "❌ Error processing bus 504: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 479 in Odometer_FG\n",
            "❌ Error processing bus 479: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 457 in Odometer_SVP\n",
            "❌ Error processing bus 457: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 441 in Odometer_SVP\n",
            "❌ Error processing bus 441: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 572 in Odometer_FG\n",
            "❌ Error processing bus 572: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 50/83 buses processed (60.2%)\n",
            "📑 Found worksheet 588 in Odometer_SVP\n",
            "❌ Error processing bus 588: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 476 in Odometer_SVP\n",
            "❌ Error processing bus 476: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 528 in Odometer_SVP\n",
            "❌ Error processing bus 528: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 970 in Odometer_DP\n",
            "❌ Error processing bus 970: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 593 in Odometer_FG\n",
            "✅ Bus 593: Week(0), Month(1), Year(36) non-operating days\n",
            "📑 Found worksheet 598 in Odometer_SVP\n",
            "❌ Error processing bus 598: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 612 in Odometer_SVP\n",
            "❌ Error processing bus 612: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 461 in Odometer_FG\n",
            "❌ Error processing bus 461: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 474 in Odometer_SVP\n",
            "❌ Error processing bus 474: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 590 in Odometer_SVP\n",
            "❌ Error processing bus 590: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 60/83 buses processed (72.3%)\n",
            "📑 Found worksheet 478 in Odometer_FG\n",
            "❌ Error processing bus 478: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 516 in Odometer_FG\n",
            "❌ Error processing bus 516: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 495 in Odometer_SVP\n",
            "❌ Error processing bus 495: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 574 in Odometer_SVP\n",
            "❌ Error processing bus 574: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 575 in Odometer_SVP\n",
            "❌ Error processing bus 575: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 438 in Odometer_FG\n",
            "✅ Bus 438: Week(0), Month(1), Year(76) non-operating days\n",
            "📑 Found worksheet 599 in Odometer_SVP\n",
            "❌ Error processing bus 599: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 562 in Odometer_FG\n",
            "❌ Error processing bus 562: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 507 in Odometer_FG\n",
            "❌ Error processing bus 507: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 550 in Odometer_SVP\n",
            "❌ Error processing bus 550: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 70/83 buses processed (84.3%)\n",
            "📑 Found worksheet 523 in Odometer_SVP\n",
            "❌ Error processing bus 523: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 597 in Odometer_SVP\n",
            "❌ Error processing bus 597: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 584 in Odometer_SVP\n",
            "❌ Error processing bus 584: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 469 in Odometer_SVP\n",
            "❌ Error processing bus 469: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 592 in Odometer_FG\n",
            "❌ Error processing bus 592: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 511 in Odometer_SVP\n",
            "❌ Error processing bus 511: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 605 in Odometer_SVP\n",
            "❌ Error processing bus 605: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 564 in Odometer_SVP\n",
            "❌ Error processing bus 564: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 585 in Odometer_SVP\n",
            "❌ Error processing bus 585: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 980 in Odometer_MB\n",
            "❌ Error processing bus 980: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 80/83 buses processed (96.4%)\n",
            "📑 Found worksheet 514 in Odometer_SVP\n",
            "❌ Error processing bus 514: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 603 in Odometer_SVP\n",
            "❌ Error processing bus 603: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 530 in Odometer_SVP\n",
            "❌ Error processing bus 530: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 83/83 buses processed (100.0%)\n",
            "✅ Completed processing all 83 buses\n",
            "\n",
            "📝 STEP 4: Updating report...\n",
            "✅ Updated rows 2-84 in Report_EO1\n",
            "✅ All 83 rows updated successfully in Report_EO1!\n",
            "🎉 Process finished successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ea82b1898bef>:309: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(range_to_update, chunk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R3\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# Cached function to get worksheet data\n",
        "@lru_cache_with_expiry(maxsize=100, ttl=300)  # Cache for 5 minutes\n",
        "def get_worksheet_data(spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Get worksheet data with caching\"\"\"\n",
        "    cache_key = f\"{spreadsheet_name}:{worksheet_name}\"\n",
        "\n",
        "    with cache_lock:\n",
        "        if cache_key in data_cache:\n",
        "            print(f\"📋 Cache hit for {cache_key}\")\n",
        "            return data_cache[cache_key]\n",
        "\n",
        "    # Function to find the actual worksheet object\n",
        "    def find_worksheet(spreadsheet_list, worksheet_name, spreadsheet_names):\n",
        "        for idx, spreadsheet in enumerate(spreadsheet_list):\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(worksheet_name)\n",
        "                print(f\"📑 Found worksheet {worksheet_name} in {spreadsheet_names[idx]}\")\n",
        "                return sheet, spreadsheet_names[idx]\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None, None\n",
        "\n",
        "    # Cache miss, need to retrieve data\n",
        "    try:\n",
        "        sheet, source = find_worksheet(odometer_spreadsheets, worksheet_name, spreadsheet_names)\n",
        "        if sheet:\n",
        "            data = sheet.get_all_values()\n",
        "\n",
        "            with cache_lock:\n",
        "                data_cache[cache_key] = (data, source)\n",
        "\n",
        "            return data, source\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error retrieving data for {worksheet_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# === STEP 1: CLEAR DATA from AH, AI, AJ Columns ===\n",
        "print(\"\\n🧹 STEP 1: Clearing data...\")\n",
        "\n",
        "clear_range_Report_EO1_Worksheet = ['AH2:AJ200']\n",
        "Report_EO1_Worksheet.batch_clear(clear_range_Report_EO1_Worksheet)\n",
        "\n",
        "# === STEP 2: GET CURRENT DATE ===\n",
        "print(\"\\n📅 STEP 2: Getting current date...\")\n",
        "current_date = datetime.now()\n",
        "current_date_str = current_date.strftime('%d,%b%y')\n",
        "\n",
        "# Calculate dates for different time periods\n",
        "one_week_ago = current_date - timedelta(days=7)\n",
        "one_month_ago = current_date - timedelta(days=30)\n",
        "one_year_ago = current_date - timedelta(days=365)\n",
        "\n",
        "# === STEP 3: PROCESS BUSES ===\n",
        "print(\"\\n🚌 STEP 3: Processing buses...\")\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize results with thread safety\n",
        "bus_results = {}\n",
        "results_lock = threading.Lock()\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse date string with multiple format support\"\"\"\n",
        "    for fmt in ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def calculate_non_operating_days(bus):\n",
        "    \"\"\"Calculate non-operating days for a bus across different time periods\"\"\"\n",
        "    try:\n",
        "        # Get the worksheet data for this bus\n",
        "        data, source = get_worksheet_data(\"All\", bus)\n",
        "\n",
        "        if not data or len(data) < 2:  # Need at least header + 1 data row\n",
        "            print(f\"⚠️ No data found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Extract date and odometer data\n",
        "        # Column B (index 1) is date and column M (index 12) is odometer\n",
        "        date_odometer_map = {}\n",
        "\n",
        "        for row in data[1:]:  # Skip header\n",
        "            if len(row) <= 12:  # Skip rows that don't have enough columns\n",
        "                continue\n",
        "\n",
        "            date_str = row[1].strip()\n",
        "            odometer_str = row[12].strip()\n",
        "\n",
        "            if not date_str or not odometer_str:\n",
        "                continue\n",
        "\n",
        "            parsed_date = parse_date(date_str)\n",
        "            if not parsed_date:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                odometer_value = int(odometer_str)\n",
        "                # Use date as key (without time) to group multiple entries on same date\n",
        "                date_key = parsed_date.date()\n",
        "\n",
        "                # For multiple entries on same date, keep the highest odometer value\n",
        "                if date_key in date_odometer_map:\n",
        "                    date_odometer_map[date_key] = max(date_odometer_map[date_key], odometer_value)\n",
        "                else:\n",
        "                    date_odometer_map[date_key] = odometer_value\n",
        "\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        if not date_odometer_map:\n",
        "            print(f\"⚠️ No valid date-odometer pairs found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Sort unique dates in ascending order\n",
        "        sorted_dates = sorted(date_odometer_map.keys())\n",
        "\n",
        "        # Identify non-operating days (only between dates, not on the same date)\n",
        "        non_operating_days = []\n",
        "\n",
        "        # Find date ranges where bus was not operating\n",
        "        for i in range(1, len(sorted_dates)):\n",
        "            current_date = sorted_dates[i]\n",
        "            prev_date = sorted_dates[i-1]\n",
        "\n",
        "            # Get odometer values\n",
        "            current_odo = date_odometer_map[current_date]\n",
        "            prev_odo = date_odometer_map[prev_date]\n",
        "\n",
        "            # If odometer value didn't change between different dates, bus didn't operate on current date\n",
        "            if current_odo == prev_odo and current_date != prev_date:\n",
        "                non_operating_days.append(current_date)\n",
        "\n",
        "            # Check for missing days between consecutive dates\n",
        "            date_diff = (current_date - prev_date).days\n",
        "            if date_diff > 1:\n",
        "                # For all days in between, mark as non-operating\n",
        "                for day_offset in range(1, date_diff):\n",
        "                    missing_date = prev_date + timedelta(days=day_offset)\n",
        "                    non_operating_days.append(missing_date)\n",
        "\n",
        "        # Count non-operating days in different periods\n",
        "        week_non_op = 0\n",
        "        month_non_op = 0\n",
        "        year_non_op = 0\n",
        "\n",
        "        for non_op_date in non_operating_days:\n",
        "            # Convert to datetime for comparison with time periods\n",
        "            non_op_datetime = datetime.combine(non_op_date, datetime.min.time())\n",
        "\n",
        "            if non_op_datetime >= one_week_ago:\n",
        "                week_non_op += 1\n",
        "            if non_op_datetime >= one_month_ago:\n",
        "                month_non_op += 1\n",
        "            if non_op_datetime >= one_year_ago:\n",
        "                year_non_op += 1\n",
        "\n",
        "        # Calculate results\n",
        "        with results_lock:\n",
        "            bus_results[bus] = {\n",
        "                'week_non_op': week_non_op,\n",
        "                'month_non_op': month_non_op,\n",
        "                'year_non_op': year_non_op\n",
        "            }\n",
        "\n",
        "        print(f\"✅ Bus {bus}: Week({week_non_op}), Month({month_non_op}), Year({year_non_op}) non-operating days\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing bus {bus}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process buses in parallel batches\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(calculate_non_operating_days, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# === STEP 4: UPDATE REPORT ===\n",
        "print(\"\\n📝 STEP 4: Updating report...\")\n",
        "\n",
        "# Prepare data for batch update\n",
        "updated_rows = []\n",
        "for row_idx, bus in row_map.items():\n",
        "    result = bus_results.get(bus, {})\n",
        "\n",
        "    week_non_op = result.get('week_non_op', '')\n",
        "    month_non_op = result.get('month_non_op', '')\n",
        "    year_non_op = result.get('year_non_op', '')\n",
        "\n",
        "    updated_rows.append([week_non_op, month_non_op, year_non_op])\n",
        "\n",
        "# Prepare batch update\n",
        "if updated_rows:\n",
        "    # Split into manageable chunks\n",
        "    MAX_ROWS_PER_UPDATE = 1000  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(updated_rows, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AH{start_row}:AJ{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(updated_rows)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"🎉 Process finished successfully!\")"
      ],
      "metadata": {
        "id": "jOvuRKCxfXJR",
        "outputId": "dfc222cc-c9ef-456a-c283-1b610a7a4f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing performance optimizations...\n",
            "\n",
            "🧹 STEP 1: Clearing data...\n",
            "\n",
            "📅 STEP 2: Getting current date...\n",
            "\n",
            "🚌 STEP 3: Processing buses...\n",
            "🔎 Total buses to process: 83\n",
            "Sample buses: ['587', '475', '515', '486', '999']\n",
            "📑 Found worksheet 587 in Odometer_SVP\n",
            "✅ Bus 587: Week(122), Month(131), Year(349) non-operating days\n",
            "📑 Found worksheet 475 in Odometer_SVP\n",
            "✅ Bus 475: Week(0), Month(0), Year(260) non-operating days\n",
            "📑 Found worksheet 515 in Odometer_SVP\n",
            "✅ Bus 515: Week(0), Month(0), Year(94) non-operating days\n",
            "📑 Found worksheet 486 in Odometer_SVP\n",
            "✅ Bus 486: Week(0), Month(0), Year(245) non-operating days\n",
            "📑 Found worksheet 999 in Odometer_BT\n",
            "✅ Bus 999: Week(0), Month(0), Year(78) non-operating days\n",
            "📑 Found worksheet 578 in Odometer_SVP\n",
            "✅ Bus 578: Week(3), Month(14), Year(220) non-operating days\n",
            "📑 Found worksheet 440 in Odometer_SVP\n",
            "✅ Bus 440: Week(205), Month(224), Year(398) non-operating days\n",
            "📑 Found worksheet 508 in Odometer_SVP\n",
            "✅ Bus 508: Week(614), Month(623), Year(814) non-operating days\n",
            "📑 Found worksheet 561 in Odometer_FG\n",
            "✅ Bus 561: Week(0), Month(0), Year(58) non-operating days\n",
            "📑 Found worksheet 471 in Odometer_SVP\n",
            "✅ Bus 471: Week(1), Month(8), Year(227) non-operating days\n",
            "Progress: 10/83 buses processed (12.0%)\n",
            "📑 Found worksheet 462 in Odometer_SVP\n",
            "✅ Bus 462: Week(4), Month(27), Year(174) non-operating days\n",
            "📑 Found worksheet 463 in Odometer_FG\n",
            "✅ Bus 463: Week(4), Month(27), Year(362) non-operating days\n",
            "📑 Found worksheet 460 in Odometer_SVP\n",
            "✅ Bus 460: Week(235), Month(258), Year(425) non-operating days\n",
            "📑 Found worksheet 613 in Odometer_SVP\n",
            "✅ Bus 613: Week(0), Month(0), Year(45) non-operating days\n",
            "📑 Found worksheet 604 in Odometer_SVP\n",
            "✅ Bus 604: Week(2), Month(15), Year(200) non-operating days\n",
            "📑 Found worksheet 529 in Odometer_FG\n",
            "✅ Bus 529: Week(4), Month(27), Year(135) non-operating days\n",
            "📑 Found worksheet 437 in Odometer_FG\n",
            "✅ Bus 437: Week(0), Month(2), Year(172) non-operating days\n",
            "📑 Found worksheet 493 in Odometer_FG\n",
            "✅ Bus 493: Week(4), Month(15), Year(220) non-operating days\n",
            "📑 Found worksheet 489 in Odometer_SVP\n",
            "✅ Bus 489: Week(0), Month(0), Year(0) non-operating days\n",
            "📑 Found worksheet 563 in Odometer_SVP\n",
            "✅ Bus 563: Week(2), Month(16), Year(223) non-operating days\n",
            "Progress: 20/83 buses processed (24.1%)\n",
            "📑 Found worksheet 521 in Odometer_SVP\n",
            "✅ Bus 521: Week(2), Month(13), Year(213) non-operating days\n",
            "📑 Found worksheet 534 in Odometer_SVP\n",
            "✅ Bus 534: Week(2), Month(6), Year(142) non-operating days\n",
            "📑 Found worksheet 538 in Odometer_SVP\n",
            "✅ Bus 538: Week(1), Month(7), Year(161) non-operating days\n",
            "📑 Found worksheet 596 in Odometer_SVP\n",
            "✅ Bus 596: Week(3), Month(16), Year(210) non-operating days\n",
            "📑 Found worksheet 444 in Odometer_SVP\n",
            "✅ Bus 444: Week(2), Month(8), Year(254) non-operating days\n",
            "📑 Found worksheet 614 in Odometer_SVP\n",
            "✅ Bus 614: Week(0), Month(3), Year(135) non-operating days\n",
            "📑 Found worksheet 536 in Odometer_SVP\n",
            "✅ Bus 536: Week(1), Month(12), Year(125) non-operating days\n",
            "📑 Found worksheet 535 in Odometer_SVP\n",
            "✅ Bus 535: Week(2), Month(7), Year(90) non-operating days\n",
            "📑 Found worksheet 546 in Odometer_SVP\n",
            "✅ Bus 546: Week(2), Month(13), Year(198) non-operating days\n",
            "📑 Found worksheet 990 in Odometer_RT\n",
            "✅ Bus 990: Week(111), Month(134), Year(370) non-operating days\n",
            "Progress: 30/83 buses processed (36.1%)\n",
            "📑 Found worksheet 615 in Odometer_FG\n",
            "✅ Bus 615: Week(4), Month(23), Year(53) non-operating days\n",
            "📑 Found worksheet 473 in Odometer_SVP\n",
            "✅ Bus 473: Week(0), Month(0), Year(198) non-operating days\n",
            "📑 Found worksheet 595 in Odometer_SVP\n",
            "✅ Bus 595: Week(0), Month(12), Year(203) non-operating days\n",
            "📑 Found worksheet 617 in Odometer_FG\n",
            "✅ Bus 617: Week(1), Month(1), Year(38) non-operating days\n",
            "📑 Found worksheet 589 in Odometer_SVP\n",
            "✅ Bus 589: Week(0), Month(0), Year(184) non-operating days\n",
            "📑 Found worksheet 573 in Odometer_FG\n",
            "✅ Bus 573: Week(0), Month(0), Year(30) non-operating days\n",
            "📑 Found worksheet 472 in Odometer_SVP\n",
            "✅ Bus 472: Week(5842), Month(5849), Year(5985) non-operating days\n",
            "📑 Found worksheet 459 in Odometer_SVP\n",
            "✅ Bus 459: Week(0), Month(5), Year(290) non-operating days\n",
            "📑 Found worksheet 518 in Odometer_SVP\n",
            "✅ Bus 518: Week(0), Month(0), Year(0) non-operating days\n",
            "📑 Found worksheet 577 in Odometer_SVP\n",
            "✅ Bus 577: Week(2), Month(11), Year(173) non-operating days\n",
            "Progress: 40/83 buses processed (48.2%)\n",
            "📑 Found worksheet 560 in Odometer_FG\n",
            "✅ Bus 560: Week(1), Month(1), Year(93) non-operating days\n",
            "📑 Found worksheet 548 in Odometer_SVP\n",
            "✅ Bus 548: Week(3), Month(12), Year(208) non-operating days\n",
            "📑 Found worksheet 442 in Odometer_SVP\n",
            "✅ Bus 442: Week(1), Month(14), Year(244) non-operating days\n",
            "📑 Found worksheet 616 in Odometer_FG\n",
            "✅ Bus 616: Week(0), Month(2), Year(9) non-operating days\n",
            "📑 Found worksheet 547 in Odometer_SVP\n",
            "✅ Bus 547: Week(233), Month(246), Year(498) non-operating days\n",
            "📑 Found worksheet 504 in Odometer_SVP\n",
            "✅ Bus 504: Week(100), Month(114), Year(413) non-operating days\n",
            "📑 Found worksheet 479 in Odometer_FG\n",
            "✅ Bus 479: Week(0), Month(0), Year(32) non-operating days\n",
            "📑 Found worksheet 457 in Odometer_SVP\n",
            "✅ Bus 457: Week(2), Month(12), Year(217) non-operating days\n",
            "📑 Found worksheet 441 in Odometer_SVP\n",
            "✅ Bus 441: Week(111), Month(134), Year(370) non-operating days\n",
            "📑 Found worksheet 572 in Odometer_FG\n",
            "✅ Bus 572: Week(224), Month(224), Year(228) non-operating days\n",
            "Progress: 50/83 buses processed (60.2%)\n",
            "📑 Found worksheet 588 in Odometer_SVP\n",
            "✅ Bus 588: Week(0), Month(2), Year(116) non-operating days\n",
            "📑 Found worksheet 476 in Odometer_SVP\n",
            "✅ Bus 476: Week(1281), Month(1290), Year(1452) non-operating days\n",
            "📑 Found worksheet 528 in Odometer_SVP\n",
            "✅ Bus 528: Week(0), Month(3), Year(224) non-operating days\n",
            "📑 Found worksheet 970 in Odometer_DP\n",
            "✅ Bus 970: Week(899), Month(922), Year(1095) non-operating days\n",
            "📑 Found worksheet 593 in Odometer_FG\n",
            "✅ Bus 593: Week(0), Month(1), Year(36) non-operating days\n",
            "📑 Found worksheet 598 in Odometer_SVP\n",
            "✅ Bus 598: Week(2), Month(10), Year(212) non-operating days\n",
            "📑 Found worksheet 612 in Odometer_SVP\n",
            "✅ Bus 612: Week(88), Month(93), Year(202) non-operating days\n",
            "📑 Found worksheet 461 in Odometer_FG\n",
            "✅ Bus 461: Week(0), Month(10), Year(98) non-operating days\n",
            "📑 Found worksheet 474 in Odometer_SVP\n",
            "✅ Bus 474: Week(11), Month(25), Year(264) non-operating days\n",
            "📑 Found worksheet 590 in Odometer_SVP\n",
            "✅ Bus 590: Week(0), Month(11), Year(201) non-operating days\n",
            "Progress: 60/83 buses processed (72.3%)\n",
            "📑 Found worksheet 478 in Odometer_FG\n",
            "✅ Bus 478: Week(1), Month(5), Year(101) non-operating days\n",
            "📑 Found worksheet 516 in Odometer_FG\n",
            "✅ Bus 516: Week(1), Month(1), Year(76) non-operating days\n",
            "📑 Found worksheet 495 in Odometer_SVP\n",
            "✅ Bus 495: Week(750), Month(752), Year(884) non-operating days\n",
            "📑 Found worksheet 574 in Odometer_SVP\n",
            "✅ Bus 574: Week(0), Month(0), Year(113) non-operating days\n",
            "📑 Found worksheet 575 in Odometer_SVP\n",
            "✅ Bus 575: Week(0), Month(0), Year(114) non-operating days\n",
            "📑 Found worksheet 438 in Odometer_FG\n",
            "✅ Bus 438: Week(0), Month(1), Year(76) non-operating days\n",
            "📑 Found worksheet 599 in Odometer_SVP\n",
            "✅ Bus 599: Week(2), Month(12), Year(189) non-operating days\n",
            "📑 Found worksheet 562 in Odometer_FG\n",
            "✅ Bus 562: Week(2), Month(5), Year(186) non-operating days\n",
            "📑 Found worksheet 507 in Odometer_FG\n",
            "✅ Bus 507: Week(0), Month(0), Year(69) non-operating days\n",
            "📑 Found worksheet 550 in Odometer_SVP\n",
            "✅ Bus 550: Week(243), Month(266), Year(525) non-operating days\n",
            "Progress: 70/83 buses processed (84.3%)\n",
            "📑 Found worksheet 523 in Odometer_SVP\n",
            "✅ Bus 523: Week(10891), Month(10911), Year(11141) non-operating days\n",
            "📑 Found worksheet 597 in Odometer_SVP\n",
            "✅ Bus 597: Week(1), Month(16), Year(210) non-operating days\n",
            "📑 Found worksheet 584 in Odometer_SVP\n",
            "✅ Bus 584: Week(0), Month(0), Year(74) non-operating days\n",
            "📑 Found worksheet 469 in Odometer_SVP\n",
            "✅ Bus 469: Week(0), Month(10), Year(213) non-operating days\n",
            "📑 Found worksheet 592 in Odometer_FG\n",
            "✅ Bus 592: Week(0), Month(0), Year(6) non-operating days\n",
            "📑 Found worksheet 511 in Odometer_SVP\n",
            "✅ Bus 511: Week(0), Month(15), Year(163) non-operating days\n",
            "📑 Found worksheet 605 in Odometer_SVP\n",
            "✅ Bus 605: Week(2), Month(10), Year(180) non-operating days\n",
            "📑 Found worksheet 564 in Odometer_SVP\n",
            "✅ Bus 564: Week(0), Month(0), Year(0) non-operating days\n",
            "📑 Found worksheet 585 in Odometer_SVP\n",
            "✅ Bus 585: Week(12754), Month(12756), Year(12828) non-operating days\n",
            "📑 Found worksheet 980 in Odometer_MB\n",
            "✅ Bus 980: Week(5843), Month(5866), Year(6007) non-operating days\n",
            "Progress: 80/83 buses processed (96.4%)\n",
            "📑 Found worksheet 514 in Odometer_SVP\n",
            "✅ Bus 514: Week(3), Month(10), Year(216) non-operating days\n",
            "📑 Found worksheet 603 in Odometer_SVP\n",
            "✅ Bus 603: Week(74), Month(83), Year(246) non-operating days\n",
            "📑 Found worksheet 530 in Odometer_SVP\n",
            "✅ Bus 530: Week(3), Month(15), Year(277) non-operating days\n",
            "Progress: 83/83 buses processed (100.0%)\n",
            "✅ Completed processing all 83 buses\n",
            "\n",
            "📝 STEP 4: Updating report...\n",
            "✅ Updated rows 2-84 in Report_EO1\n",
            "✅ All 83 rows updated successfully in Report_EO1!\n",
            "🎉 Process finished successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-6a3cc092d4d0>:309: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(range_to_update, chunk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R5\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# Cached function to get worksheet data\n",
        "@lru_cache_with_expiry(maxsize=100, ttl=300)  # Cache for 5 minutes\n",
        "def get_worksheet_data(spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Get worksheet data with caching\"\"\"\n",
        "    cache_key = f\"{spreadsheet_name}:{worksheet_name}\"\n",
        "\n",
        "    with cache_lock:\n",
        "        if cache_key in data_cache:\n",
        "            print(f\"📋 Cache hit for {cache_key}\")\n",
        "            return data_cache[cache_key]\n",
        "\n",
        "    # Function to find the actual worksheet object\n",
        "    def find_worksheet(spreadsheet_list, worksheet_name, spreadsheet_names):\n",
        "        for idx, spreadsheet in enumerate(spreadsheet_list):\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(worksheet_name)\n",
        "                print(f\"📑 Found worksheet {worksheet_name} in {spreadsheet_names[idx]}\")\n",
        "                return sheet, spreadsheet_names[idx]\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None, None\n",
        "\n",
        "    # Cache miss, need to retrieve data\n",
        "    try:\n",
        "        sheet, source = find_worksheet(odometer_spreadsheets, worksheet_name, spreadsheet_names)\n",
        "        if sheet:\n",
        "            data = sheet.get_all_values()\n",
        "\n",
        "            with cache_lock:\n",
        "                data_cache[cache_key] = (data, source)\n",
        "\n",
        "            return data, source\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error retrieving data for {worksheet_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# === STEP 1: CLEAR DATA from AH, AI, AJ Columns ===\n",
        "print(\"\\n🧹 STEP 1: Clearing data...\")\n",
        "\n",
        "clear_range_Report_EO1_Worksheet = ['AH2:AJ200']\n",
        "Report_EO1_Worksheet.batch_clear(clear_range_Report_EO1_Worksheet)\n",
        "\n",
        "# === STEP 2: GET CURRENT DATE ===\n",
        "print(\"\\n📅 STEP 2: Getting current date...\")\n",
        "current_date = datetime.now()\n",
        "current_date_str = current_date.strftime('%d,%b%y')\n",
        "\n",
        "# Calculate dates for different time periods\n",
        "one_week_ago = current_date - timedelta(days=7)\n",
        "one_month_ago = current_date - timedelta(days=30)\n",
        "one_year_ago = current_date - timedelta(days=365)\n",
        "\n",
        "# === STEP 3: PROCESS BUSES ===\n",
        "print(\"\\n🚌 STEP 3: Processing buses...\")\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize results with thread safety\n",
        "bus_results = {}\n",
        "results_lock = threading.Lock()\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse date string with multiple format support\"\"\"\n",
        "    for fmt in ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def calculate_non_operating_days(bus):\n",
        "    \"\"\"Calculate non-operating days for a bus across different time periods\"\"\"\n",
        "    try:\n",
        "        # Get the worksheet data for this bus\n",
        "        data, source = get_worksheet_data(\"All\", bus)\n",
        "\n",
        "        if not data or len(data) < 2:  # Need at least header + 1 data row\n",
        "            print(f\"⚠️ No data found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Extract date and odometer data\n",
        "        # Column B (index 1) is date and column M (index 12) is odometer\n",
        "        date_odometer_map = {}\n",
        "\n",
        "        for row in data[1:]:  # Skip header\n",
        "            if len(row) <= 12:  # Skip rows that don't have enough columns\n",
        "                continue\n",
        "\n",
        "            date_str = row[1].strip()\n",
        "            odometer_str = row[12].strip()\n",
        "\n",
        "            if not date_str or not odometer_str:\n",
        "                continue\n",
        "\n",
        "            parsed_date = parse_date(date_str)\n",
        "            if not parsed_date:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                odometer_value = int(odometer_str)\n",
        "                # Use date as key (without time) to group multiple entries on same date\n",
        "                date_key = parsed_date.date()\n",
        "\n",
        "                # For multiple entries on same date, keep the highest odometer value\n",
        "                if date_key in date_odometer_map:\n",
        "                    date_odometer_map[date_key] = max(date_odometer_map[date_key], odometer_value)\n",
        "                else:\n",
        "                    date_odometer_map[date_key] = odometer_value\n",
        "\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        if not date_odometer_map:\n",
        "            print(f\"⚠️ No valid date-odometer pairs found for bus {bus}\")\n",
        "            return None\n",
        "\n",
        "        # Sort unique dates in ascending order\n",
        "        sorted_dates = sorted(date_odometer_map.keys())\n",
        "\n",
        "        # Identify non-operating days (only between dates, not on the same date)\n",
        "        non_operating_days = []\n",
        "\n",
        "        # Find date ranges where bus was not operating\n",
        "        for i in range(1, len(sorted_dates)):\n",
        "            current_date = sorted_dates[i]\n",
        "            prev_date = sorted_dates[i-1]\n",
        "\n",
        "            # Get odometer values\n",
        "            current_odo = date_odometer_map[current_date]\n",
        "            prev_odo = date_odometer_map[prev_date]\n",
        "\n",
        "            # If odometer value didn't change between different dates, bus didn't operate on current date\n",
        "            if current_odo == prev_odo and current_date != prev_date:\n",
        "                non_operating_days.append(current_date)\n",
        "\n",
        "            # Check for missing days between consecutive dates\n",
        "            date_diff = (current_date - prev_date).days\n",
        "            if date_diff > 1:\n",
        "                # For all days in between, mark as non-operating\n",
        "                for day_offset in range(1, date_diff):\n",
        "                    missing_date = prev_date + timedelta(days=day_offset)\n",
        "                    non_operating_days.append(missing_date)\n",
        "\n",
        "        # Count non-operating days in different periods\n",
        "        today = datetime.now().date()\n",
        "        one_week_ago_date = (today - timedelta(days=7)).date()\n",
        "        one_month_ago_date = (today - timedelta(days=30)).date()\n",
        "        one_year_ago_date = (today - timedelta(days=365)).date()\n",
        "\n",
        "        # Filter non-operating days for each time period\n",
        "        week_non_op_days = [d for d in non_operating_days if d >= one_week_ago_date and d <= today]\n",
        "        month_non_op_days = [d for d in non_operating_days if d >= one_month_ago_date and d <= today]\n",
        "        year_non_op_days = [d for d in non_operating_days if d >= one_year_ago_date and d <= today]\n",
        "\n",
        "        week_non_op = len(week_non_op_days)\n",
        "        month_non_op = len(month_non_op_days)\n",
        "        year_non_op = len(year_non_op_days)\n",
        "\n",
        "        # Calculate results\n",
        "        with results_lock:\n",
        "            bus_results[bus] = {\n",
        "                'week_non_op': min(week_non_op, 7),  # Can't have more than 7 non-op days in a week\n",
        "                'month_non_op': min(month_non_op, 30),  # Cap at 30 for a month\n",
        "                'year_non_op': min(year_non_op, 365)  # Cap at 365 for a year\n",
        "            }\n",
        "\n",
        "        print(f\"✅ Bus {bus}: Week({week_non_op}), Month({month_non_op}), Year({year_non_op}) non-operating days\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing bus {bus}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process buses in parallel batches\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(calculate_non_operating_days, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# === STEP 4: UPDATE REPORT ===\n",
        "print(\"\\n📝 STEP 4: Updating report...\")\n",
        "\n",
        "# Prepare data for batch update\n",
        "updated_rows = []\n",
        "for row_idx, bus in row_map.items():\n",
        "    result = bus_results.get(bus, {})\n",
        "\n",
        "    week_non_op = result.get('week_non_op', '')\n",
        "    month_non_op = result.get('month_non_op', '')\n",
        "    year_non_op = result.get('year_non_op', '')\n",
        "\n",
        "    updated_rows.append([week_non_op, month_non_op, year_non_op])\n",
        "\n",
        "# Prepare batch update\n",
        "if updated_rows:\n",
        "    # Split into manageable chunks\n",
        "    MAX_ROWS_PER_UPDATE = 1000  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(updated_rows, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AH{start_row}:AJ{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(updated_rows)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"🎉 Process finished successfully!\")"
      ],
      "metadata": {
        "id": "BETC4Ac-gT6i",
        "outputId": "11e58305-4a66-420f-e777-ff63b3d7bff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing performance optimizations...\n",
            "\n",
            "🧹 STEP 1: Clearing data...\n",
            "\n",
            "📅 STEP 2: Getting current date...\n",
            "\n",
            "🚌 STEP 3: Processing buses...\n",
            "🔎 Total buses to process: 83\n",
            "Sample buses: ['587', '475', '515', '486', '999']\n",
            "📑 Found worksheet 587 in Odometer_SVP\n",
            "❌ Error processing bus 587: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 475 in Odometer_SVP\n",
            "❌ Error processing bus 475: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 515 in Odometer_SVP\n",
            "❌ Error processing bus 515: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 486 in Odometer_SVP\n",
            "❌ Error processing bus 486: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 999 in Odometer_BT\n",
            "❌ Error processing bus 999: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 578 in Odometer_SVP\n",
            "❌ Error processing bus 578: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 440 in Odometer_SVP\n",
            "❌ Error processing bus 440: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 508 in Odometer_SVP\n",
            "❌ Error processing bus 508: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 561 in Odometer_FG\n",
            "❌ Error processing bus 561: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 471 in Odometer_SVP\n",
            "❌ Error processing bus 471: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 10/83 buses processed (12.0%)\n",
            "📑 Found worksheet 462 in Odometer_SVP\n",
            "❌ Error processing bus 462: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 463 in Odometer_FG\n",
            "❌ Error processing bus 463: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 460 in Odometer_SVP\n",
            "❌ Error processing bus 460: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 613 in Odometer_SVP\n",
            "❌ Error processing bus 613: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 604 in Odometer_SVP\n",
            "❌ Error processing bus 604: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 529 in Odometer_FG\n",
            "❌ Error processing bus 529: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 437 in Odometer_FG\n",
            "❌ Error processing bus 437: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 493 in Odometer_FG\n",
            "❌ Error processing bus 493: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 489 in Odometer_SVP\n",
            "❌ Error processing bus 489: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 563 in Odometer_SVP\n",
            "❌ Error processing bus 563: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 20/83 buses processed (24.1%)\n",
            "📑 Found worksheet 521 in Odometer_SVP\n",
            "❌ Error processing bus 521: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 534 in Odometer_SVP\n",
            "❌ Error processing bus 534: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 538 in Odometer_SVP\n",
            "❌ Error processing bus 538: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 596 in Odometer_SVP\n",
            "❌ Error processing bus 596: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 444 in Odometer_SVP\n",
            "❌ Error processing bus 444: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 614 in Odometer_SVP\n",
            "❌ Error processing bus 614: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 536 in Odometer_SVP\n",
            "❌ Error processing bus 536: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 535 in Odometer_SVP\n",
            "❌ Error processing bus 535: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 546 in Odometer_SVP\n",
            "❌ Error processing bus 546: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 990 in Odometer_RT\n",
            "❌ Error processing bus 990: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 30/83 buses processed (36.1%)\n",
            "📑 Found worksheet 615 in Odometer_FG\n",
            "❌ Error processing bus 615: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 473 in Odometer_SVP\n",
            "❌ Error processing bus 473: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 595 in Odometer_SVP\n",
            "❌ Error processing bus 595: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 617 in Odometer_FG\n",
            "❌ Error processing bus 617: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 589 in Odometer_SVP\n",
            "❌ Error processing bus 589: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 573 in Odometer_FG\n",
            "❌ Error processing bus 573: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 472 in Odometer_SVP\n",
            "❌ Error processing bus 472: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 459 in Odometer_SVP\n",
            "❌ Error processing bus 459: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 518 in Odometer_SVP\n",
            "❌ Error processing bus 518: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 577 in Odometer_SVP\n",
            "❌ Error processing bus 577: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 40/83 buses processed (48.2%)\n",
            "📑 Found worksheet 560 in Odometer_FG\n",
            "❌ Error processing bus 560: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 548 in Odometer_SVP\n",
            "❌ Error processing bus 548: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 442 in Odometer_SVP\n",
            "❌ Error processing bus 442: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 616 in Odometer_FG\n",
            "❌ Error processing bus 616: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 547 in Odometer_SVP\n",
            "❌ Error processing bus 547: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 504 in Odometer_SVP\n",
            "❌ Error processing bus 504: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 479 in Odometer_FG\n",
            "❌ Error processing bus 479: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 457 in Odometer_SVP\n",
            "❌ Error processing bus 457: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 441 in Odometer_SVP\n",
            "❌ Error processing bus 441: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 572 in Odometer_FG\n",
            "❌ Error processing bus 572: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 50/83 buses processed (60.2%)\n",
            "📑 Found worksheet 588 in Odometer_SVP\n",
            "❌ Error processing bus 588: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 476 in Odometer_SVP\n",
            "❌ Error processing bus 476: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 528 in Odometer_SVP\n",
            "❌ Error processing bus 528: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 970 in Odometer_DP\n",
            "❌ Error processing bus 970: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 593 in Odometer_FG\n",
            "❌ Error processing bus 593: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 598 in Odometer_SVP\n",
            "❌ Error processing bus 598: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 612 in Odometer_SVP\n",
            "❌ Error processing bus 612: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 461 in Odometer_FG\n",
            "❌ Error processing bus 461: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 474 in Odometer_SVP\n",
            "❌ Error processing bus 474: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 590 in Odometer_SVP\n",
            "❌ Error processing bus 590: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 60/83 buses processed (72.3%)\n",
            "📑 Found worksheet 478 in Odometer_FG\n",
            "❌ Error processing bus 478: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 516 in Odometer_FG\n",
            "❌ Error processing bus 516: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 495 in Odometer_SVP\n",
            "❌ Error processing bus 495: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 574 in Odometer_SVP\n",
            "❌ Error processing bus 574: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 575 in Odometer_SVP\n",
            "❌ Error processing bus 575: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 438 in Odometer_FG\n",
            "❌ Error processing bus 438: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 599 in Odometer_SVP\n",
            "❌ Error processing bus 599: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 562 in Odometer_FG\n",
            "❌ Error processing bus 562: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 507 in Odometer_FG\n",
            "❌ Error processing bus 507: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 550 in Odometer_SVP\n",
            "❌ Error processing bus 550: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 70/83 buses processed (84.3%)\n",
            "📑 Found worksheet 523 in Odometer_SVP\n",
            "❌ Error processing bus 523: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 597 in Odometer_SVP\n",
            "❌ Error processing bus 597: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 584 in Odometer_SVP\n",
            "❌ Error processing bus 584: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 469 in Odometer_SVP\n",
            "❌ Error processing bus 469: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 592 in Odometer_FG\n",
            "❌ Error processing bus 592: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 511 in Odometer_SVP\n",
            "❌ Error processing bus 511: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 605 in Odometer_SVP\n",
            "❌ Error processing bus 605: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 564 in Odometer_SVP\n",
            "❌ Error processing bus 564: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 585 in Odometer_SVP\n",
            "❌ Error processing bus 585: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 980 in Odometer_MB\n",
            "❌ Error processing bus 980: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 80/83 buses processed (96.4%)\n",
            "📑 Found worksheet 514 in Odometer_SVP\n",
            "❌ Error processing bus 514: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 603 in Odometer_SVP\n",
            "❌ Error processing bus 603: 'datetime.date' object has no attribute 'date'\n",
            "📑 Found worksheet 530 in Odometer_SVP\n",
            "❌ Error processing bus 530: 'datetime.date' object has no attribute 'date'\n",
            "Progress: 83/83 buses processed (100.0%)\n",
            "✅ Completed processing all 83 buses\n",
            "\n",
            "📝 STEP 4: Updating report...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-632ae05fe42e>:308: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(range_to_update, chunk)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated rows 2-84 in Report_EO1\n",
            "✅ All 83 rows updated successfully in Report_EO1!\n",
            "🎉 Process finished successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R6 Deepseek\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# LRU Cache decorator for expensive operations\n",
        "def lru_cache_with_expiry(maxsize=128, typed=False, ttl=600):\n",
        "    \"\"\"LRU Cache with expiry time\"\"\"\n",
        "    def decorator(func):\n",
        "        cache_dict = {}\n",
        "        cache_times = {}\n",
        "        cache_lock = threading.Lock()\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            key = str(args) + str(kwargs)\n",
        "            current_time = time.time()\n",
        "\n",
        "            with cache_lock:\n",
        "                # Check if key exists and not expired\n",
        "                if key in cache_dict and current_time - cache_times[key] < ttl:\n",
        "                    return cache_dict[key]\n",
        "\n",
        "                # Call the function and cache result\n",
        "                result = func(*args, **kwargs)\n",
        "                cache_dict[key] = result\n",
        "                cache_times[key] = current_time\n",
        "\n",
        "                # Remove oldest entries if cache is too large\n",
        "                if len(cache_dict) > maxsize:\n",
        "                    oldest_key = min(cache_times, key=cache_times.get)\n",
        "                    cache_dict.pop(oldest_key)\n",
        "                    cache_times.pop(oldest_key)\n",
        "\n",
        "                return result\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Batch processing helper functions\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# === MAIN SCRIPT ===\n",
        "\n",
        "# === STEP 1: CLEAR DATA ===\n",
        "print(\"\\n🧹 STEP 1: Clearing existing data...\")\n",
        "clear_range = ['AH2:AJ200']\n",
        "Report_EO1_Worksheet.batch_clear(clear_range)\n",
        "print(\"✅ Cleared columns AH-AJ in rows 2-200\")\n",
        "\n",
        "# === STEP 2: GET DATE RANGES ===\n",
        "print(\"\\n📅 STEP 2: Calculating date ranges...\")\n",
        "current_date = datetime.now()\n",
        "date_formats = ['%d,%b%y', '%d-%b-%y', '%d/%b/%y']  # Supported date formats\n",
        "\n",
        "# Calculate reference dates\n",
        "week_ago = current_date - timedelta(days=7)\n",
        "month_ago = current_date - timedelta(days=30)\n",
        "year_ago = current_date - timedelta(days=365)\n",
        "\n",
        "print(f\"Current date: {current_date.strftime('%d,%b%y')}\")\n",
        "print(f\"Last week: {week_ago.strftime('%d,%b%y')}\")\n",
        "print(f\"Last month: {month_ago.strftime('%d,%b%y')}\")\n",
        "print(f\"Last year: {year_ago.strftime('%d,%b%y')}\")\n",
        "\n",
        "# === STEP 3: PROCESS BUS DATA ===\n",
        "print(\"\\n🚌 STEP 3: Processing bus data...\")\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize result storage with thread safety\n",
        "non_op_days_map = defaultdict(dict)  # {bus: {'week': X, 'month': Y, 'year': Z}}\n",
        "map_lock = threading.Lock()\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Try parsing date with multiple formats\"\"\"\n",
        "    for fmt in date_formats:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def get_bus_odometer_data(bus):\n",
        "    \"\"\"Retrieve and clean odometer data for a bus\"\"\"\n",
        "    try:\n",
        "        # Try to find the bus in all spreadsheets\n",
        "        for spreadsheet in odometer_spreadsheets:\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(bus)\n",
        "                data = sheet.get_all_values()\n",
        "                if data:\n",
        "                    # Process data: date, odometer reading (assuming column 1 is date, column 12 is odometer)\n",
        "                    clean_data = []\n",
        "                    for row in data[1:]:  # Skip header\n",
        "                        if len(row) >= 12:\n",
        "                            date_str = row[1].strip()\n",
        "                            odo_str = row[11].strip()  # Assuming column L (12th col) is odometer\n",
        "                            if date_str and odo_str and odo_str.isdigit():\n",
        "                                date = parse_date(date_str)\n",
        "                                if date:\n",
        "                                    clean_data.append((date, int(odo_str)))\n",
        "                    return clean_data\n",
        "            except:\n",
        "                continue\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error getting data for bus {bus}: {e}\")\n",
        "        return []\n",
        "\n",
        "def calculate_non_operating_days(bus_data, current_date):\n",
        "    \"\"\"Calculate non-operating days for different periods\"\"\"\n",
        "    if not bus_data:\n",
        "        return {'week': 0, 'month': 0, 'year': 0}\n",
        "\n",
        "    # Sort data by date\n",
        "    bus_data.sort(key=lambda x: x[0])\n",
        "\n",
        "    # Initialize counters\n",
        "    non_op_week = 0\n",
        "    non_op_month = 0\n",
        "    non_op_year = 0\n",
        "\n",
        "    # Track all dates in the data\n",
        "    date_set = {entry[0] for entry in bus_data}\n",
        "    min_date = min(bus_data[0][0], current_date - timedelta(days=365))\n",
        "    max_date = current_date\n",
        "\n",
        "    # Method 1: Same odometer on consecutive days\n",
        "    for i in range(1, len(bus_data)):\n",
        "        prev_date, prev_odo = bus_data[i-1]\n",
        "        curr_date, curr_odo = bus_data[i]\n",
        "\n",
        "        if curr_odo == prev_odo:\n",
        "            # Bus didn't operate on curr_date\n",
        "            days_diff = (curr_date - prev_date).days - 1\n",
        "            if days_diff >= 0:\n",
        "                # Check which periods this non-op day falls into\n",
        "                for single_date in [prev_date + timedelta(days=d+1) for d in range(days_diff)]:\n",
        "                    if single_date > current_date:\n",
        "                        continue\n",
        "                    if single_date >= week_ago:\n",
        "                        non_op_week += 1\n",
        "                    if single_date >= month_ago:\n",
        "                        non_op_month += 1\n",
        "                    if single_date >= year_ago:\n",
        "                        non_op_year += 1\n",
        "\n",
        "    # Method 2: Missing dates between entries\n",
        "    all_dates = [entry[0] for entry in bus_data]\n",
        "    for i in range(1, len(all_dates)):\n",
        "        prev_date = all_dates[i-1]\n",
        "        curr_date = all_dates[i]\n",
        "        date_diff = (curr_date - prev_date).days\n",
        "\n",
        "        if date_diff > 1:\n",
        "            # Missing days between prev_date and curr_date\n",
        "            for day in range(1, date_diff):\n",
        "                missing_date = prev_date + timedelta(days=day)\n",
        "                if missing_date > current_date:\n",
        "                    continue\n",
        "                # Check which periods this missing day falls into\n",
        "                if missing_date >= week_ago:\n",
        "                    non_op_week += 1\n",
        "                if missing_date >= month_ago:\n",
        "                    non_op_month += 1\n",
        "                if missing_date >= year_ago:\n",
        "                    non_op_year += 1\n",
        "\n",
        "    return {\n",
        "        'week': non_op_week,\n",
        "        'month': non_op_month,\n",
        "        'year': non_op_year\n",
        "    }\n",
        "\n",
        "def process_bus(bus):\n",
        "    \"\"\"Process a single bus to calculate non-operating days\"\"\"\n",
        "    try:\n",
        "        # Get odometer data for this bus\n",
        "        bus_data = get_bus_odometer_data(bus)\n",
        "\n",
        "        # Calculate non-operating days\n",
        "        non_op_days = calculate_non_operating_days(bus_data, current_date)\n",
        "\n",
        "        # Store results\n",
        "        with map_lock:\n",
        "            non_op_days_map[bus] = non_op_days\n",
        "\n",
        "        print(f\"✅ Processed {bus}: {non_op_days['week']}w/{non_op_days['month']}m/{non_op_days['year']}y\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Process buses in parallel\n",
        "MAX_WORKERS = 5  # Adjust based on API limits\n",
        "BATCH_SIZE = 10  # Process this many buses at once\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(process_bus, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches to avoid API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# === STEP 4: UPDATE REPORT ===\n",
        "print(\"\\n📝 STEP 4: Updating report worksheet...\")\n",
        "\n",
        "# Prepare update data\n",
        "update_data = []\n",
        "for row_idx, bus in row_map.items():\n",
        "    non_op_days = non_op_days_map.get(bus, {'week': 0, 'month': 0, 'year': 0})\n",
        "    update_data.append([\n",
        "        non_op_days['week'],\n",
        "        non_op_days['month'],\n",
        "        non_op_days['year']\n",
        "    ])\n",
        "\n",
        "# Update worksheet in batches\n",
        "if update_data:\n",
        "    MAX_ROWS_PER_UPDATE = 100  # Adjust based on API limits\n",
        "    row_chunks = chunk_list(update_data, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AH{start_row}:AJ{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        # Small delay between batch updates\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(update_data)} rows updated successfully in Report_EO1!\")\n",
        "else:\n",
        "    print(\"❌ No updates made to Report_EO1.\")\n",
        "\n",
        "print(\"🎉 Script execution completed successfully!\")"
      ],
      "metadata": {
        "id": "2AGlsJ_QhokL",
        "outputId": "c9f81103-a029-431d-caee-df00508a7b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing performance optimizations...\n",
            "\n",
            "🧹 STEP 1: Clearing existing data...\n",
            "✅ Cleared columns AH-AJ in rows 2-200\n",
            "\n",
            "📅 STEP 2: Calculating date ranges...\n",
            "Current date: 16,May25\n",
            "Last week: 09,May25\n",
            "Last month: 16,Apr25\n",
            "Last year: 16,May24\n",
            "\n",
            "🚌 STEP 3: Processing bus data...\n",
            "🔎 Total buses to process: 83\n",
            "Sample buses: ['587', '475', '515', '486', '999']\n",
            "✅ Processed 587: 0w/0m/0y\n",
            "✅ Processed 515: 0w/0m/0y\n",
            "✅ Processed 486: 0w/0m/0y\n",
            "✅ Processed 475: 0w/0m/0y\n",
            "✅ Processed 578: 0w/0m/0y\n",
            "✅ Processed 440: 0w/0m/0y\n",
            "✅ Processed 508: 0w/0m/0y\n",
            "✅ Processed 999: 0w/0m/0y\n",
            "✅ Processed 471: 0w/0m/0y\n",
            "✅ Processed 561: 0w/0m/0y\n",
            "Progress: 10/83 buses processed (12.0%)\n",
            "✅ Processed 463: 0w/0m/0y\n",
            "✅ Processed 613: 0w/0m/0y\n",
            "✅ Processed 604: 0w/0m/0y\n",
            "✅ Processed 460: 0w/0m/0y\n",
            "✅ Processed 462: 0w/0m/0y\n",
            "✅ Processed 529: 0w/0m/0y\n",
            "✅ Processed 437: 0w/0m/0y\n",
            "✅ Processed 563: 0w/0m/0y\n",
            "✅ Processed 489: 0w/0m/0y\n",
            "✅ Processed 493: 0w/0m/0y\n",
            "Progress: 20/83 buses processed (24.1%)\n",
            "✅ Processed 521: 0w/0m/0y\n",
            "✅ Processed 444: 0w/0m/0y\n",
            "✅ Processed 596: 0w/0m/0y\n",
            "✅ Processed 534: 0w/0m/0y\n",
            "✅ Processed 538: 0w/0m/0y\n",
            "✅ Processed 614: 0w/0m/0y\n",
            "✅ Processed 536: 0w/0m/0y\n",
            "✅ Processed 535: 0w/0m/0y\n",
            "✅ Processed 546: 0w/0m/0y\n",
            "✅ Processed 990: 0w/0m/0y\n",
            "Progress: 30/83 buses processed (36.1%)\n",
            "✅ Processed 617: 0w/0m/0y\n",
            "✅ Processed 615: 0w/0m/0y\n",
            "✅ Processed 589: 0w/0m/0y\n",
            "✅ Processed 473: 0w/0m/0y\n",
            "✅ Processed 595: 0w/0m/0y\n",
            "✅ Processed 573: 0w/0m/0y\n",
            "✅ Processed 472: 0w/0m/0y\n",
            "✅ Processed 459: 0w/0m/0y\n",
            "✅ Processed 577: 0w/0m/0y\n",
            "✅ Processed 518: 0w/0m/0y\n",
            "Progress: 40/83 buses processed (48.2%)\n",
            "✅ Processed 547: 0w/0m/0y\n",
            "✅ Processed 548: 0w/0m/0y\n",
            "✅ Processed 560: 0w/0m/0y\n",
            "✅ Processed 442: 0w/0m/0y\n",
            "✅ Processed 616: 0w/0m/0y\n",
            "✅ Processed 504: 0w/0m/0y\n",
            "✅ Processed 457: 0w/0m/0y\n",
            "✅ Processed 479: 0w/0m/0y\n",
            "✅ Processed 441: 0w/0m/0y\n",
            "✅ Processed 572: 0w/0m/0y\n",
            "Progress: 50/83 buses processed (60.2%)\n",
            "✅ Processed 476: 0w/0m/0y\n",
            "✅ Processed 588: 0w/0m/0y\n",
            "✅ Processed 528: 0w/0m/0y\n",
            "✅ Processed 593: 0w/0m/0y\n",
            "✅ Processed 612: 0w/0m/0y\n",
            "✅ Processed 598: 0w/0m/0y\n",
            "✅ Processed 461: 0w/0m/0y\n",
            "✅ Processed 474: 0w/0m/0y\n",
            "✅ Processed 590: 0w/0m/0y\n",
            "✅ Processed 970: 0w/0m/0y\n",
            "Progress: 60/83 buses processed (72.3%)\n",
            "✅ Processed 516: 0w/0m/0y\n",
            "✅ Processed 575: 0w/0m/0y\n",
            "✅ Processed 495: 0w/0m/0y\n",
            "✅ Processed 574: 0w/0m/0y\n",
            "✅ Processed 478: 0w/0m/0y\n",
            "✅ Processed 438: 0w/0m/0y\n",
            "✅ Processed 599: 0w/0m/0y\n",
            "✅ Processed 562: 0w/0m/0y\n",
            "✅ Processed 550: 0w/0m/0y\n",
            "✅ Processed 507: 0w/0m/0y\n",
            "Progress: 70/83 buses processed (84.3%)\n",
            "✅ Processed 592: 0w/0m/0y\n",
            "✅ Processed 597: 0w/0m/0y\n",
            "✅ Processed 523: 0w/0m/0y\n",
            "✅ Processed 584: 0w/0m/0y\n",
            "✅ Processed 469: 0w/0m/0y\n",
            "✅ Processed 511: 0w/0m/0y\n",
            "✅ Processed 564: 0w/0m/0y\n",
            "✅ Processed 605: 0w/0m/0y\n",
            "✅ Processed 585: 0w/0m/0y\n",
            "✅ Processed 980: 0w/0m/0y\n",
            "Progress: 80/83 buses processed (96.4%)\n",
            "✅ Processed 530: 0w/0m/0y\n",
            "✅ Processed 514: 0w/0m/0y\n",
            "✅ Processed 603: 0w/0m/0y\n",
            "Progress: 83/83 buses processed (100.0%)\n",
            "✅ Completed processing all 83 buses\n",
            "\n",
            "📝 STEP 4: Updating report worksheet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-bbc726604f3f>:284: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(range_to_update, chunk)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated rows 2-84 in Report_EO1\n",
            "✅ All 83 rows updated successfully in Report_EO1!\n",
            "🎉 Script execution completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R7\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "# === MAIN SCRIPT ===\n",
        "\n",
        "# === STEP 1: CLEAR DATA ===\n",
        "print(\"\\n🧹 STEP 1: Clearing existing data...\")\n",
        "clear_range = ['AH2:AJ200']\n",
        "Report_EO1_Worksheet.batch_clear(clear_range)\n",
        "print(\"✅ Cleared columns AH-AJ in rows 2-200\")\n",
        "\n",
        "# === STEP 2: GET DATE RANGES ===\n",
        "print(\"\\n📅 STEP 2: Calculating date ranges...\")\n",
        "current_date = datetime.now()\n",
        "date_formats = ['%d,%b%y', '%d-%b-%y', '%d/%b/%y', '%d,%b %y', '%d %b %y']  # More date formats\n",
        "\n",
        "# Calculate reference dates\n",
        "week_ago = current_date - timedelta(days=7)\n",
        "month_ago = current_date - timedelta(days=30)\n",
        "year_ago = current_date - timedelta(days=365)\n",
        "\n",
        "print(f\"Current date: {current_date.strftime('%d,%b%y')}\")\n",
        "print(f\"Last week: {week_ago.strftime('%d,%b%y')}\")\n",
        "print(f\"Last month: {month_ago.strftime('%d,%b%y')}\")\n",
        "print(f\"Last year: {year_ago.strftime('%d,%b%y')}\")\n",
        "\n",
        "# === STEP 3: PROCESS BUS DATA ===\n",
        "print(\"\\n🚌 STEP 3: Processing bus data...\")\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:  # Make sure row has at least 2 columns\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()  # Clean bus name\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])  # Log first 5 buses\n",
        "\n",
        "# Initialize result storage with thread safety\n",
        "non_op_days_map = defaultdict(lambda: {'week': 0, 'month': 0, 'year': 0})\n",
        "map_lock = threading.Lock()\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Try parsing date with multiple formats\"\"\"\n",
        "    date_str = date_str.strip().replace(' ', '')  # Clean the date string\n",
        "    for fmt in date_formats:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt).date()\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def get_bus_odometer_data(bus):\n",
        "    \"\"\"Retrieve and clean odometer data for a bus\"\"\"\n",
        "    try:\n",
        "        # Try to find the bus in all spreadsheets\n",
        "        for spreadsheet in odometer_spreadsheets:\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(bus)\n",
        "                data = sheet.get_all_values()\n",
        "                if data and len(data) > 1:  # Has at least header + 1 row\n",
        "                    clean_data = []\n",
        "                    for row in data[1:]:  # Skip header\n",
        "                        if len(row) >= 13:  # Ensure row has column M\n",
        "                            date_str = row[1].strip()   # Column B (date)\n",
        "                            odo_str = row[12].strip()  # Column M (odometer)\n",
        "\n",
        "                            # Clean odometer value (remove commas, etc.)\n",
        "                            odo_str = ''.join(c for c in odo_str if c.isdigit())\n",
        "\n",
        "                            if date_str and odo_str and odo_str.isdigit():\n",
        "                                date = parse_date(date_str)\n",
        "                                if date:\n",
        "                                    clean_data.append((date, int(odo_str)))\n",
        "                    return clean_data\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Worksheet error for bus {bus}: {str(e)}\")\n",
        "                continue\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error getting data for bus {bus}: {e}\")\n",
        "        return []\n",
        "\n",
        "def calculate_non_operating_days(bus_data, current_date):\n",
        "    \"\"\"Calculate non-operating days for different periods\"\"\"\n",
        "    if not bus_data or len(bus_data) < 2:\n",
        "        return {'week': 0, 'month': 0, 'year': 0}\n",
        "\n",
        "    # Convert current_date to date object for comparison\n",
        "    current_date = current_date.date()\n",
        "    week_ago_date = (current_date - timedelta(days=7))\n",
        "    month_ago_date = (current_date - timedelta(days=30))\n",
        "    year_ago_date = (current_date - timedelta(days=365))\n",
        "\n",
        "    # Sort data by date\n",
        "    bus_data.sort()\n",
        "\n",
        "    # Initialize counters\n",
        "    non_op_days = {'week': 0, 'month': 0, 'year': 0}\n",
        "\n",
        "    # Method 1: Same odometer on consecutive days\n",
        "    for i in range(1, len(bus_data)):\n",
        "        prev_date, prev_odo = bus_data[i-1]\n",
        "        curr_date, curr_odo = bus_data[i]\n",
        "\n",
        "        if curr_odo == prev_odo:\n",
        "            # The day after prev_date is non-operating\n",
        "            non_op_date = prev_date + timedelta(days=1)\n",
        "\n",
        "            # Only count if it's before current_date and matches curr_date\n",
        "            if non_op_date < curr_date and non_op_date <= current_date:\n",
        "                # Check which periods this non-op day falls into\n",
        "                if non_op_date >= week_ago_date:\n",
        "                    non_op_days['week'] += 1\n",
        "                if non_op_date >= month_ago_date:\n",
        "                    non_op_days['month'] += 1\n",
        "                if non_op_date >= year_ago_date:\n",
        "                    non_op_days['year'] += 1\n",
        "\n",
        "    # Method 2: Missing dates between entries\n",
        "    all_dates = [d for d, _ in bus_data]\n",
        "    for i in range(1, len(all_dates)):\n",
        "        prev_date = all_dates[i-1]\n",
        "        curr_date = all_dates[i]\n",
        "        days_diff = (curr_date - prev_date).days\n",
        "\n",
        "        if days_diff > 1:\n",
        "            # For each missing day between entries\n",
        "            for day in range(1, days_diff):\n",
        "                missing_date = prev_date + timedelta(days=day)\n",
        "                if missing_date > current_date:\n",
        "                    continue\n",
        "\n",
        "                # Check which periods this missing day falls into\n",
        "                if missing_date >= week_ago_date:\n",
        "                    non_op_days['week'] += 1\n",
        "                if missing_date >= month_ago_date:\n",
        "                    non_op_days['month'] += 1\n",
        "                if missing_date >= year_ago_date:\n",
        "                    non_op_days['year'] += 1\n",
        "\n",
        "    return non_op_days\n",
        "\n",
        "def process_bus(bus):\n",
        "    \"\"\"Process a single bus to calculate non-operating days\"\"\"\n",
        "    try:\n",
        "        # Get odometer data for this bus\n",
        "        bus_data = get_bus_odometer_data(bus)\n",
        "\n",
        "        if not bus_data:\n",
        "            print(f\"⚠️ No valid odometer data found for {bus}\")\n",
        "            return False\n",
        "\n",
        "        # Calculate non-operating days\n",
        "        non_op_days = calculate_non_operating_days(bus_data, current_date)\n",
        "\n",
        "        # Store results\n",
        "        with map_lock:\n",
        "            non_op_days_map[bus] = non_op_days\n",
        "\n",
        "        print(f\"✅ {bus}: {non_op_days['week']}w/{non_op_days['month']}m/{non_op_days['year']}y (Total entries: {len(bus_data)})\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Process buses in parallel\n",
        "MAX_WORKERS = 5\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    \"\"\"Process buses in parallel batches\"\"\"\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(process_bus, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        # Small delay between batches\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "# Process all buses in batches\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# === STEP 4: UPDATE REPORT ===\n",
        "print(\"\\n📝 STEP 4: Updating report worksheet...\")\n",
        "\n",
        "# Prepare update data\n",
        "update_data = []\n",
        "for row_idx, bus in row_map.items():\n",
        "    non_op_days = non_op_days_map.get(bus, {'week': 0, 'month': 0, 'year': 0})\n",
        "    update_data.append([\n",
        "        non_op_days['week'],\n",
        "        non_op_days['month'],\n",
        "        non_op_days['year']\n",
        "    ])\n",
        "\n",
        "# Update worksheet in batches\n",
        "if update_data:\n",
        "    MAX_ROWS_PER_UPDATE = 100\n",
        "    row_chunks = chunk_list(update_data, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AH{start_row}:AJ{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row} in Report_EO1\")\n",
        "\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(update_data)} rows updated successfully!\")\n",
        "else:\n",
        "    print(\"❌ No updates made - check if data was processed correctly\")\n",
        "\n",
        "print(\"🎉 Script execution completed successfully!\")"
      ],
      "metadata": {
        "id": "JrGa-C58i5N5",
        "outputId": "411dc753-f27d-49b9-c117-d2d9a1e62127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing performance optimizations...\n",
            "\n",
            "🧹 STEP 1: Clearing existing data...\n",
            "✅ Cleared columns AH-AJ in rows 2-200\n",
            "\n",
            "📅 STEP 2: Calculating date ranges...\n",
            "Current date: 16,May25\n",
            "Last week: 09,May25\n",
            "Last month: 16,Apr25\n",
            "Last year: 16,May24\n",
            "\n",
            "🚌 STEP 3: Processing bus data...\n",
            "🔎 Total buses to process: 83\n",
            "Sample buses: ['587', '475', '515', '486', '999']\n",
            "⚠️ Worksheet error for bus 999: 999\n",
            "✅ 587: 4w/14m/210y (Total entries: 177)\n",
            "✅ 475: 0w/0m/257y (Total entries: 136)\n",
            "✅ 515: 0w/0m/93y (Total entries: 389)\n",
            "⚠️ Worksheet error for bus 999: 999\n",
            "✅ 486: 0w/0m/247y (Total entries: 140)\n",
            "✅ 578: 3w/15m/221y (Total entries: 199)\n",
            "⚠️ Worksheet error for bus 561: 561\n",
            "✅ 508: 6w/14m/203y (Total entries: 281)\n",
            "✅ 440: 8w/26m/198y (Total entries: 301)\n",
            "✅ 561: 0w/0m/59y (Total entries: 607)\n",
            "✅ 471: 3w/11m/229y (Total entries: 183)\n",
            "✅ 999: 0w/0m/77y (Total entries: 362)\n",
            "Progress: 10/83 buses processed (12.0%)\n",
            "✅ 604: 3w/15m/201y (Total entries: 235)\n",
            "✅ 613: 0w/0m/46y (Total entries: 359)\n",
            "⚠️ Worksheet error for bus 463: 463\n",
            "✅ 462: 6w/29m/173y (Total entries: 449)\n",
            "✅ 460: 8w/31m/195y (Total entries: 387)\n",
            "⚠️ Worksheet error for bus 437: 437\n",
            "⚠️ Worksheet error for bus 493: 493\n",
            "✅ 463: 5w/27m/362y (Total entries: 5)\n",
            "⚠️ Worksheet error for bus 529: 529\n",
            "✅ 493: 4w/12m/210y (Total entries: 457)\n",
            "✅ 437: 0w/0m/124y (Total entries: 1071)\n",
            "✅ 489: 0w/0m/0y (Total entries: 1)\n",
            "✅ 529: 3w/26m/84y (Total entries: 1218)\n",
            "✅ 563: 2w/16m/223y (Total entries: 219)\n",
            "Progress: 20/83 buses processed (24.1%)\n",
            "✅ 596: 3w/16m/210y (Total entries: 159)\n",
            "✅ 538: 1w/7m/162y (Total entries: 219)\n",
            "✅ 521: 3w/13m/214y (Total entries: 249)\n",
            "✅ 534: 2w/7m/142y (Total entries: 473)\n",
            "✅ 444: 2w/8m/255y (Total entries: 168)\n",
            "⚠️ Worksheet error for bus 990: 990\n",
            "✅ 535: 2w/7m/89y (Total entries: 397)\n",
            "✅ 536: 1w/13m/125y (Total entries: 277)\n",
            "✅ 614: 0w/3m/133y (Total entries: 308)\n",
            "✅ 546: 2w/14m/196y (Total entries: 225)\n",
            "⚠️ Worksheet error for bus 990: 990\n",
            "⚠️ Worksheet error for bus 990: 990\n",
            "✅ 990: 8w/31m/267y (Total entries: 139)\n",
            "Progress: 30/83 buses processed (36.1%)\n",
            "⚠️ Worksheet error for bus 615: 615\n",
            "⚠️ Worksheet error for bus 617: 617\n",
            "✅ 473: 0w/0m/199y (Total entries: 146)\n",
            "✅ 617: 0w/0m/0y (Total entries: 985)\n",
            "✅ 615: 4w/13m/13y (Total entries: 857)\n",
            "✅ 589: 0w/0m/184y (Total entries: 172)\n",
            "⚠️ Worksheet error for bus 573: 573\n",
            "✅ 595: 0w/12m/191y (Total entries: 240)\n",
            "✅ 573: 0w/0m/1y (Total entries: 1491)\n",
            "✅ 518: 0w/0m/0y (Total entries: 1)\n",
            "✅ 472: 7w/13m/147y (Total entries: 358)\n",
            "✅ 459: 0w/5m/290y (Total entries: 88)\n",
            "✅ 577: 3w/11m/168y (Total entries: 237)\n",
            "Progress: 40/83 buses processed (48.2%)\n",
            "⚠️ Worksheet error for bus 616: 616\n",
            "✅ 442: 1w/13m/244y (Total entries: 173)\n",
            "✅ 548: 3w/13m/209y (Total entries: 225)\n",
            "⚠️ Worksheet error for bus 560: 560\n",
            "✅ 616: 0w/0m/0y (Total entries: 987)\n",
            "⚠️ Worksheet error for bus 479: 479\n",
            "✅ 547: 5w/17m/270y (Total entries: 143)\n",
            "✅ 560: 0w/0m/79y (Total entries: 1052)\n",
            "✅ 504: 6w/21m/319y (Total entries: 104)\n",
            "✅ 479: 0w/0m/3y (Total entries: 1460)\n",
            "⚠️ Worksheet error for bus 572: 572\n",
            "✅ 457: 2w/12m/219y (Total entries: 250)\n",
            "✅ 441: 8w/31m/267y (Total entries: 145)\n",
            "✅ 572: 2w/2m/6y (Total entries: 308)\n",
            "Progress: 50/83 buses processed (60.2%)\n",
            "⚠️ Worksheet error for bus 970: 970\n",
            "⚠️ Worksheet error for bus 593: 593\n",
            "⚠️ Worksheet error for bus 970: 970\n",
            "✅ 528: 0w/4m/227y (Total entries: 203)\n",
            "✅ 588: 0w/2m/116y (Total entries: 292)\n",
            "⚠️ Worksheet error for bus 970: 970\n",
            "✅ 593: 0w/0m/0y (Total entries: 1495)\n",
            "⚠️ Worksheet error for bus 970: 970\n",
            "✅ 476: 2w/11m/172y (Total entries: 344)\n",
            "⚠️ Worksheet error for bus 461: 461\n",
            "✅ 598: 2w/11m/210y (Total entries: 160)\n",
            "✅ 612: 2w/6m/114y (Total entries: 463)\n",
            "✅ 461: 0w/6m/6y (Total entries: 1472)\n",
            "✅ 474: 5w/19m/256y (Total entries: 250)\n",
            "⚠️ Worksheet error for bus 970: 970\n",
            "✅ 590: 0w/12m/199y (Total entries: 238)\n",
            "✅ 970: 8w/31m/202y (Total entries: 294)\n",
            "Progress: 60/83 buses processed (72.3%)\n",
            "⚠️ Worksheet error for bus 478: 478\n",
            "⚠️ Worksheet error for bus 516: 516\n",
            "✅ 478: 0w/0m/53y (Total entries: 1286)\n",
            "✅ 516: 0w/0m/18y (Total entries: 1420)\n",
            "✅ 495: 2w/4m/128y (Total entries: 365)\n",
            "⚠️ Worksheet error for bus 438: 438\n",
            "✅ 574: 0w/0m/112y (Total entries: 161)\n",
            "⚠️ Worksheet error for bus 562: 562\n",
            "✅ 575: 0w/0m/114y (Total entries: 157)\n",
            "⚠️ Worksheet error for bus 507: 507\n",
            "✅ 438: 0w/0m/0y (Total entries: 1498)\n",
            "✅ 599: 3w/12m/189y (Total entries: 155)\n",
            "✅ 562: 0w/2m/184y (Total entries: 351)\n",
            "✅ 507: 0w/0m/5y (Total entries: 1458)\n",
            "✅ 550: 8w/31m/288y (Total entries: 178)\n",
            "Progress: 70/83 buses processed (84.3%)\n",
            "⚠️ Worksheet error for bus 592: 592\n",
            "✅ 469: 0w/11m/209y (Total entries: 350)\n",
            "✅ 592: 0w/0m/0y (Total entries: 1381)\n",
            "✅ 597: 2w/16m/207y (Total entries: 167)\n",
            "✅ 523: 8w/27m/258y (Total entries: 208)\n",
            "✅ 584: 0w/0m/74y (Total entries: 74)\n",
            "⚠️ Worksheet error for bus 980: 980\n",
            "✅ 511: 0w/15m/162y (Total entries: 211)\n",
            "✅ 605: 2w/10m/172y (Total entries: 269)\n",
            "✅ 564: 0w/0m/0y (Total entries: 62)\n",
            "⚠️ Worksheet error for bus 980: 980\n",
            "✅ 585: 1w/3m/74y (Total entries: 401)\n",
            "⚠️ Worksheet error for bus 980: 980\n",
            "⚠️ Worksheet error for bus 980: 980\n",
            "✅ 980: 8w/31m/170y (Total entries: 321)\n",
            "Progress: 80/83 buses processed (96.4%)\n",
            "✅ 530: 4w/15m/277y (Total entries: 150)\n",
            "✅ 603: 4w/14m/170y (Total entries: 338)\n",
            "✅ 514: 3w/9m/215y (Total entries: 250)\n",
            "Progress: 83/83 buses processed (100.0%)\n",
            "✅ Completed processing all 83 buses\n",
            "\n",
            "📝 STEP 4: Updating report worksheet...\n",
            "✅ Updated rows 2-84 in Report_EO1\n",
            "✅ All 83 rows updated successfully!\n",
            "🎉 Script execution completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-7132d516e07f>:250: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(range_to_update, chunk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now getting dates in AK, AL, AM columns"
      ],
      "metadata": {
        "id": "T9jcFIRAmHTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === PERFORMANCE OPTIMIZATION SETUP ===\n",
        "print(\"🚀 Initializing performance optimizations...\")\n",
        "\n",
        "# Cache for worksheet data to reduce API calls\n",
        "data_cache = {}\n",
        "cache_lock = threading.Lock()\n",
        "\n",
        "def chunk_list(lst, chunk_size):\n",
        "    \"\"\"Split list into chunks of specified size\"\"\"\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "# === MAIN SCRIPT ===\n",
        "\n",
        "# === STEP 1: CLEAR DATA ===\n",
        "print(\"\\n🧹 STEP 1: Clearing existing data...\")\n",
        "clear_range = ['AH2:AM200']  # Now clearing through column AM\n",
        "Report_EO1_Worksheet.batch_clear(clear_range)\n",
        "print(\"✅ Cleared columns AH-AM in rows 2-200\")\n",
        "\n",
        "# === STEP 2: GET DATE RANGES ===\n",
        "print(\"\\n📅 STEP 2: Calculating date ranges...\")\n",
        "current_date = datetime.now()\n",
        "date_formats = ['%d,%b%y', '%d-%b-%y', '%d/%b/%y', '%d,%b %y', '%d %b %y']\n",
        "\n",
        "# Calculate reference dates\n",
        "week_ago = current_date - timedelta(days=7)\n",
        "month_ago = current_date - timedelta(days=30)\n",
        "year_ago = current_date - timedelta(days=365)\n",
        "\n",
        "print(f\"Current date: {current_date.strftime('%d,%b%y')}\")\n",
        "print(f\"Last week: {week_ago.strftime('%d,%b%y')}\")\n",
        "print(f\"Last month: {month_ago.strftime('%d,%b%y')}\")\n",
        "print(f\"Last year: {year_ago.strftime('%d,%b%y')}\")\n",
        "\n",
        "# === STEP 3: PROCESS BUS DATA ===\n",
        "print(\"\\n🚌 STEP 3: Processing bus data...\")\n",
        "\n",
        "# Get main data from Report_EO1\n",
        "main_data = Report_EO1_Worksheet.get_all_values()\n",
        "header, rows = main_data[0], main_data[1:]\n",
        "\n",
        "# Map each row to bus\n",
        "row_map = {}\n",
        "bus_set = set()\n",
        "for i, row in enumerate(rows, start=2):\n",
        "    try:\n",
        "        if len(row) > 1:\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "                bus_set.add(bus)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"🔎 Total buses to process: {len(bus_set)}\")\n",
        "if bus_set:\n",
        "    print(\"Sample buses:\", list(bus_set)[:5])\n",
        "\n",
        "# Initialize result storage with thread safety\n",
        "non_op_days_map = defaultdict(lambda: {\n",
        "    'week': {'count': 0, 'dates': []},\n",
        "    'month': {'count': 0, 'dates': []},\n",
        "    'year': {'count': 0, 'dates': []}\n",
        "})\n",
        "map_lock = threading.Lock()\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Try parsing date with multiple formats\"\"\"\n",
        "    date_str = date_str.strip().replace(' ', '')\n",
        "    for fmt in date_formats:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt).date()\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def format_date_list(dates):\n",
        "    \"\"\"Format list of dates as semicolon-separated string\"\"\"\n",
        "    return \";\".join(d.strftime('%d,%b%y') for d in sorted(dates))\n",
        "\n",
        "def get_bus_odometer_data(bus):\n",
        "    \"\"\"Retrieve and clean odometer data for a bus\"\"\"\n",
        "    try:\n",
        "        for spreadsheet in odometer_spreadsheets:\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(bus)\n",
        "                data = sheet.get_all_values()\n",
        "                if data and len(data) > 1:\n",
        "                    clean_data = []\n",
        "                    for row in data[1:]:\n",
        "                        if len(row) >= 13:\n",
        "                            date_str = row[1].strip()\n",
        "                            odo_str = row[12].strip()\n",
        "                            odo_str = ''.join(c for c in odo_str if c.isdigit())\n",
        "                            if date_str and odo_str and odo_str.isdigit():\n",
        "                                date = parse_date(date_str)\n",
        "                                if date:\n",
        "                                    clean_data.append((date, int(odo_str)))\n",
        "                    return clean_data\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error getting data for bus {bus}: {e}\")\n",
        "        return []\n",
        "\n",
        "def calculate_non_operating_days(bus_data, current_date):\n",
        "    \"\"\"Calculate non-operating days with actual dates\"\"\"\n",
        "    result = {\n",
        "        'week': {'count': 0, 'dates': []},\n",
        "        'month': {'count': 0, 'dates': []},\n",
        "        'year': {'count': 0, 'dates': []}\n",
        "    }\n",
        "\n",
        "    if not bus_data or len(bus_data) < 2:\n",
        "        return result\n",
        "\n",
        "    current_date = current_date.date()\n",
        "    week_ago_date = (current_date - timedelta(days=7))\n",
        "    month_ago_date = (current_date - timedelta(days=30))\n",
        "    year_ago_date = (current_date - timedelta(days=365))\n",
        "\n",
        "    bus_data.sort()\n",
        "\n",
        "    # Method 1: Same odometer on consecutive days\n",
        "    for i in range(1, len(bus_data)):\n",
        "        prev_date, prev_odo = bus_data[i-1]\n",
        "        curr_date, curr_odo = bus_data[i]\n",
        "\n",
        "        if curr_odo == prev_odo:\n",
        "            non_op_date = prev_date + timedelta(days=1)\n",
        "            if non_op_date < curr_date and non_op_date <= current_date:\n",
        "                # Add to appropriate periods\n",
        "                if non_op_date >= week_ago_date:\n",
        "                    result['week']['count'] += 1\n",
        "                    result['week']['dates'].append(non_op_date)\n",
        "                if non_op_date >= month_ago_date:\n",
        "                    result['month']['count'] += 1\n",
        "                    result['month']['dates'].append(non_op_date)\n",
        "                if non_op_date >= year_ago_date:\n",
        "                    result['year']['count'] += 1\n",
        "                    result['year']['dates'].append(non_op_date)\n",
        "\n",
        "    # Method 2: Missing dates between entries\n",
        "    all_dates = [d for d, _ in bus_data]\n",
        "    for i in range(1, len(all_dates)):\n",
        "        prev_date = all_dates[i-1]\n",
        "        curr_date = all_dates[i]\n",
        "        days_diff = (curr_date - prev_date).days\n",
        "\n",
        "        if days_diff > 1:\n",
        "            for day in range(1, days_diff):\n",
        "                missing_date = prev_date + timedelta(days=day)\n",
        "                if missing_date > current_date:\n",
        "                    continue\n",
        "\n",
        "                if missing_date >= week_ago_date:\n",
        "                    result['week']['count'] += 1\n",
        "                    result['week']['dates'].append(missing_date)\n",
        "                if missing_date >= month_ago_date:\n",
        "                    result['month']['count'] += 1\n",
        "                    result['month']['dates'].append(missing_date)\n",
        "                if missing_date >= year_ago_date:\n",
        "                    result['year']['count'] += 1\n",
        "                    result['year']['dates'].append(missing_date)\n",
        "\n",
        "    # Remove duplicates and sort dates\n",
        "    for period in result:\n",
        "        result[period]['dates'] = sorted(list(set(result[period]['dates'])))\n",
        "\n",
        "    return result\n",
        "\n",
        "def process_bus(bus):\n",
        "    \"\"\"Process a single bus to calculate non-operating days\"\"\"\n",
        "    try:\n",
        "        bus_data = get_bus_odometer_data(bus)\n",
        "\n",
        "        if not bus_data:\n",
        "            print(f\"⚠️ No valid odometer data found for {bus}\")\n",
        "            return False\n",
        "\n",
        "        non_op_days = calculate_non_operating_days(bus_data, current_date)\n",
        "\n",
        "        with map_lock:\n",
        "            non_op_days_map[bus] = non_op_days\n",
        "\n",
        "        print(f\"✅ {bus}: Week({non_op_days['week']['count']}): {format_date_list(non_op_days['week']['dates'])}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Process buses in parallel\n",
        "MAX_WORKERS = 5\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "def process_buses_in_batches(buses):\n",
        "    total = len(buses)\n",
        "    completed = 0\n",
        "\n",
        "    bus_chunks = chunk_list(list(buses), BATCH_SIZE)\n",
        "\n",
        "    for chunk in bus_chunks:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(chunk))) as executor:\n",
        "            futures = {executor.submit(process_bus, bus): bus for bus in chunk}\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                bus = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    completed += 1\n",
        "                    if completed % 10 == 0 or completed == total:\n",
        "                        print(f\"Progress: {completed}/{total} buses processed ({completed/total:.1%})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing bus {bus}: {e}\")\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"✅ Completed processing all {total} buses\")\n",
        "\n",
        "if bus_set:\n",
        "    process_buses_in_batches(bus_set)\n",
        "\n",
        "# === STEP 4: UPDATE REPORT ===\n",
        "print(\"\\n📝 STEP 4: Updating report worksheet...\")\n",
        "\n",
        "update_data = []\n",
        "for row_idx, bus in row_map.items():\n",
        "    non_op_days = non_op_days_map.get(bus, {\n",
        "        'week': {'count': 0, 'dates': []},\n",
        "        'month': {'count': 0, 'dates': []},\n",
        "        'year': {'count': 0, 'dates': []}\n",
        "    })\n",
        "\n",
        "    update_data.append([\n",
        "        non_op_days['week']['count'],\n",
        "        non_op_days['month']['count'],\n",
        "        non_op_days['year']['count'],\n",
        "        format_date_list(non_op_days['week']['dates']),\n",
        "        format_date_list(non_op_days['month']['dates']),\n",
        "        format_date_list(non_op_days['year']['dates'])\n",
        "    ])\n",
        "\n",
        "if update_data:\n",
        "    MAX_ROWS_PER_UPDATE = 100\n",
        "    row_chunks = chunk_list(update_data, MAX_ROWS_PER_UPDATE)\n",
        "\n",
        "    for i, chunk in enumerate(row_chunks):\n",
        "        start_row = 2 + i * MAX_ROWS_PER_UPDATE\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        range_to_update = f\"AH{start_row}:AM{end_row}\"\n",
        "\n",
        "        Report_EO1_Worksheet.update(range_to_update, chunk)\n",
        "        print(f\"✅ Updated rows {start_row}-{end_row}\")\n",
        "\n",
        "        if i < len(row_chunks) - 1:\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"✅ All {len(update_data)} rows updated successfully!\")\n",
        "else:\n",
        "    print(\"❌ No updates made - check if data was processed correctly\")\n",
        "\n",
        "print(\"🎉 Script execution completed successfully!\")"
      ],
      "metadata": {
        "id": "c_PgrC7AmGga",
        "outputId": "c8504ab6-5733-4fde-bad3-f287d734cd77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing performance optimizations...\n",
            "\n",
            "🧹 STEP 1: Clearing existing data...\n",
            "✅ Cleared columns AH-AM in rows 2-200\n",
            "\n",
            "📅 STEP 2: Calculating date ranges...\n",
            "Current date: 16,May25\n",
            "Last week: 09,May25\n",
            "Last month: 16,Apr25\n",
            "Last year: 16,May24\n",
            "\n",
            "🚌 STEP 3: Processing bus data...\n",
            "🔎 Total buses to process: 83\n",
            "Sample buses: ['587', '475', '515', '486', '999']\n",
            "✅ 486: Week(0): ✅ 587: Week(4): 10,May25;12,May25;15,May25;16,May25\n",
            "\n",
            "✅ 475: Week(0): \n",
            "✅ 515: Week(0): \n",
            "✅ 440: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "✅ 578: Week(3): 10,May25;11,May25;13,May25\n",
            "✅ 508: Week(6): 09,May25;10,May25;12,May25;14,May25;15,May25;16,May25\n",
            "✅ 561: Week(0): \n",
            "✅ 471: Week(3): 11,May25;14,May25;15,May25\n",
            "✅ 999: Week(0): \n",
            "Progress: 10/83 buses processed (12.0%)\n",
            "✅ 462: Week(6): 09,May25;10,May25;11,May25;12,May25;13,May25;15,May25\n",
            "✅ 460: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "✅ 604: Week(3): 09,May25;10,May25;11,May25\n",
            "✅ 463: Week(5): 09,May25;10,May25;11,May25;12,May25\n",
            "✅ 613: Week(0): \n",
            "✅ 529: Week(3): 09,May25;10,May25;11,May25\n",
            "✅ 437: Week(0): \n",
            "✅ 493: Week(4): 09,May25;10,May25;11,May25;12,May25\n",
            "✅ 489: Week(0): \n",
            "✅ 563: Week(2): 13,May25;14,May25\n",
            "Progress: 20/83 buses processed (24.1%)\n",
            "✅ 444: Week(2): 11,May25;12,May25\n",
            "✅ 596: Week(3): 11,May25;12,May25;13,May25\n",
            "✅ 521: Week(3): 09,May25;10,May25;12,May25\n",
            "✅ 534: Week(2): 12,May25;14,May25\n",
            "✅ 538: Week(1): 11,May25\n",
            "✅ 614: Week(0): \n",
            "✅ 536: Week(1): 11,May25\n",
            "✅ 535: Week(2): 10,May25;12,May25\n",
            "✅ 546: Week(2): 10,May25;12,May25\n",
            "✅ 990: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "Progress: 30/83 buses processed (36.1%)\n",
            "✅ 615: Week(4): 09,May25;10,May25;11,May25;12,May25\n",
            "✅ 595: Week(0): \n",
            "✅ 473: Week(0): \n",
            "✅ 589: Week(0): \n",
            "✅ 617: Week(0): \n",
            "✅ 472: Week(7): 09,May25;10,May25;11,May25;12,May25;13,May25;15,May25;16,May25\n",
            "✅ 573: Week(0): \n",
            "✅ 459: Week(0): \n",
            "✅ 518: Week(0): \n",
            "✅ 577: Week(3): 09,May25;11,May25;12,May25\n",
            "Progress: 40/83 buses processed (48.2%)\n",
            "✅ 616: Week(0): \n",
            "✅ 548: Week(3): 10,May25;12,May25;13,May25\n",
            "✅ 547: Week(5): 09,May25;11,May25;13,May25;15,May25;16,May25\n",
            "✅ 442: Week(1): 10,May25\n",
            "✅ 560: Week(0): \n",
            "✅ 479: Week(0): \n",
            "✅ 504: Week(6): 10,May25;11,May25;12,May25;13,May25;15,May25;16,May25\n",
            "✅ 441: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "✅ 457: Week(2): 12,May25;13,May25\n",
            "✅ 572: Week(2): 15,May25;16,May25\n",
            "Progress: 50/83 buses processed (60.2%)\n",
            "✅ 528: Week(0): \n",
            "✅ 588: Week(0): \n",
            "✅ 476: Week(2): 12,May25;16,May25\n",
            "✅ 593: Week(0): \n",
            "✅ 598: Week(2): 10,May25;13,May25\n",
            "✅ 612: Week(2): 09,May25;16,May25\n",
            "✅ 474: Week(5): 09,May25;10,May25;11,May25;12,May25;16,May25\n",
            "✅ 461: Week(0): \n",
            "✅ 590: Week(0): \n",
            "✅ 970: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "Progress: 60/83 buses processed (72.3%)\n",
            "✅ 516: Week(0): \n",
            "✅ 575: Week(0): \n",
            "✅ 495: Week(2): 11,May25;16,May25\n",
            "✅ 574: Week(0): \n",
            "✅ 478: Week(0): \n",
            "✅ 599: Week(3): 09,May25;11,May25;12,May25\n",
            "✅ 550: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "✅ 438: Week(0): \n",
            "✅ 507: Week(0): \n",
            "✅ 562: Week(0): \n",
            "Progress: 70/83 buses processed (84.3%)\n",
            "✅ 584: Week(0): \n",
            "✅ 469: Week(0): \n",
            "✅ 597: Week(2): 09,May25;13,May25\n",
            "✅ 523: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "✅ 592: Week(0): \n",
            "✅ 605: Week(2): 11,May25;13,May25\n",
            "✅ 564: Week(0): \n",
            "✅ 511: Week(0): \n",
            "✅ 585: Week(1): 16,May25\n",
            "✅ 980: Week(8): 09,May25;10,May25;11,May25;12,May25;13,May25;14,May25;15,May25;16,May25\n",
            "Progress: 80/83 buses processed (96.4%)\n",
            "✅ 514: Week(3): 10,May25;11,May25;12,May25\n",
            "✅ 530: Week(4): 09,May25;10,May25;11,May25;12,May25\n",
            "✅ 603: Week(4): 10,May25;12,May25;15,May25;16,May25\n",
            "Progress: 83/83 buses processed (100.0%)\n",
            "✅ Completed processing all 83 buses\n",
            "\n",
            "📝 STEP 4: Updating report worksheet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-afda1858d992>:261: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(range_to_update, chunk)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated rows 2-84\n",
            "✅ All 83 rows updated successfully!\n",
            "🎉 Script execution completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R10\n",
        "\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "DATE_FORMATS = ['%d,%b%y', '%d-%b-%y', '%d/%b/%y', '%d,%b %y', '%d %b %y']\n",
        "MAX_WORKERS = 5\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "# === INITIALIZATION ===\n",
        "print(\"🚀 Initializing non-operating days calculator...\")\n",
        "current_date = datetime.now()\n",
        "\n",
        "# === CORE FUNCTIONS ===\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse date from multiple possible formats\"\"\"\n",
        "    date_str = date_str.strip().replace(' ', '')\n",
        "    for fmt in DATE_FORMATS:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt).date()\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def format_date_list(dates):\n",
        "    \"\"\"Format dates as DD,MonYY separated by semicolons\"\"\"\n",
        "    return \";\".join(d.strftime('%d,%b%y') for d in sorted(dates))\n",
        "\n",
        "def get_bus_odometer_data(bus):\n",
        "    \"\"\"Retrieve odometer readings for a specific bus\"\"\"\n",
        "    try:\n",
        "        for spreadsheet in odometer_spreadsheets:\n",
        "            try:\n",
        "                sheet = spreadsheet.worksheet(bus)\n",
        "                data = sheet.get_all_values()\n",
        "                if data and len(data) > 1:\n",
        "                    clean_data = []\n",
        "                    for row in data[1:]:  # Skip header\n",
        "                        if len(row) >= 13:  # Ensure column M exists\n",
        "                            date_str = row[1].strip()   # Column B (date)\n",
        "                            odo_str = row[12].strip()  # Column M (odometer)\n",
        "                            odo_str = ''.join(c for c in odo_str if c.isdigit())\n",
        "                            if date_str and odo_str and odo_str.isdigit():\n",
        "                                date = parse_date(date_str)\n",
        "                                if date:\n",
        "                                    clean_data.append((date, int(odo_str)))\n",
        "                    return clean_data\n",
        "            except:\n",
        "                continue\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error getting data for {bus}: {e}\")\n",
        "        return []\n",
        "\n",
        "def calculate_non_operating_days(bus_data, current_date):\n",
        "    \"\"\"Accurately identify non-operating days based on odometer changes\"\"\"\n",
        "    current_date = current_date.date()\n",
        "    periods = {\n",
        "        'week': current_date - timedelta(days=7),\n",
        "        'month': current_date - timedelta(days=30),\n",
        "        'year': current_date - timedelta(days=365)\n",
        "    }\n",
        "\n",
        "    results = {p: {'count': 0, 'dates': set()} for p in periods}\n",
        "\n",
        "    if not bus_data or len(bus_data) < 2:\n",
        "        return results\n",
        "\n",
        "    # Sort and remove duplicate dates\n",
        "    bus_data.sort()\n",
        "    unique_data = []\n",
        "    last_date = None\n",
        "    for date, odo in bus_data:\n",
        "        if date != last_date:\n",
        "            unique_data.append((date, odo))\n",
        "            last_date = date\n",
        "\n",
        "    # Find all dates when odometer increased (operating days)\n",
        "    operating_dates = set()\n",
        "    prev_odo = None\n",
        "    for date, odo in unique_data:\n",
        "        if prev_odo is not None and odo > prev_odo:\n",
        "            operating_dates.add(date)\n",
        "        prev_odo = odo\n",
        "\n",
        "    # Generate complete date range to examine\n",
        "    min_date = min(d for d, _ in unique_data)\n",
        "    max_date = max(d for d, _ in unique_data)\n",
        "    all_dates = [min_date + timedelta(days=x)\n",
        "                for x in range((max_date - min_date).days + 1)]\n",
        "\n",
        "    # Classify each date\n",
        "    odometer_readings = {date: odo for date, odo in unique_data}\n",
        "    prev_odo = None\n",
        "\n",
        "    for date in all_dates:\n",
        "        current_odo = odometer_readings.get(date)\n",
        "\n",
        "        # Rule 1: Odometer increased - operating day\n",
        "        if date in operating_dates:\n",
        "            prev_odo = current_odo\n",
        "            continue\n",
        "\n",
        "        # Rule 2: Same odometer or missing date - non-operating\n",
        "        if (current_odo is not None and prev_odo is not None and current_odo == prev_odo) or \\\n",
        "           (date not in odometer_readings):\n",
        "\n",
        "            # Add to appropriate periods\n",
        "            for period, cutoff in periods.items():\n",
        "                if date >= cutoff:\n",
        "                    results[period]['dates'].add(date)\n",
        "\n",
        "        # Update previous odometer if available\n",
        "        if current_odo is not None:\n",
        "            prev_odo = current_odo\n",
        "\n",
        "    # Finalize results\n",
        "    for period in results:\n",
        "        results[period]['dates'] = sorted(results[period]['dates'])\n",
        "        results[period]['count'] = len(results[period]['dates'])\n",
        "\n",
        "    return results\n",
        "\n",
        "# === MAIN EXECUTION ===\n",
        "def main():\n",
        "    print(\"\\n🧹 STEP 1: Clearing existing data...\")\n",
        "    Report_EO1_Worksheet.batch_clear(['AH2:AM200'])\n",
        "    print(\"✅ Cleared columns AH-AM\")\n",
        "\n",
        "    print(\"\\n📅 STEP 2: Loading bus list...\")\n",
        "    main_data = Report_EO1_Worksheet.get_all_values()\n",
        "    row_map = {}\n",
        "    for i, row in enumerate(main_data[1:], start=2):\n",
        "        try:\n",
        "            bus = re.sub(r'[^a-zA-Z0-9]', '', str(row[1]).strip()).upper()\n",
        "            if bus:\n",
        "                row_map[i] = bus\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error processing row {i}: {e}\")\n",
        "\n",
        "    print(f\"🔍 Found {len(row_map)} buses to process\")\n",
        "\n",
        "    print(\"\\n🚌 STEP 3: Calculating non-operating days...\")\n",
        "    non_op_days_map = defaultdict(dict)\n",
        "    lock = threading.Lock()\n",
        "\n",
        "    def process_bus(bus, row_idx):\n",
        "        try:\n",
        "            bus_data = get_bus_odometer_data(bus)\n",
        "            if not bus_data:\n",
        "                print(f\"⚠️ No data for {bus}\")\n",
        "                return\n",
        "\n",
        "            results = calculate_non_operating_days(bus_data, current_date)\n",
        "\n",
        "            with lock:\n",
        "                non_op_days_map[row_idx] = {\n",
        "                    'bus': bus,\n",
        "                    'week': results['week'],\n",
        "                    'month': results['month'],\n",
        "                    'year': results['year']\n",
        "                }\n",
        "\n",
        "            print(f\"✅ {bus}: {results['week']['count']}w/{results['month']['count']}m/{results['year']['count']}y\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to process {bus}: {e}\")\n",
        "\n",
        "    # Process in parallel batches\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        futures = []\n",
        "        for row_idx, bus in row_map.items():\n",
        "            futures.append(executor.submit(process_bus, bus, row_idx))\n",
        "            if len(futures) >= BATCH_SIZE:\n",
        "                concurrent.futures.wait(futures)\n",
        "                futures = []\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "        if futures:\n",
        "            concurrent.futures.wait(futures)\n",
        "\n",
        "    print(\"\\n📝 STEP 4: Updating worksheet...\")\n",
        "    update_data = []\n",
        "    for row_idx, bus in row_map.items():\n",
        "        results = non_op_days_map.get(row_idx, {\n",
        "            'week': {'count': 0, 'dates': []},\n",
        "            'month': {'count': 0, 'dates': []},\n",
        "            'year': {'count': 0, 'dates': []}\n",
        "        })\n",
        "\n",
        "        update_data.append([\n",
        "            results['week']['count'],\n",
        "            results['month']['count'],\n",
        "            results['year']['count'],\n",
        "            format_date_list(results['week']['dates']),\n",
        "            format_date_list(results['month']['dates']),\n",
        "            format_date_list(results['year']['dates'])\n",
        "        ])\n",
        "\n",
        "    # Batch update\n",
        "    for i in range(0, len(update_data), 100):\n",
        "        batch = update_data[i:i+100]\n",
        "        range_start = i + 2\n",
        "        range_end = range_start + len(batch) - 1\n",
        "        Report_EO1_Worksheet.update(\n",
        "            f'AH{range_start}:AM{range_end}',\n",
        "            batch\n",
        "        )\n",
        "        print(f\"✅ Updated rows {range_start}-{range_end}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(\"\\n🎉 Done! Non-operating days calculation complete.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "VuNGbsnmqWvi",
        "outputId": "f09a1f19-0e5b-46da-d60d-94286df09bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Initializing non-operating days calculator...\n",
            "\n",
            "🧹 STEP 1: Clearing existing data...\n",
            "✅ Cleared columns AH-AM\n",
            "\n",
            "📅 STEP 2: Loading bus list...\n",
            "🔍 Found 83 buses to process\n",
            "\n",
            "🚌 STEP 3: Calculating non-operating days...\n",
            "✅ 442: 1w/13m/246y\n",
            "✅ 444: 2w/8m/255y\n",
            "✅ 441: 112w/135m/371y\n",
            "✅ 440: 206w/224m/401y\n",
            "✅ 457: 2w/12m/219y\n",
            "✅ 459: 0w/5m/290y\n",
            "✅ 460: 236w/259m/425y\n",
            "✅ 462: 6w/29m/173y\n",
            "✅ 469: 0w/11m/212y\n",
            "✅ 471: 3w/11m/229y\n",
            "✅ 474: 12w/26m/267y\n",
            "✅ 473: 0w/0m/199y\n",
            "✅ 476: 1281w/1290m/1454y\n",
            "✅ 475: 0w/0m/260y\n",
            "✅ 472: 5843w/5849m/5985y\n",
            "✅ 489: 0w/0m/0y\n",
            "✅ 486: 0w/0m/246y\n",
            "✅ 504: 100w/115m/414y\n",
            "✅ 495: 749w/751m/892y\n",
            "✅ 508: 615w/623m/815y\n",
            "✅ 518: 0w/0m/0y\n",
            "✅ 521: 3w/13m/214y\n",
            "✅ 511: 0w/15m/164y\n",
            "✅ 514: 3w/9m/217y\n",
            "✅ 515: 0w/0m/95y\n",
            "✅ 530: 4w/15m/279y\n",
            "✅ 523: 10892w/10911m/11142y\n",
            "✅ 528: 0w/4m/227y\n",
            "✅ 534: 2w/7m/143y\n",
            "✅ 535: 2w/7m/90y\n",
            "✅ 547: 234w/246m/499y\n",
            "✅ 538: 1w/7m/163y\n",
            "✅ 536: 1w/13m/127y\n",
            "✅ 548: 3w/13m/209y\n",
            "✅ 546: 2w/14m/199y\n",
            "✅ 564: 0w/0m/0y\n",
            "✅ 550: 244w/267m/529y\n",
            "✅ 563: 2w/16m/225y\n",
            "✅ 574: 0w/0m/114y\n",
            "✅ 575: 0w/0m/114y\n",
            "✅ 577: 3w/11m/175y✅ 584: 0w/0m/74y\n",
            "\n",
            "✅ 585: 12754w/12756m/12830y\n",
            "✅ 587: 122w/132m/349y\n",
            "✅ 578: 3w/15m/222y\n",
            "✅ 589: 0w/0m/185y\n",
            "✅ 595: 0w/12m/204y\n",
            "✅ 588: 0w/2m/117y\n",
            "✅ 590: 0w/12m/202y\n",
            "✅ 596: 3w/16m/212y\n",
            "✅ 598: 2w/11m/213y✅ 599: 3w/12m/190y\n",
            "\n",
            "✅ 603: 74w/84m/249y\n",
            "✅ 597: 2w/17m/212y\n",
            "✅ 604: 3w/15m/201y\n",
            "✅ 605: 2w/10m/181y\n",
            "✅ 612: 89w/93m/203y\n",
            "✅ 613: 0w/0m/58y\n",
            "✅ 614: 0w/3m/145y\n",
            "✅ 437: 0w/1m/169y\n",
            "✅ 479: 0w/0m/25y\n",
            "✅ 478: 1w/1m/77y\n",
            "✅ 438: 0w/0m/74y\n",
            "✅ 463: 5w/28m/363y\n",
            "✅ 461: 0w/10m/80y\n",
            "✅ 493: 5w/13m/217y\n",
            "✅ 507: 0w/0m/55y\n",
            "✅ 516: 0w/0m/65y\n",
            "✅ 529: 4w/27m/135y\n",
            "✅ 560: 1w/1m/91y\n",
            "✅ 561: 0w/0m/59y\n",
            "✅ 562: 1w/3m/185y\n",
            "✅ 572: 224w/224m/228y\n",
            "✅ 592: 0w/0m/1y\n",
            "✅ 573: 0w/0m/28y\n",
            "✅ 593: 0w/0m/35y\n",
            "✅ 615: 5w/23m/51y\n",
            "✅ 616: 0w/0m/2y\n",
            "✅ 617: 1w/1m/35y\n",
            "✅ 999: 0w/0m/78y\n",
            "✅ 990: 112w/135m/371y\n",
            "✅ 980: 5844w/5867m/6007y\n",
            "✅ 970: 900w/923m/1097y\n",
            "\n",
            "📝 STEP 4: Updating worksheet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-fec457a84873>:210: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  Report_EO1_Worksheet.update(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "APIError",
          "evalue": "APIError: [400]: Your input contains more than the maximum of 50000 characters in a single cell.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fec457a84873>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-fec457a84873>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mrange_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mrange_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         Report_EO1_Worksheet.update(\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;34mf'AH{range_start}:AM{range_end}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gspread/worksheet.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, values, range_name, raw, major_dimension, value_input_option, include_values_in_response, response_value_render_option, response_date_time_render_option)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         }\n\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m         response = self.client.values_update(\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspreadsheet_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mfull_range_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gspread/http_client.py\u001b[0m in \u001b[0;36mvalues_update\u001b[0;34m(self, id, range, params, body)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[1;32m    172\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPREADSHEET_VALUES_URL\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"put\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gspread/http_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIError\u001b[0m: APIError: [400]: Your input contains more than the maximum of 50000 characters in a single cell."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrVWB4d5SqXX3dnp9QXSnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}